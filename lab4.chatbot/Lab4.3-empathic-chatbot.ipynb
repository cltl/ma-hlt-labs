{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3 - Chatbot to detect emotions\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "In this notebook, we will create a Telegram chatbot that will detect the emotion in a message, and respond appropriately according to a set of keywords in the same message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main goal of this notebook**: The most important goal of this notebook is to have a Telegram chatbot that can detect emotion, and detect keywords in the received messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At the end of this notebook, you will**:\n",
    "\n",
    "* **Integrate knowledge you have learned in the previous labs such as**:\n",
    "  * **Load a pre-trained emotion classifier**\n",
    "  * **Measure semantic similarity between a set of words**\n",
    "  * **Use a predefined question - answering dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an empathic semantic chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plan is the following. You learned how to build a emotion classifier in Lab3. You also learned how to load a word embedding model and get the words that are most similar to a word. Having these skills, it should not be so difficult to:\n",
    "\n",
    "1) send each message to the emotion classifier and to get the emotion\n",
    "2) match each token or the most similar words from a token against a set of keywords and to find a match\n",
    "\n",
    "This would be the basic design for a chatbot that given the emotion and keywords associated with a message gives a certain response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "from pprint import PrettyPrinter\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from utils import read_token, read_qa, BotHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the pre-trained models we have: an emotion classifier we built and the word embedding model that was used to build it or that we want to use to match the keywords. We assume you still have the emotion detection classifiers stored in the models folder of Lab3. You may need to adapt the path in the following code to match your local set up.\n",
    "\n",
    "The next function loads a whole bunch of models that we need so that you do not need to worry about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_classifier():\n",
    "    \"\"\" Function to load pre-trained machine learning models needed \"\"\"\n",
    "    filename_vectorizer = '../lab3.machine_learning/models/utterance_vec.sav'\n",
    "    filename_transformer = '../lab3.machine_learning/models/utterance_transf.sav'\n",
    "    filename_encoder = '../lab3.machine_learning/models/label_encoder.sav'\n",
    "    filename_classifier = '../lab3.machine_learning/models/svm_linear_clf_bow.sav'\n",
    "\n",
    "    # load the classifier and the vectorizer from disk\n",
    "    loaded_classifier = pickle.load(open(filename_classifier, 'rb'))\n",
    "    loaded_vectorizer = pickle.load(open(filename_vectorizer, 'rb'))\n",
    "    loaded_transformer = pickle.load(open(filename_transformer, 'rb'))\n",
    "    loaded_label_encoder = pickle.load(open(filename_encoder, 'rb'))\n",
    "    \n",
    "    preprocessing_tools = {'vectorizer': loaded_vectorizer, \n",
    "                           'transformer': loaded_transformer,\n",
    "                           'label_encoder': loaded_label_encoder}\n",
    "\n",
    "    return loaded_classifier, preprocessing_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_semantic_model():\n",
    "    \"\"\" Function to load word embedding models needed \"\"\"\n",
    "    ### Adapt the path according to your local settings to point to your word embedding model\n",
    "    path_to_model = '/Users/selbaez/Documents/PhD/data/word_embeddings/GoogleNews-vectors-negative300.bin'\n",
    "    embedding_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to define the funtions to classify the emotion and to get similar words. To classify the emotion in the message we will need the message and the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classify_emotion(message, classifier, preprocessing_tools):\n",
    "    \"\"\" Function to process a message and predict the emotion it reflects \"\"\"\n",
    "    # Remember our classifier expects a list of texts so we simply put the message in a list\n",
    "    message = [message]\n",
    "\n",
    "    # We use the transform function to represent the message as a vector according to the model\n",
    "    # This works for the Bag-of-Words classifier that we created\n",
    "    counts = preprocessing_tools['vectorizer'].transform(message) ### This is the vector according to the count model\n",
    "    tfidf = preprocessing_tools['transformer'].transform(counts)  ### this is the vector according to the TFIDF model\n",
    "    \n",
    "    # Predict\n",
    "    predictions = classifier.predict(tfidf)\n",
    "\n",
    "    # Map prediction to a label\n",
    "    for predicted_label in predictions:\n",
    "        predicted_emotion = preprocessing_tools['label_encoder'].classes_[predicted_label]\n",
    "\n",
    "    return predicted_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function only works for the Bag-of-Word classifiers created. Think about what function is needed to classify a message according to the word embedding models. How to represent the message with a vector that can be handled by a model based on averaged word embedding vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to define the function by which we will try to match the topic keywords (e.g. music) to the tokens found in the message. First we will have to expand the meaning of the message by finding similar words to the ones the user sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar_words(embedding_model, message, num_similar_words=10, verbose=False):\n",
    "    \"\"\" Function to enrich the message with similar words for better keyword detection \"\"\"\n",
    "    # TODO filter by content words\n",
    "    tokens = nltk.tokenize.word_tokenize(message)\n",
    "\n",
    "    similar_words = defaultdict(set)\n",
    "    for token in set(tokens):\n",
    "        # Add the token itself to the enriched message\n",
    "        similar_words[token].add(token)\n",
    "        \n",
    "        # Try getting similar words if the vector for the given token is found\n",
    "        try:\n",
    "            word_neighborhood = embedding_model.most_similar(positive=[token], topn=num_similar_words)\n",
    "            # Add neighbor words to enrich the message\n",
    "            for item in word_neighborhood:\n",
    "                word = item[0].lower()\n",
    "                similar_words[word].add(token)\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(\"token '%s' not in embedding vocabulary\" % token)\n",
    "\n",
    "    if verbose:\n",
    "        PrettyPrinter(indent=2).pprint(similar_words)\n",
    "\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can find the intersection between our enriched message tokens and the pre-defined keywords in our qa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_keyword_intersection(enriched_message, keywords):\n",
    "    \"\"\" Function to determine if the message matches certain keywords according to some semantic similarity or relatedness\"\"\"\n",
    "    message_words = enriched_message.keys()\n",
    "\n",
    "    word_intersection = list(set(keywords) & set(message_words))\n",
    "\n",
    "    matched_words = {w: enriched_message[w] for w in word_intersection}\n",
    "\n",
    "    return matched_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is create a response, given an incoming message. Here we can call the functions we defined before to classify emotion, enrich the meaning of the message, and match keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_response(message, qa_data, classifier, preprocessing_tools, embedding_model):\n",
    "    # Determine default response\n",
    "    reply = \"I cannot respond to this\"\n",
    "    \n",
    "    # Enrich the message\n",
    "    similar_words = get_similar_words(embedding_model, message)\n",
    "    enriched_message = message + ' ' + ' '.join(similar_words.keys())\n",
    "    \n",
    "    # Classify emotion in message\n",
    "    emotion = classify_emotion(message, classifier, preprocessing_tools)\n",
    "    \n",
    "    # Loop through the predefined intents, and generate a response if there is a match (emotion + keywords)\n",
    "    for i in qa_data['intents']:\n",
    "        \n",
    "        # Only consider intents related to the emotion detected\n",
    "        if emotion == i['category']:\n",
    "            \n",
    "            # Try to match the message to the set of predefined keywords\n",
    "            word_intersection = get_keyword_intersection(similar_words, i['keywords'])\n",
    "\n",
    "            # If there is a match, generate a response response \n",
    "            if word_intersection:\n",
    "                print(\"Emotion detected: {emotion}\".format(emotion=emotion))\n",
    "                print(\"Keywords detected [(keyword): (message_token)]: \\n\\t{intersection}\".format(intersection=word_intersection))\n",
    "\n",
    "                response = random.choice(i['responses'])\n",
    "                break\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous notebooks, we create our BotHandler and respond to the last message sent to the Telegram chatbot by a specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CLTL_TOKEN = read_token()\n",
    "user_id = 408043639\n",
    "bot = BotHandler(CLTL_TOKEN)\n",
    "\n",
    "qa_data = read_qa(qa_path = './data/emotions.json')\n",
    "classifier, preprocessing_tools = load_classifier()\n",
    "embedding_model = load_semantic_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token 'Maluma' not in embedding vocabulary\n",
      "token 'of' not in embedding vocabulary\n",
      "token ',' not in embedding vocabulary\n",
      "Emotion detected: surprise\n",
      "Keywords detected [(keyword): (message_token)]: \n",
      "\t{'melody': {'song'}, 'song': {'singing', 'song'}}\n",
      "Received: I can't believe Adele is singing the next reggaeton song with Maluma, I thought she was not that type of artist\n",
      "Responded: I am just as shocked as you about this music\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/ma-hlt-labs/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "last_message = bot.get_last_message_by(user_id)\n",
    "response = create_response(last_message, qa_data, classifier, \n",
    "                           preprocessing_tools, embedding_model)\n",
    "bot.send_message_to(user_id, response)\n",
    "\n",
    "print(\"Received: {message}\".format(message=last_message))\n",
    "print(\"Responded: {response}\".format(response=response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
