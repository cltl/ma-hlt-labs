{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1 How to use the Large Language Models: BERT and RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how you can use a Large Language Model (LLM) throught the Transformer package. LLMs models are published regularly on the huggingface platform: https://huggingface.co/models\n",
    "\n",
    "These models are very big (Gigabytes) and require a computer with sufficient memory to load. Furthermore, loading these models takes some time as well. It is also possible to copy such a model to your disk and to load the local copy. Still a substantial memory is needed to load it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "There is whole family of transformer models developed by different research groups and published on the Huggingface platform. We will look at two popular LLMs BERT and its sequel RoBERTa, specifically its crosslingual variant XML-RoBERTa.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, without human labelling. BERT was pretrained with two objectives:\n",
    "\n",
    "* Masked language modeling (MLM): given a sentence, the model randomly masks 15% of the words in the input. It next runs the entire masked sentence through the model to predict the masked words from the unmasked words. The weights of the unmasked words are adapted to make the right predictions, just as with word embeddings, but do this together usiing **Attention**. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which only mask the next tokens. BERT learns a bidirectional representation of the sentence.\n",
    "* Next sentence prediction (NSP): the model concatenates two masked sentences as input during pretraining. Sometimes they correspond to sentences that were following each other in the original text (positive example), sometimes it is a random sentence (negative example). The model then has to predict if the two sentences were following each other or not. A special [CLS] token preceding the sentence representation is adapted to make the right prediction.\n",
    "\n",
    "The core trick of the model is **Attention**, hence the seminal paper \"Attention is all you need\" (Vaswani et al 2017). Attention refers to the principle that the embedding representation of a word in a sequence that needs to predict the masked tokens gets help from other tokens. Likewise, the model learns which words in the context pay most attention to the role of predicting masked words. In the next two examples, the representation of **star** will be different due to the attention of the other context words in order to predict **movie** and **planet** respectively:\n",
    "\n",
    "* the [masked] star went in rehab --> masked=movie\n",
    "* the orbit of the [masked] around the star is not stable --> masked=planet\n",
    "\n",
    "  \n",
    "This is applied to all words in a sequence and over many (11 to more than 20) layers with different attention heads.\n",
    "\n",
    "Eventually running many times over large amounts of data, the models learn an inner representation of the English language (a language model) that can then be used to represent sentences in texts as contextual vectors. If you have a dataset of labeled sentences for instance, you can train a standard classifier using the representations produced by the BERT model as inputs. This is called fine-tuning: \n",
    "\n",
    "![finetune](fine-tuning.png)\n",
    "\n",
    "These contextual representations turn out to be very effective in downstream tasks after fine-tuning. In many papers, researchers demonstrated that contextual representations capture implicit properties that are better for training a classifier than feature-engineered representations. Whereas terrabytes of text data has been used for pretraining the LLM, relatively little data is needed to fine-tune the model for a specific classification task. A small training set (Megabytes) is represented through the LLM and a classification head is trained to associate labels, such as sentiment, topic, etc.\n",
    "\n",
    "RoBERTa is a more efficient sequel to BERT in which the next sentence prediction was dropped, more data, more layers and wider contexts were used (Liu et al 2019). It often gives better performance than BERT in downstream tasks but it is bigger and more heavy to use.\n",
    "\n",
    "XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages (Lample and Conneau 2019). XLM-RoBERTa can represent text in any of these languages and when finetuned in one language (or more) the classification head can be applied to all these languages. This makes it possible to profit from for example English training data that is available for to be used for all languages in the model even if there is no labeled data in that specific language.\n",
    "\n",
    "## References\n",
    "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "\n",
    "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzma ́n, Edouard Grave, Myle Ott, Luke Zettle- moyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 8440– 8451, Online. Association for Computational Lin- guistics.\n",
    "\n",
    "Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n",
    "\n",
    "Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. \"Roberta: A robustly optimized bert pretraining approach.\" arXiv preprint arXiv:1907.11692 (2019).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model in a prefabbed NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the English case-sensitive BERT model that is provided by the Huggingface platform. It is possible to load the model itself in combination with its tokenizer to get a representation of a text for all words across all layers. It is however rather complex to exploit these representations for specific tasks. Huggingface therefore provides an option to create a pipeline to perform an NLP task with a pretrained model:\n",
    "\n",
    "    \"The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\"\n",
    "\n",
    "More information can be found here: https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html\n",
    "\n",
    "You can find information on the different tasks and how to call these. We will use these pipelines in this class for a gentle introduction. In notebook 4.3, will use the pipeline module to load fine-tuned models to perform sentiment analysis and emotion detection. In notebook 4.4, we try out finetuned crosslingual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires installing the deep learning packages **pytorch** and **transformers**. For this lab you should use the transformers version 4.16.0. Once installed, you can comment out the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install numpy==1.26.4\n",
    "#!pip install torch==2.0.0 / 2.2.2\n",
    "#!pip install transformers==4.16.0 / 4.41.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use the two pretrained Language Models BERT and XLM-RoBERTa to perform the basic task of predicting a masked word in the context of a sentence. \n",
    "\n",
    "The models are available on the huggingface.co platform:\n",
    "\n",
    "* https://huggingface.co/bert-base-cased\n",
    "* https://huggingface.co/xlm-roberta-base\n",
    "\n",
    "On the platform you can find documentation, the actual files that make up the model (in Files and versions) and an online demo: Hosted inference API. Look around a bit in the platform to see what is there and how the models are released. For fun, download BERT's ```vocab.txt``` file and inspect it to see what words and pieces of words BERT represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try out the model online, but we want to use it in our programs locally. Therefore we are going to load it in this notebook from huggingface.co and use it in a ```pipeline``` to do the MASKED-task: predicting the worlds that are most likely to occur in a specific masked position in a sentence. For this, we use the **fill-mask** pipeline as an interface for the model.\n",
    "\n",
    "We create an instance of the pipeline class, where we specify the task **fill-mask** and the name of the model. When creating the instance, the constructor scans the Huggingface platform for the model (or your local cache) and its configuration for the task. If you have loaded the model before, it will find it cached on your local disk. If not, it will download it from Huggingface, which may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('fill-mask', model='bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When downloading the model and creating the pipeline, you may see a warning that some weights (parameters) are not downloaded, specifically for the ```[CLS]``` token. The ```[CLS]``` token is used for text classification and is not needed for masked token prediction. By creating a pipeline for masked-token-prediction, the downloader inferred it can do without these weights to make it more efficient.\n",
    "\n",
    "The parameter **device** is optional. If you leave it out or set the value to \"-1\", you will use only your CPU. If your machine has GPUs, such as my Mac, you can set the value to the number of GPUs you want to use. If you can use the GPUs, loading and running will be a lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our ```pipe``` has been created, we can use it by giving it the text as input but replace the words we want to predict with ```[MASK]```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A keyboard and a mouse are connected to the computer. 0.9537659287452698\n",
      "A keyboard and a keyboard are connected to the computer. 0.005814633797854185\n",
      "A keyboard and a screen are connected to the computer. 0.0053267935290932655\n",
      "A keyboard and a monitor are connected to the computer. 0.002222249750047922\n",
      "A keyboard and a microphone are connected to the computer. 0.0020764158107340336\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('A keyboard and a [MASK] are connected to the computer.'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat chased the dog for minutes. 0.1145443245768547\n",
      "The cat chased the mouse for minutes. 0.06594377756118774\n",
      "The cat chased the cat for minutes. 0.04837099462747574\n",
      "The cat chased the bird for minutes. 0.042223136872053146\n",
      "The cat chased the rabbit for minutes. 0.03711290657520294\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('The cat chased the [MASK] for minutes.'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the different predictions for the MASKED position that the context triggered different words. We also see that the probabilities descrease rapidly for the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So **mouse** is showing up in both predictions but it is ambiguous. Can we reverse this and see whether **mouse** makes the right repdictions in these two contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICT mouse:\n",
      "\t A keyboard and a mouse are connected to the computer. 0.9509470462799072\n",
      "\t A button and a mouse are connected to the computer. 0.006262615323066711\n",
      "\t A monitor and a mouse are connected to the computer. 0.005059094168245792\n",
      "\t A mouse and a mouse are connected to the computer. 0.00461850268766284\n",
      "\t A cat and a mouse are connected to the computer. 0.004106519743800163\n",
      "Animal mouse:\n",
      "\t The cat chased the mouse for minutes. 0.38550305366516113\n",
      "\t The dog chased the mouse for minutes. 0.10169946402311325\n",
      "\t The mouse chased the mouse for minutes. 0.03561084717512131\n",
      "\t The boy chased the mouse for minutes. 0.02418743446469307\n",
      "\t The man chased the mouse for minutes. 0.018139027059078217\n"
     ]
    }
   ],
   "source": [
    "print(\"ICT mouse:\")\n",
    "for res in pipe('A [MASK] and a mouse are connected to the computer.'):\n",
    "    print(\"\\t\",res['sequence'], res['score'])\n",
    "\n",
    "print(\"Animal mouse:\")\n",
    "for res in pipe('The [MASK] chased the mouse for minutes.'):\n",
    "    print(\"\\t\",res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first results with higher scores look good but there are also weird things in the lists, as **cat** still shows up in the first list and **mouse** chasing **mouses** is not very likley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of words, we can also prompt for names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr Smith was charged with murder. 0.01834821328520775\n",
      "Mr Brown was charged with murder. 0.01094571128487587\n",
      "Mr Jones was charged with murder. 0.009453780949115753\n",
      "Mr Johnson was charged with murder. 0.00939914584159851\n",
      "Mr Williams was charged with murder. 0.009156161919236183\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('Mr [MASK] was charged with murder.'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the other way around, we can prompt for what people are charged for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr Williams was charged with murder. 0.28759217262268066\n",
      "Mr Williams was charged with assault. 0.2819986939430237\n",
      "Mr Williams was charged with theft. 0.08729258179664612\n",
      "Mr Williams was charged with fraud. 0.04526205733418465\n",
      "Mr Williams was charged with corruption. 0.028985124081373215\n",
      "Mrs Williams was charged with murder. 0.34363028407096863\n",
      "Mrs Williams was charged with assault. 0.24265511333942413\n",
      "Mrs Williams was charged with theft. 0.07686624675989151\n",
      "Mrs Williams was charged with rape. 0.04139396548271179\n",
      "Mrs Williams was charged with fraud. 0.0361587293446064\n"
     ]
    }
   ],
   "source": [
    "for res in pipe('Mr Williams was charged with [MASK].'):\n",
    "    print(res['sequence'], res['score'])\n",
    "    \n",
    "for res in pipe('Mrs Williams was charged with [MASK].'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what these models do when they generate these answers! Did they index knowledge on individuals as facts or do they only have a hunch about what vocabulary items can be expected in this position?\n",
    "\n",
    "Also note that by only changing ```Mr``` into ```Mrs``` we get a different result and different scores. The differences do not make much sense to me as it would be more likely that a ```Mr Williams``` is charged with rape than a ```Mrs Williams```. \n",
    "\n",
    "This tells us that 1) models are biased for gender and 2) they do not use common sense reasoning and knowledge as we do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load XLM-RoBERTa from huggingface in a fill-mask pipeline as we have done before for English BERT to perform word predictions in different languages covered by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "xlmpipe = pipeline('fill-mask', model='xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded, we can use this ```xlmpipe``` in a similar way. The only difference being that the masked-token in this model is represented as ```<mask```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wish I was a girl . 0.09122683852910995\n",
      "I wish I was a teenager . 0.07413578033447266\n",
      "I wish I was a poet . 0.0635061040520668\n",
      "I wish I was a child . 0.04012115299701691\n",
      "I wish I was a virgin . 0.03340147063136101\n"
     ]
    }
   ],
   "source": [
    "for res in xlmpipe('I wish I was a <mask>.'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us now try some texts in other languages with the same XLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ik wou dat ik een vrouw was. 0.12419918924570084\n",
      "Ik wou dat ik een meisje was. 0.058724191039800644\n",
      "Ik wou dat ik een engel was. 0.04070577770471573\n",
      "Ik wou dat ik een homo was. 0.03964514657855034\n",
      "Ik wou dat ik een mens was. 0.03921167552471161\n"
     ]
    }
   ],
   "source": [
    "for res in xlmpipe('Ik wou dat ik een <mask> was.'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich mochte gerne ein Mädchen sein. 0.054317839443683624\n",
      "Ich mochte gerne ein Moderator sein. 0.04253319650888443\n",
      "Ich mochte gerne ein Mensch sein. 0.0409962497651577\n",
      "Ich mochte gerne ein Paar sein. 0.029233723878860474\n",
      "Ich mochte gerne ein Engel sein. 0.028841273859143257\n"
     ]
    }
   ],
   "source": [
    "for res in xlmpipe('Ich mochte gerne ein <mask> sein.'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The man was charged with murder. 0.3557787537574768\n",
      "The suspect was charged with murder. 0.19354721903800964\n",
      "The woman was charged with murder. 0.08891984075307846\n",
      "The victim was charged with murder. 0.08566290885210037\n",
      "The officer was charged with murder. 0.03933110460639\n"
     ]
    }
   ],
   "source": [
    "for res in xlmpipe('The <mask> was charged with murder.'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De verdachte is aangeklaagd voor moord. 0.4496552348136902\n",
      "De man is aangeklaagd voor moord. 0.3227398693561554\n",
      "De vrouw is aangeklaagd voor moord. 0.042014263570308685\n",
      "De politie is aangeklaagd voor moord. 0.019670894369482994\n",
      "De bestuurder is aangeklaagd voor moord. 0.01778385229408741\n"
     ]
    }
   ],
   "source": [
    "for res in xlmpipe('De <mask> is aangeklaagd voor moord.'):\n",
    "    print(res['sequence'], res['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So note that the returned results are different across the languages. This means there are different associations triggered even though the semantics of the input sentences is the same. So in addition to biases such as gender, race, etc. there are also linguistic-cultural biases in the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check which languages are covered in XLM-RoBERTa and try out other word predictions in other languages yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT and RoBERTa use a different representation for the masked token. This is an incidental difference. The can check the vocabulary file of the model to see what tokens are used.\n",
    "\n",
    "You can find the actual files on huggingface.co. Go to a model of your choice and use the option \"Files and versions\" to get a listing of the files for a model. The file ```tokenizer.json``` contains the vocabulary:\n",
    "\n",
    "* https://huggingface.co/bert-base-cased/raw/main/tokenizer.json\n",
    "* https://huggingface.co/xlm-roberta-base/blob/main/tokenizer.json\n",
    "\n",
    "These files a big but can be opened in a browser as raw data or downloaded individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
