{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02963a8-0199-40dd-80e5-c3a528e5bbb9",
   "metadata": {},
   "source": [
    "# Lab 3.5 Interannotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8893635e-1a2a-4ca5-9b7d-e6b2edc9e520",
   "metadata": {},
   "source": [
    "This notebook shows how you can calculate the Inter-Annotator agreement (IAA) for the annotation of conversations with Llama3 for Lab0. There are three well-known methods to calculate the IAA: Cohen's kappa, Fleiss Kappa and Krippendorf's alpha. All three measures compare the agreement against the chance that the annotators accidently agree. Cohen and Fleiss compare the agreement against the expected agreement by chance and Krippendorf against the expected disagreement by chance. Cohen can only compare two annotators and nominal data, Fleiss more than two annotators of nominal data, Krippendorf more than two annotators and many different data types. In general, scores of 0.6 or higher are considerd to be of sufficient quality. If scores are lower, the annotations cannot be trusted.\n",
    "\n",
    "We will use the **metrics** module from NLTK which provides an **agreement.AnnotationTask** function to feed the data that needs to be compared. We will take the data from the annotated Llama conversations that are stored as JSON file. We first import **json** and the **NLTK** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce745454-1ed6-4f99-b5da-455b44903f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from  nltk.metrics import agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210bed78-03cb-4407-a3fb-7b1ef181163d",
   "metadata": {},
   "source": [
    "In the data folder, you find two annotations of the same conversation. The fake annotators are \"Piek\" and \"Hennie\". We can read the data from the file to create a json object for the original conversation with the two annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c341195-c366-49ce-a490-2a4164fb012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 35\n"
     ]
    }
   ],
   "source": [
    "# Assume JSON files with a conversation as a list of turns\n",
    "# each turn as an Annotator and a Gold label field\n",
    "# All conversations should have an equal number of turns\n",
    "\n",
    "file1 = open(\"./data/iaa/annotator_Piek_human_Piek_chat_with_llama.json\")\n",
    "conversation1 = json.load(file1)\n",
    "\n",
    "file2 = open(\"./data/iaa/annotator_Hennie_human_Piek_chat_with_llama.json\")\n",
    "conversation2 = json.load(file2)\n",
    "\n",
    "print(len(conversation1), len(conversation2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d0df3-f637-4835-b1ee-7ab7bc191deb",
   "metadata": {},
   "source": [
    "By ckecking the length, we verify that the conversations have the same number of turns. The next function will extract a list of annotations from the data in the form of a tuple that consists of the name of the annotator, the turn identifier and the label. We are ignoring the turns uttered by Llama, as these were annotated automatically as \"neutral\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35f39b9c-d34f-439a-9754-e20b5e199c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function takes as parameters a conversation in JSON format \n",
    "## and it returns a list of annotations in the form of a tuple:\n",
    "## annotator, turn id, label\n",
    "def get_chat_annotations(conversation=[]):\n",
    "    annotations = []\n",
    "    for turn in conversation:\n",
    "        speaker = turn['speaker']\n",
    "        turn_id = turn['turn_id']\n",
    "        if not speaker=='Llama':\n",
    "            annotator= turn['Annotator']\n",
    "            gold = turn['Gold']\n",
    "            annotations.append((annotator, turn_id, gold))\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c3241d-29e1-4911-9b9e-ea89dfe6381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 17\n",
      "2 2\n",
      "4 4\n",
      "6 6\n",
      "8 8\n",
      "10 10\n",
      "12 12\n",
      "14 14\n",
      "16 16\n",
      "18 18\n",
      "20 20\n",
      "22 22\n",
      "24 24\n",
      "26 26\n",
      "28 28\n",
      "30 30\n",
      "32 32\n",
      "34 34\n"
     ]
    }
   ],
   "source": [
    "anno1 = get_chat_annotations(conversation1)\n",
    "anno2 = get_chat_annotations(conversation2)\n",
    "print(len(anno1), len(anno2))\n",
    "for a1, a2 in zip(anno1, anno2):\n",
    "    print(a1[1], a2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022dedea-5419-4634-8574-df07c5d8dd6e",
   "metadata": {},
   "source": [
    "We extracted list of annotations with an equal length. By zipping both lists and getting the second element from the tuple for each pair, we can check if they have the same turn ids. Our data seems to be compatible. Now we can concatenate the two lists and call the **AnnotationTask** function with the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcb0bd0d-5de3-4ac4-b23e-2c8b6de73cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.47345132743362833\n",
      "Fleiss's Kappa: 0.47345132743362833\n",
      "Krippendorf's Kappa: 0.48206278026905836\n"
     ]
    }
   ],
   "source": [
    "data = anno1+anno2\n",
    "iaa = agreement.AnnotationTask(data=data)\n",
    "\n",
    "print(\"Cohen's Kappa:\", iaa.kappa())\n",
    "print(\"Fleiss's Kappa:\", iaa.multi_kappa())\n",
    "print(\"Krippendorf's Kappa:\", iaa.alpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99854f4b-8bc7-4947-a659-8984e8b950c5",
   "metadata": {},
   "source": [
    "All three measures score below .6 so these two annotators disagree a lot. This means the annotation is not trustworthy and the annotation task needs to be reconsidered.\n",
    "\n",
    "You can easily see how you can do this for more than two annotators by extracting more annoations and concatenating these in data: data = ann1+anno2+anno3+anno4, etc.\n",
    "\n",
    "If you have a large crowd of annotations, you could take the majority vote as the adjudicated label. Next you can compare each annotator's labels against the adjudicated data set and validate the quality of the annotator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8645d54f-2768-4a40-899c-456992eeb6d8",
   "metadata": {},
   "source": [
    "### End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51eeb90-fb39-40a9-9733-c4635dd9c772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
