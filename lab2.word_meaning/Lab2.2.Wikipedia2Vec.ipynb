{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB2.2: Word embeddings from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce you to word embeddings. Word embeddings are vector representations for words learned by a neural network for predicting the words that occur in their context. The weights applied to the hidden layer to make the correct predictions are taken as the vector representation for the meaning of the word. Usually, vector sizes are limited to 300 to 500 dimensions (context words). The advantage is that comparing vectors across words always match for certain dimensions: i.e. the vectors are dense vectors but match most strongly when words occur in similar contexts.\n",
    "\n",
    "Although there are many packages and data sets with embeddings, we focus here on publicly available and trainable embeddings, especially for multiple languages. We will use here the wikipedia2vec package that has pretrained models in various languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acnowledgement: https://wikipedia2vec.github.io/wikipedia2vec/\n",
    "\n",
    "To use the embeddings created from wikipedia (in a specific language) you need to do 3 things (also described on the above website):\n",
    "\n",
    "* Use `pip install Wikipedia2Vec` from the command line/terminal to install the package on your local computer\n",
    "* download an embedding model trained from wikipedia and unpack the compressed file with a decompression application\n",
    "* import the package in your notebook\n",
    "* load the local copy of the embedding model\n",
    "\n",
    "We guide you through these steps in this notebook and explain the basic functions. As there are different models for different languages, you can do this for any of the available languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pip install Wikipedia2Vec` on the command line to install the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download pre-trained models in various languages from: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "Note that there are different variants trained for 100 and 300 dimensions. If your computer has limited capacity, it is better to start with the 100 dimensions. For this notebook, we will download enwiki_20180420_100d.pkl.bz2, which is a compressed version of the 100 dimensions embeddings model built from the English Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE!\n",
    "\n",
    "If you fail to install the Wikipedia2Vec package, do not waste too much time fixing this. You can also switch to the notebook *Lab2.2b.Wikipedia2Vec_Gensim.ipynb* which explains how you can load the Wikipedia2Vec  text models in another package *Gensim*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you succesfully install Wikipedia2Vec you can proceed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When installed succesfully you can use the next import in your notebook. There is no need to install it again. \n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to download a model in a format that Wikipedia2Vec can load. The binary versions (pkl) are compressed in a bz2 format. You need to decompressing the file to a file with the extension \".pkl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the path to your local copy of an embedding model.\n",
    "# Here we specify an example of such a path. Adapt the path to where you have stored the donwload\n",
    "# Make sure it is decompressed. The *.bz2 file will not load. \n",
    "\n",
    "MODEL_FILE='/Users/piek/Desktop/ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.pkl'\n",
    "wiki2vec = Wikipedia2Vec.load(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By loading the model, we created an object with the name \"wiki2vec\" through which we can call functions and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_build_entity_neg_table',\n",
       " '_build_uniform_neg_table',\n",
       " '_build_unigram_neg_table',\n",
       " '_build_word_neg_table',\n",
       " 'dictionary',\n",
       " 'get_entity',\n",
       " 'get_entity_vector',\n",
       " 'get_vector',\n",
       " 'get_word',\n",
       " 'get_word_vector',\n",
       " 'load',\n",
       " 'load_text',\n",
       " 'most_similar',\n",
       " 'most_similar_by_vector',\n",
       " 'save',\n",
       " 'save_text',\n",
       " 'syn0',\n",
       " 'syn1',\n",
       " 'train',\n",
       " 'train_params']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wiki2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accessing word representations of different models\n",
    "\n",
    "Models may be stored in different (sometimes in confusing) formats, but they all boil down to these components:\n",
    "\n",
    "* a matrix of word vectors \n",
    "* a vocabulary\n",
    "* a mapping between vectors in the matrix to the words in the vocabulary (often via indices)\n",
    "\n",
    "Think about what a matrix is (no not the movie). You know that a vector is a list of digits, such that each digit is a value for a dimension in an n-dimensional space. Well, if you have a list of these vectors you have a matrix of n-columns and m-rows. Each row corresponds to the vector of a word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is represented internally as a...\n",
      "<class 'wikipedia2vec.wikipedia2vec.Wikipedia2Vec'>\n"
     ]
    }
   ],
   "source": [
    "# Explore the wiki2vec model as a python object:\n",
    "print('The model is represented internally as a...')\n",
    "print(type(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a dictionary that contains words among others. Let's check how big the vocabulary is of the model derived from English Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model vocabulary is represented internally as a...\n",
      "<class 'wikipedia2vec.dictionary.Dictionary'>\n",
      "1937422\n"
     ]
    }
   ],
   "source": [
    "vocabulary = wiki2vec.dictionary\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(vocabulary))\n",
    "print(len(list(vocabulary.words())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost two millions words are present in this model. That is a lot more than in the English WordNet. Let's check some of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words from the model vocabulary:\n",
      "[<Word s>, <Word sa>, <Word san>, <Word sand>, <Word sanda>, <Word sandal>, <Word sandali>, <Word sandalia>, <Word sandaliatus>, <Word sandalinas>, <Word sandaling>, <Word sandalio>, <Word sandaliyah>, <Word sandalo>, <Word sandalodes>, <Word sandalon>, <Word sandalops>, <Word sandalore>, <Word sandalov>, <Word sandalow>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "print('Some words from the model vocabulary:')\n",
    "print(list(vocabulary.words())[:20]) #Note that :20 gives the first 20 items in the list, print(list(vocabulary.words())[-1]) gives the last word\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in the vocabulary, we can now get the vector. We assume that 'man' is in the vocabulary. We can use the *get_word_vector* function to lookup the vector from the matrix for the word 'man':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information stored in the vocabulary for the word \"man\":\n",
      "<class 'numpy.memmap'>\n",
      "Distributional meaning of \"man\" in Wikipedia: [-2.37749144e-01  2.68582195e-01 -9.62369144e-02  2.70704746e-01\n",
      " -2.24097610e-01 -2.48913109e-01  1.06461413e-01  4.12168130e-02\n",
      " -5.34945190e-01 -1.44513458e-01 -8.70477855e-02 -1.87745050e-01\n",
      "  1.98523641e-01 -1.64299533e-01  1.02062628e-01 -1.78317577e-01\n",
      " -5.51789738e-02  2.19180398e-02 -2.18049601e-01  1.56891569e-01\n",
      " -2.83530265e-01 -3.29926699e-01 -6.78404942e-02  3.50453734e-01\n",
      " -3.24131519e-01 -9.09007853e-04 -1.23354875e-01 -3.45233470e-01\n",
      " -4.52311546e-01  7.44896114e-01  1.46970570e-01 -1.25839904e-01\n",
      " -1.07294962e-01  4.01940018e-01  1.11972339e-01  2.22993977e-02\n",
      " -3.72039467e-01  2.02560142e-01  3.16281393e-02  2.91241556e-02\n",
      " -2.40586206e-01  1.36774838e-01 -1.75260063e-02  1.01980194e-01\n",
      "  8.33696201e-02  5.01191735e-01 -3.97316903e-01  4.00523953e-02\n",
      " -1.65336326e-01 -1.89155132e-01 -1.44131929e-01  6.28692061e-02\n",
      " -5.18540621e-01 -2.63796657e-01  3.16548571e-02 -6.03113696e-02\n",
      "  1.01237603e-01 -5.40754557e-01 -3.52765709e-01 -1.28057823e-01\n",
      " -2.61692256e-01 -2.60725200e-01 -9.15315449e-02  3.09405506e-01\n",
      "  4.46753263e-01 -2.52553999e-01 -2.84168422e-01 -9.30303395e-01\n",
      " -3.27120796e-02 -4.66910541e-01  4.06448603e-01  2.04477414e-01\n",
      "  2.22286612e-01  2.50097722e-01 -4.57673043e-01  4.08855408e-01\n",
      "  1.26054496e-01 -2.00009704e-01 -8.70249942e-02 -2.70096391e-01\n",
      "  3.45325351e-01  7.20757395e-02  1.27733245e-01 -2.56741885e-02\n",
      "  2.31918097e-01  3.71200472e-01  5.39735258e-02  1.50234312e-01\n",
      "  2.97337472e-01  7.63115808e-02 -8.79110396e-02  2.38627642e-01\n",
      "  3.56182575e-01 -6.44256771e-02  2.08609164e-01 -3.86539042e-01\n",
      " -1.94404759e-02  5.60759604e-01 -6.43926084e-01  2.10351840e-01]\n"
     ]
    }
   ],
   "source": [
    "print('Information stored in the vocabulary for the word \"man\":')\n",
    "man_vector=wiki2vec.get_word_vector('man')\n",
    "print(type(man_vector))\n",
    "print('Distributional meaning of \"man\" in Wikipedia:', man_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a vector is a sorted bunch of numbers, each representing a dimension. These numbers are actually the weights learned by the neural network that are applied to the hidden layer when learning to predict the context words of 'man'.\n",
    "\n",
    "How many do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vector dimensions: 100\n"
     ]
    }
   ],
   "source": [
    "print('Number of vector dimensions:', len(man_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a surprise: we loaded a model with 100 dimensions based on a hidden layer with 100 neurons. This is true for all words so also for 'dog'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector4dog = wiki2vec.get_word_vector('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector4dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01304934  0.64761317  0.10454331 -0.3116781   0.14754549  0.0808935\n",
      " -0.15227112  0.26503846 -0.64620554 -0.20578592  0.15636262 -0.20720603\n",
      "  0.41793343  0.03861991 -0.01935025 -0.22413553  0.22274837 -0.34524342\n",
      " -0.42599422  0.102845   -0.21360567 -0.02671032  0.19456221  0.3651903\n",
      " -0.22647302 -0.27360198  0.03258029 -0.02785098 -0.23588972  0.5077206\n",
      "  0.37592876 -0.22071666 -0.05057421  0.7909033   0.1343578  -0.07903094\n",
      " -0.4099386   0.15587732 -0.00657076  0.1236117  -0.54740536 -0.08774299\n",
      " -0.3738407  -0.25297046 -0.4688306  -0.11844479 -0.05014395  0.32674935\n",
      " -0.17993684 -0.26620498  0.09679675  0.28913295 -0.4815562  -0.3374474\n",
      "  0.24882683  0.17436764  0.0888719  -0.18725184 -0.33120757 -0.1903342\n",
      "  0.05470906 -0.61491376 -0.42699674 -0.10787722  0.13698857 -0.14450763\n",
      "  0.05210021 -0.5711191  -0.38591218 -0.6626211   0.2417067  -0.01411594\n",
      "  0.39739552  0.13306352 -0.6726368  -0.22698367  0.1793001   0.24538207\n",
      "  0.15446481 -0.09232827 -0.02473994 -0.4611105  -0.13171642 -0.21940833\n",
      "  0.21425298  0.49103034 -0.21856439  0.24628595  0.08428791  0.13236591\n",
      " -0.45648926  0.00804436  0.6242295  -0.02165581 -0.08403038 -0.47217292\n",
      "  0.11906766  0.32987368 -0.9190552   0.09625731]\n"
     ]
    }
   ],
   "source": [
    "print(vector4dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the representations are compatible across the words, we can compare two vector representations through the cosine similarity function:\n",
    "\n",
    "![Cosine similarity](./images/cosine-full.png \"Logo Title Text 1\")\n",
    "\n",
    "So suppose we have two vectors A and B, each with 100 slots, this formula (taken from the Wikipedia page) tells you to sum the results of multiplying each slot across A and B:\n",
    "\n",
    "A[0]\\*B[0]+A[1]\\*B[1]+....A[99]\\*B[99]\n",
    "\n",
    "And divide this sum by the square-root of the total sum of the slots of A, multiplied by the square-root of the total sum of the slots of B. Dividing it that way normalises the value between 0 and 1 and it makes the sum of the products of the numerator relative to the product of the sums of the individual vectors.\n",
    "\n",
    "Embedding software uses such measures to obtain the most similar words. We already have the vector for 'dog' so we can now use the wiki2vec.most_similar_by_vector() function to ask for the words that are most simlar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<Word dog>, 0.99999994),\n",
       " (<Word dogs>, 0.8637307),\n",
       " (<Word cat>, 0.8286426),\n",
       " (<Word puppy>, 0.81508684),\n",
       " (<Word rabbit>, 0.8042291),\n",
       " (<Word montarges>, 0.798108),\n",
       " (<Word poodle>, 0.79497886),\n",
       " (<Word barfy>, 0.7915491),\n",
       " (<Word cockapoo>, 0.783462),\n",
       " (<Word pekapoos>, 0.78286505)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki2vec.most_similar_by_vector(vector4dog, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good. You can also first get a word object and use the wiki2vec.most_similar() function to get the same result. This function however requires a word object from the vocabulary. Let's see how that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'wikipedia2vec.dictionary.Word'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'count',\n",
       " 'doc_count',\n",
       " 'index',\n",
       " 'text']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_object_dog=wiki2vec.get_word('dog')\n",
    "print(type(word_object_dog))\n",
    "dir(word_object_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word objects are defined in the Wikipedia2Vec package as special data types, (e.g. you do not find them on other paclages such as Gensim). They provide the index to the vector in the matrix but also frequency stats at the token level and the documet level. We can also directly apply the e *most_similar* function without having to get the vector first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token frequency: 116223\n",
      "Document frequency: 54616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Word dog>, 0.99999994),\n",
       " (<Word dogs>, 0.8637307),\n",
       " (<Word cat>, 0.8286426),\n",
       " (<Word puppy>, 0.81508684),\n",
       " (<Word rabbit>, 0.8042291),\n",
       " (<Word montarges>, 0.798108),\n",
       " (<Word poodle>, 0.79497886),\n",
       " (<Word barfy>, 0.7915491),\n",
       " (<Word cockapoo>, 0.783462),\n",
       " (<Word pekapoos>, 0.78286505)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Token frequency:', word_object_dog.count)\n",
    "print('Document frequency:', word_object_dog.doc_count)\n",
    "wiki2vec.most_similar(word_object_dog, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we discussed the notion of information value in the class as expressed by the 'TD\\*IDF' formulae (Term frequency times the inversed document frequency). We could easily get such a simplified information value score based on the Wikipedia articles by dividing the token count by the document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information value of \"dog\" 2.1280027830672332\n",
      "Information value of \"cat\" 2.2003139717425433\n",
      "Information value of \"link\" 1.7776395603831898\n",
      "Information value of \"article\" 1.5689368990990131\n",
      "Information value of \"Trump\" 3.7625386706665416\n",
      "Information value of \"Poetin\" 2.25\n"
     ]
    }
   ],
   "source": [
    "print('Information value of \"dog\"', wiki2vec.get_word('dog').count/wiki2vec.get_word('dog').doc_count)\n",
    "print('Information value of \"cat\"', wiki2vec.get_word('cat').count/wiki2vec.get_word('cat').doc_count)\n",
    "\n",
    "print('Information value of \"link\"', wiki2vec.get_word('link').count/wiki2vec.get_word('link').doc_count)\n",
    "print('Information value of \"article\"', wiki2vec.get_word('article').count/wiki2vec.get_word('article').doc_count)\n",
    "\n",
    "print('Information value of \"Trump\"', wiki2vec.get_word('trump').count/wiki2vec.get_word('trump').doc_count)\n",
    "print('Information value of \"Poetin\"', wiki2vec.get_word('poetin').count/wiki2vec.get_word('poetin').doc_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that general words have lower scores than names of inviduals and typical Wikipedia jargon has lowest information value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia does not only have text but also a lot of entity mentions. Wiki2Vec therefore allows you to obtain entities and entity vectors as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'wikipedia2vec.dictionary.Entity'>\n",
      "Token frequency: 687\n",
      "Document frequency: 582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Entity Scarlett Johansson>, 1.0),\n",
       " (<Word charlize>, 0.7644369),\n",
       " (<Word winslet>, 0.727508),\n",
       " (<Entity Hilary Swank>, 0.7215545),\n",
       " (<Word paltrow>, 0.71570885),\n",
       " (<Entity Eva Green>, 0.71551895),\n",
       " (<Word noomi>, 0.714082),\n",
       " (<Entity Kate Winslet>, 0.71038455),\n",
       " (<Entity Keira Knightley>, 0.7100761),\n",
       " (<Word blanchett>, 0.70951277)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scarlett_entity=wiki2vec.get_entity('Scarlett Johansson')\n",
    "print(type(scarlett_entity))\n",
    "print('Token frequency:', scarlett_entity.count)\n",
    "print('Document frequency:', scarlett_entity.doc_count)\n",
    "\n",
    "wiki2vec.most_similar(scarlett_entity, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most similar entries are a mixture of other entities and words. This is nice property of Wikipedia2Vec because it includes the linked entities as a structure.\n",
    "\n",
    "Also note that the token frequency of 'Scarlett Johansson' is just a bit higher than the document frequency. This means she is mentioned about once per article and linked to other actors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as for words, you can get the entity vector as well and get the most similar by vector results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<Entity Scarlett Johansson>, 1.0),\n",
       " (<Word charlize>, 0.7644369),\n",
       " (<Word winslet>, 0.727508),\n",
       " (<Entity Hilary Swank>, 0.7215545),\n",
       " (<Word paltrow>, 0.71570885),\n",
       " (<Entity Eva Green>, 0.71551895),\n",
       " (<Word noomi>, 0.714082),\n",
       " (<Entity Kate Winslet>, 0.71038455),\n",
       " (<Entity Keira Knightley>, 0.7100761),\n",
       " (<Word blanchett>, 0.70951277)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scarlett=wiki2vec.get_entity_vector('Scarlett Johansson')\n",
    "print(len(scarlett))\n",
    "wiki2vec.most_similar_by_vector(scarlett, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Links to existing models available for download\n",
    "\n",
    "Follow the links to browse available models. The sources listed below contain English models trained using different algorithms, data with different degrees of preprocessing and varying hyperparameter settings. Some resources also include models in other languages (even Dutch with a bit of luck).\n",
    "\n",
    "### Large and commonly used models (English):\n",
    "\n",
    "* Google word2vec: can be downloaded from here (follow link in instructions): http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "* GloVe (trained on various corpora): https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "* FastText embeddings (Facebook): https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "* Models with different algorithms, hyperparamters, dimensions and degrees of preprocessing (e.g. dependency parsing windows):  https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html\n",
    "\n",
    "\n",
    "### Various models in English & other languages:\n",
    "\n",
    "* word2vec trained on Wikipedia for various languages (including Dutch): https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "* Various algorithms and parameters for English and other languages: http://vectors.nlpl.eu/repository/#\n",
    "\n",
    "* Word2vec wikipedia for English and German: https://github.com/idio/wiki2vec\n",
    "\n",
    "* Facebook's fastText (https://fasttext.cc) for languages other than English: https://fasttext.cc/docs/en/crawl-vectors.html \n",
    "\n",
    "\n",
    "Gensim even lets you download models directly via their api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: You can build your own word embedding model from a text corpus. Normally, you need a very large corpus to do this but it may be beneficiary to create a dedicated word embedding model for your application. If you build your own embedding space you can visualise it in:\n",
    "    http://projector.tensorflow.org/?config=https://wikipedia2vec.github.io/projector_files/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
