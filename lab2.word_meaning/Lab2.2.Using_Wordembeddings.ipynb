{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB2.2: Word embeddings for different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce you to word embeddings. Word embeddings are vector representations for words learned by a neural network to predict words that occur in their context. The weights applied to the hidden layer to make the correct predictions form the vector that acts as a representation for the meaning of the word. Usually, vector sizes are limited to 300 to 500 dimensions.\n",
    "\n",
    "For learning these representations a large corpus of text is needed with sufficient occurrences of words in different contexts. As negative examples of words that do not occur in the context, random words are chosen.\n",
    "\n",
    "One of the advantages is that comparing vectors of words always gives a result for these certain dimensions: i.e. the vectors are dense vectors. Vectors match most strongly when words occur in similar contexts. Obviously, words that do not have a representation, such as domain specific terminology, cannot be matched either.\n",
    "\n",
    "Although there are many packages and data sets with embeddings, we focus on publicly available and trainable embeddings, especially for multiple languages. Concretely, we will use embeddings from:\n",
    "\n",
    "* [Wikipedia2Vec](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/)\n",
    "* [Fasttext](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md)\n",
    "\n",
    "Wikipedia2Vec created word embeddings for 9 languages built from the Wikipedia pages in these languages. Fasttext, created by Facebook, created embeddings for 157 languages from web data collected by the [Common Crawl project](https://commoncrawl.org).\n",
    "\n",
    "Although the embeddings are created through different Pythn packages, they are also available in a common Word2Vec text format. This makes it possible to load the embeddings regardless of how they have been created by the same package. We will use the *GenSim* package to load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Gensim: a package for handling embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is an open source and free python package that is used a lot for using and building word embeddings:\n",
    "\n",
    "* https://radimrehurek.com/gensim/#\n",
    "\n",
    "We will use Gensim to load embeddings built by different research groups in different ways but also to built you own word embedding model. You need to install Gensim version 4 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Gensim on your local machine from the command line do:\n",
    "\n",
    "    * conda install -c conda-forge gensim\n",
    "    \n",
    "    OR\n",
    "    \n",
    "    * pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/piek/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - gensim\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    gensim-4.1.2               |   py39he9d5cce_0        18.3 MB\n",
      "    llvmlite-0.38.0            |   py39h8346a28_0         236 KB\n",
      "    numba-0.55.1               |   py39hae1ba45_0         3.4 MB\n",
      "    numpy-1.21.5               |   py39h2e5f0a9_3          11 KB\n",
      "    numpy-base-1.21.5          |   py39h3b1a694_3         4.7 MB\n",
      "    openssl-1.1.1q             |       hfe4f2af_0         1.9 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        28.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  gensim             pkgs/main/osx-64::gensim-4.1.2-py39he9d5cce_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  llvmlite                            0.37.0-py39he4411ff_1 --> 0.38.0-py39h8346a28_0\n",
      "  numba                               0.54.1-py39hae1ba45_0 --> 0.55.1-py39hae1ba45_0\n",
      "  numpy                               1.20.3-py39h4b4dc7a_0 --> 1.21.5-py39h2e5f0a9_3\n",
      "  numpy-base                          1.20.3-py39he0bd621_0 --> 1.21.5-py39h3b1a694_3\n",
      "  openssl                                 1.1.1p-hfe4f2af_0 --> 1.1.1q-hfe4f2af_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "numpy-1.21.5         | 11 KB     | ##################################### | 100% \n",
      "numba-0.55.1         | 3.4 MB    | ##################################### | 100% \n",
      "llvmlite-0.38.0      | 236 KB    | ##################################### | 100% \n",
      "numpy-base-1.21.5    | 4.7 MB    | ##################################### | 100% \n",
      "openssl-1.1.1q       | 1.9 MB    | ##################################### | 100% \n",
      "gensim-4.1.2         | 18.3 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If gensim is succefully installed you can use existing models models in text (txt) format that are compatable with gensim. To do so, we first import gensim and the package *numpy*, which is often used to represent vectors and arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word embeddings built from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is often used to build and test language models. The reasons are obvious: it is large, freely available and there are Wikipedias in many languages, often also linked to each other and partly covering similar content.\n",
    "\n",
    "You can download pre-trained models in various languages from: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "There are different variants trained for 100 and 300 dimensions. If your computer has limited capacity, it is better to start with the 100 dimensions. For this notebook, we will download enwiki_20180420_100d.txt.bz2, which is a compressed version of the 100 dimensions embeddings model built from the English Wikipedia. You need to decompress the \"txt.bz2\" file to a file with the extension \".txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the path to your local download of an embedding model. Make sure it is decompressed. The *.bz2 file will not load.\n",
    "\n",
    "We can use the command line commands *cat* and *more* to inspect the beginning of the text file to see what it contains.\n",
    "Please make sure you specify the path to your own download and do not forget to add \" | more\" to the end, \n",
    "otherwise it will list a gigabyte large file in your notebook. We only want to inspect the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4530030 100\n",
      "the -0.0811 0.5145 0.0368 0.0544 -0.0662 0.2121 0.1636 -0.1011 0.0607 -0.1858 -0.1965 -0.0595 0.0703 -0.1013 0.3130 -0.1717 0.0847 -0.1222 0.1024 0.0753 -0.1384 0.0435 -0.0371 0.1932 -0.1226 -0.2227 -0.1530 -0.2890 0.2371 0.2699 0.2693 -0.1666 0.0240 0.1053 -0.1475 -0.3232 0.0236 -0.2056 0.2847 -0.2817 0.1197 0.0314 -0.1215 0.0782 -0.2850 -0.1316 -0.0844 0.1483 -0.2192 -0.0462 -0.2151 0.0582 0.0372 0.0127 -0.3074 -0.1582 -0.1393 0.0361 -0.2519 -0.0305 -0.1532 -0.0286 -0.0955 0.3037 0.5632 -0.1120 -0.0319 -0.2223 -0.2612 -0.2254 -0.1593 0.1807 0.1205 0.3695 -0.2652 -0.0490 -0.2556 0.0130 -0.0898 0.0322 0.0021 -0.2692 0.3129 0.0179 0.3913 0.5415 -0.0049 0.0884 0.1605 0.0878 0.0004 0.1465 0.1872 0.0521 -0.1492 -0.0882 0.1696 0.1894 -0.0866 0.1184\n",
      "in 0.1245 0.4200 0.2936 0.0924 -0.0669 0.0252 0.1407 -0.0729 0.0680 -0.2951 -0.2720 0.0785 0.0780 0.0248 0.0427 -0.1497 0.1013 -0.0257 0.0364 0.2647 0.0330 0.1047 0.0382 0.0138 -0.0162 -0.0733 0.0960 -0.2090 0.0561 0.1030 0.2898 -0.1914 -0.0927 0.1237 -0.0023 -0.4792 0.0523 -0.0819 0.3551 -0.2274 0.3301 0.0547 -0.1707 0.2304 -0.2599 -0.1389 -0.0106 0.1921 -0.3615 0.0077 -0.2439 0.1056 -0.0010 -0.2522 -0.2321 -0.1604 -0.2652 -0.0134 -0.3000 0.1215 0.0737 0.0215 -0.1647 0.2799 0.5886 -0.0189 -0.1250 -0.2438 -0.1621 -0.3960 -0.1078 0.2162 -0.1173 0.6267 -0.0788 0.0086 -0.1317 0.1440 -0.2035 -0.0742 -0.0536 -0.2773 0.1026 0.0356 0.3384 0.4606 -0.1893 0.0142 0.2269 0.1136 0.0962 0.1648 0.1065 0.0051 -0.1777 -0.0971 0.3434 0.2423 -0.1432 -0.1808\n",
      "of 0.0018 0.3950 0.0662 0.1279 -0.2606 0.3275 -0.0161 -0.1740 0.0530 -0.1168 -0.1781 0.0897 0.0423 -0.1886 0.2358 -0.1118 0.1510 -0.2601 0.0367 -0.0689 -0.1016 :\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat /Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.txt | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *cat* command remains active untill you stop the cell in the notebook (see the ```[*]``` before the cell). Please stop running the cell manually (the square next to the play symbol in the top menu of the notebook) to proceed.\n",
    "\n",
    "If things worked out you see the beginning of the embedding text file. It starts with two digits on the first line.\n",
    "In my case this is \"4530030\" and \"100\". The first is the length of the vocabulary and the second the number of dimensions.\n",
    "\n",
    "The next line shows the word \"the\" followed by 100 digits. The digits represent the embedding values for \"the\". The rest of the file consists of lines for each word with its embeddings. This is the so-called word2vec format for representing word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the package used, models may be stored in different (and sometimes confusing) formats, but they all boil down to these components:\n",
    "\n",
    "* a matrix of word vectors \n",
    "* a vocabulary\n",
    "* a mapping between vectors in the matrix to the words in the vocabulary (often via indices)\n",
    "\n",
    "Think about what a matrix is (no not the movie). You know that a vector is a list of digits, such that each digit is a value for a dimension in an n-dimensional space. Well, if you have a list of these vectors you have a matrix of n-columns and m-rows. Each row corresponds to the vector of a word in the vocabulary.\n",
    "\n",
    "The matrix of 3 rows and 3 columns\n",
    "```\n",
    "[[.34, .56, ,12],\n",
    " [.12, .39, ,05],\n",
    " [.78, .37, ,01]]\n",
    "```\n",
    "\n",
    "The vocabulary with the word as a key and the matrix list index that points to the row with the embedding for the word:\n",
    "\n",
    "```{\"dog\": 0, \"cat\" : 1, \"car\" : 2}```\n",
    "\n",
    "For this data, a simple lookup function for *dog* will give the embedding *[.34, .56, ,12]*.\n",
    "\n",
    "Now let's see how this is implemented in GenSim for the Wikipedia derived word embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading a model in Gensim\n",
    "\n",
    "We will now load this file using the gensim class \"KeyedVectors\", which has a function \"load_word2vec_format\" that can load such a text file. As a parameter, you give it the path to your local file. As an additional parameter you can set a limit for how many words should be read. If you have limited memory on your computer it is wise to set a limit. Note that less words will be included in the loaded model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the local copy of a model built from wikipedia\n",
    "MODEL_FILE='/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.txt'\n",
    "\n",
    "### If you have a small computer you may want to limit the number of embeddings loaded as shown below:\n",
    "## wiki2vec = KeyedVectors.load_word2vec_format(MODEL_FILE, limit=5000) \n",
    "\n",
    "### To load the full model you should drop the limit.\n",
    "# Loading the full model can take a while.\n",
    "wiki2vec = KeyedVectors.load_word2vec_format(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type of gensim object we created by loading the data and what the propepties and functions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_for_all', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "print(type(wiki2vec))\n",
    "print(dir(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \"wiki2vec\" is an instance of a KeyedVectors object.  A KeyedVectors objects is essentially a mapping between keys and vectors. Each vector is identified by its lookup key, most often a short string token.\n",
    "\n",
    "One of the functions is the \"load_word2vec_format\" function we used, but we also see interesting functions such as: 'get_vector', 'cosine_similarities', 'distance', 'most_similar', 'vocab', etc. We will look into a few of them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a dictionary that contains words. Let's check how big the vocabulary is of the model derived from English Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 100\n",
      "Vocabulary size = 4530030 4530030\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', wiki2vec.vector_size)\n",
    "print('Vocabulary size =', len(wiki2vec.key_to_index), len(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four-and-a-haf millions words are present in this model. That is a lot more than in the English WordNet. Let's check if the word \"man\" is in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a man?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a man?\")\n",
    "\"man\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlikely that there is a cultural specific Dutch word such as \"tjalk\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a tjalk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a tjalk\")\n",
    "\"tjalk\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is rich, so this one is in there too. Let's go wild:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a tjalk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a tjalk\")\n",
    "\"Is there a tjalk\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, whole sentences are not in there. We can also get the vocabulary as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model vocabulary is represented internally as a...\n",
      "<class 'dict'>\n",
      "['the', 'in', 'of', 'a', 'and', 'is', 'to', 'was', 'by', 'for', 'on', 'as', 'at', 'from', 'with', 'an', 'it', 'that', 'also', 'which', 'first', 'this', 'has', 'he', 'one', 'his', 'are', 'after', 'who', 'were', 'two', 'its', 'new', 'be', 'or', 'but', 'had', 'their', 'been', 'born', 'not', 'other', 'all', 'have', 'during', 'time', 'when', 'may', 'they', 'into']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = wiki2vec.key_to_index\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(vocabulary))\n",
    "print((list(vocabulary))[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  The embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the embedding representation for a specific word that is in the vocabulary using the get_vector function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information stored in the vocabulary for the word \"man\":\n",
      "<class 'numpy.ndarray'>\n",
      "Distributional meaning of \"man\" in Wikipedia: [-2.377e-01  2.686e-01 -9.620e-02  2.707e-01 -2.241e-01 -2.489e-01\n",
      "  1.065e-01  4.120e-02 -5.349e-01 -1.445e-01 -8.700e-02 -1.877e-01\n",
      "  1.985e-01 -1.643e-01  1.021e-01 -1.783e-01 -5.520e-02  2.190e-02\n",
      " -2.180e-01  1.569e-01 -2.835e-01 -3.299e-01 -6.780e-02  3.505e-01\n",
      " -3.241e-01 -9.000e-04 -1.234e-01 -3.452e-01 -4.523e-01  7.449e-01\n",
      "  1.470e-01 -1.258e-01 -1.073e-01  4.019e-01  1.120e-01  2.230e-02\n",
      " -3.720e-01  2.026e-01  3.160e-02  2.910e-02 -2.406e-01  1.368e-01\n",
      " -1.750e-02  1.020e-01  8.340e-02  5.012e-01 -3.973e-01  4.010e-02\n",
      " -1.653e-01 -1.892e-01 -1.441e-01  6.290e-02 -5.185e-01 -2.638e-01\n",
      "  3.170e-02 -6.030e-02  1.012e-01 -5.408e-01 -3.528e-01 -1.281e-01\n",
      " -2.617e-01 -2.607e-01 -9.150e-02  3.094e-01  4.468e-01 -2.526e-01\n",
      " -2.842e-01 -9.303e-01 -3.270e-02 -4.669e-01  4.064e-01  2.045e-01\n",
      "  2.223e-01  2.501e-01 -4.577e-01  4.089e-01  1.261e-01 -2.000e-01\n",
      " -8.700e-02 -2.701e-01  3.453e-01  7.210e-02  1.277e-01 -2.570e-02\n",
      "  2.319e-01  3.712e-01  5.400e-02  1.502e-01  2.973e-01  7.630e-02\n",
      " -8.790e-02  2.386e-01  3.562e-01 -6.440e-02  2.086e-01 -3.865e-01\n",
      " -1.940e-02  5.608e-01 -6.439e-01  2.104e-01]\n"
     ]
    }
   ],
   "source": [
    "print('Information stored in the vocabulary for the word \"man\":')\n",
    "man_vector=wiki2vec.get_vector('man')\n",
    "print(type(man_vector))\n",
    "print('Distributional meaning of \"man\" in Wikipedia:', man_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get exotic numbers for 100 dimensions. What else did you expect? \n",
    "\n",
    "A vector is a sorted bunch of numbers, each representing a dimension. These numbers are actually the weights learned by the neural network that are applied to the hidden layer when learning to predict the context words of 'man'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data object is of the type 'numpy.ndarray'. Numpy is a package for dealing with numerical data that is used a lot in machine learning. For those interested, here is the description of what it is:\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many dimensions do we have in this vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy data shape (100,)\n",
      "Number of vector dimensions: 100\n"
     ]
    }
   ],
   "source": [
    "print('Numpy data shape', man_vector.shape)\n",
    "print('Number of vector dimensions:', len(man_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a surprise: we loaded a model with 100 dimensions based on a hidden layer with 100 neurons. This is true for all words so also for 'dog'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector4dog = wiki2vec['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector4dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013   0.6476  0.1045 -0.3117  0.1475  0.0809 -0.1523  0.265  -0.6462\n",
      " -0.2058  0.1564 -0.2072  0.4179  0.0386 -0.0194 -0.2241  0.2227 -0.3452\n",
      " -0.426   0.1028 -0.2136 -0.0267  0.1946  0.3652 -0.2265 -0.2736  0.0326\n",
      " -0.0279 -0.2359  0.5077  0.3759 -0.2207 -0.0506  0.7909  0.1344 -0.079\n",
      " -0.4099  0.1559 -0.0066  0.1236 -0.5474 -0.0877 -0.3738 -0.253  -0.4688\n",
      " -0.1184 -0.0501  0.3267 -0.1799 -0.2662  0.0968  0.2891 -0.4816 -0.3374\n",
      "  0.2488  0.1744  0.0889 -0.1873 -0.3312 -0.1903  0.0547 -0.6149 -0.427\n",
      " -0.1079  0.137  -0.1445  0.0521 -0.5711 -0.3859 -0.6626  0.2417 -0.0141\n",
      "  0.3974  0.1331 -0.6726 -0.227   0.1793  0.2454  0.1545 -0.0923 -0.0247\n",
      " -0.4611 -0.1317 -0.2194  0.2143  0.491  -0.2186  0.2463  0.0843  0.1324\n",
      " -0.4565  0.008   0.6242 -0.0217 -0.084  -0.4722  0.1191  0.3299 -0.9191\n",
      "  0.0963]\n"
     ]
    }
   ],
   "source": [
    "print(vector4dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the representations are compatible across the words, we can compare two vector representations through the cosine similarity function:\n",
    "\n",
    "![Cosine similarity](./images/cosine-full.png \"Logo Title Text 1\")\n",
    "\n",
    "So suppose we have two vectors A and B, each with 100 slots, this formula (taken from the Wikipedia page) tells you to sum the results of multiplying each slot across A and B:\n",
    "\n",
    "A[0]\\*B[0]+A[1]\\*B[1]+....A[99]\\*B[99]\n",
    "\n",
    "We divide this sum by the square-root of the total sum of the slots of A, multiplied by the square-root of the total sum of the slots of B. Dividing it that way normalises the value between 0 and 1 and it makes the sum of the products of the numerator relative to the product of the sums of the individual vectors.\n",
    "\n",
    "Embedding software uses such measures to obtain the most similar words. We can now use the *most_similar_by_word* function from Gensim to ask for the words that are most similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dogs', 0.8637314438819885), ('cat', 0.8286387324333191), ('puppy', 0.8150733709335327), ('rabbit', 0.8042253255844116), ('montarges', 0.7981182932853699), ('poodle', 0.7949738502502441), ('barfy', 0.7915431261062622), ('cockapoo', 0.7834540009498596), ('pekapoos', 0.7828510999679565), ('pollicle', 0.7826345562934875)]\n"
     ]
    }
   ],
   "source": [
    "dog_sim = wiki2vec.similar_by_word(\"dog\", topn=10)\n",
    "print(dog_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7570434\n",
      "0.25811052\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.similarity(\"king\", \"queen\"))\n",
    "print(wiki2vec.similarity(\"king\", \"coffee\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('suv', 0.8843088746070862), ('cars', 0.848658561706543), ('ENTITY/Minivan', 0.8475295305252075), ('truck', 0.835063099861145), ('suvs', 0.8163775205612183)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.830649197101593), ('monarch', 0.7416260838508606), ('ENTITY/Queen_consort', 0.7348716259002686), ('laungshe', 0.7347308993339539), ('regnant', 0.7243735790252686), ('chelna', 0.7236213088035583), ('consort', 0.720160722732544), ('indlovukati', 0.71815425157547), ('kamamalu', 0.7178552150726318), ('indlovukazi', 0.714848518371582)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Word Embeddings from Facebook's Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research lab of Facebook, created embeddings in 157 languages from Common Crawl. Common Crawl is a project that scrapes the web for texts in as many languages that are present. Check out the Github of Fasttext and Common Crawl for more details:\n",
    "\n",
    "* https://commoncrawl.org/the-data/get-started/\n",
    "* https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\n",
    "\n",
    "You should download the text version of the embeddings, which are following the same word2vec format as the Wike2Vec models.\n",
    "I downloaded the embeddings for Frysian and Limburgish, which are two local, regional languages. We can inpsect these two text files in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526167 300\n",
      ". 0.0139 -0.0210 -0.0328 -0.0229 0.2410 -0.0260 0.0071 -0.0160 0.0308 0.1494 0.0172 0.0276 -0.0017 0.0205 -0.0072 -0.0089 -0.0154 -0.0288 0.0217 -0.0085 0.0114 0.0119 -0.0284 -0.0170 0.0035 -0.0065 0.0191 -0.0085 0.0190 0.0197 0.0497 -0.0014 0.0060 -0.0069 0.0119 0.0168 0.0049 -0.0049 -0.0081 0.0194 0.0187 -0.0013 -0.0073 0.0139 0.0211 0.0138 -0.0148 0.0042 0.0004 0.0085 -0.0077 -0.0274 -0.0114 -0.0073 -0.0042 0.0038 0.0009 0.1392 0.0047 -0.0014 -0.0010 0.0031 -0.0005 -0.0238 -0.0040 0.0103 0.0276 0.0052 -0.0244 0.0189 0.0208 0.0128 0.0100 -0.0016 0.0019 0.0015 0.0308 -0.0299 -0.0125 0.0094 0.0671 0.0361 -0.0079 -0.0272 0.0130 -0.0070 -0.0004 0.0052 -0.0046 -0.0036 -0.0083 0.0083 0.0192 -0.0097 0.0201 -0.0088 -0.0143 0.0176 -0.0161 -0.0087 0.0108 0.0016 -0.0036 0.0272 0.0440 -0.0102 0.0194 0.0037 0.0198 -0.0054 -0.0010 -0.0030 0.0221 -0.0367 0.0004 0.0175 0.0520 -0.0069 -0.0036 0.0293 -0.0043 -0.0152 -0.0093 0.0255 0.0003 -0.0035 0.0348 0.0176 -0.0236 0.0035 0.0583 -0.0857 -0.0101 0.0213 0.0190 0.0008 -0.0181 0.0081 0.0209 0.0074 -0.0044 -0.0183 -0.0441 -0.0121 -0.0296 -0.0273 -0.0183 -0.0292 -0.0293 0.0112 -0.0128 -0.0079 0.0020 0.0033 -0.0029 0.0103 -0.0195 0.0056 0.0107 -0.0087 -0.0199 0.0067 -0.0024 0.0161 0.0111 -0.0433 -0.0014 -0.0258 -0.0063 -0.0218 -0.1612 -0.0027 0.0227 0.0085 -0.0051 -0.0226 -0.0177 0.0192 0.0071 -0.0080 -0.0211 -0.0169 0.0163 0.1091 0.0094 0.0165 -0.0095 0.0034 -0.0841 0.0296 -0.0232 0.0172 0.0243 -0.1293 -0.0063 -0.0101 -0.0104 -0.0151 0.0037 -0.0299 -0.0078 -0.0068 0.0014 0.2565 -0.0084 -0.0145 0.0246 0.0013 -0.0019 -0.0284 -0.0003 -0.0118 -0.0512 -0.0023 -0.0297 0.0337 0.0002 0.0059 -0.0148 -0.0421 -0.0625 0.0101 -0.0099 0.0114 -0.0115 0.0051 0.0757 -0.0320 0.0172 0.0067 -0.0087 -0.0019 -0.0144 0.020:\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat  \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.fy.300.vec\" | more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306222 300\n",
      "' -0.0150 -0.0400 0.0501 -0.0717 -0.0137 0.0211 0.0416 0.0129 0.0141 0.0216 0.0250 0.0593 -0.0445 0.0193 -0.0003 -0.0169 0.0217 0.0115 -0.0477 -0.0079 -0.0130 0.0283 -0.0274 0.0024 -0.0774 -0.0215 -0.0345 -0.0077 0.0146 -0.0027 -0.0087 0.0011 -0.0105 0.1293 -0.0385 0.0089 -0.0080 -0.0460 0.0075 0.0241 0.0128 0.0006 0.0730 -0.0214 0.1288 0.0327 -0.0051 -0.0403 0.0103 0.0162 -0.0266 -0.0041 0.0075 -0.0028 0.0516 -0.0068 0.0191 -0.0159 0.0393 0.0053 -0.0347 0.0049 0.0063 -0.0582 0.0186 -0.0151 -0.0245 0.0054 0.0352 0.0268 0.0313 0.0166 -0.0037 0.0444 -0.0140 0.0595 0.0168 0.0309 -0.0075 -0.0360 0.0133 -0.0097 0.0309 -0.0339 -0.0262 -0.0637 -0.0074 -0.0029 0.0032 0.0123 -0.0102 -0.0100 -0.0129 -0.0051 -0.0047 -0.0834 -0.0273 -0.0322 -0.0355 -0.0005 -0.0230 0.0571 -0.0147 0.0053 -0.0271 0.0189 -0.0605 0.0250 0.0149 0.0120 -0.0225 -0.0752 -0.0751 0.0189 -0.0105 0.0276 -0.0156 -0.0079 -0.0502 -0.0214 0.0635 0.0127 -0.0148 0.0152 0.0049 -0.0183 0.0351 0.0391 -0.0167 0.0224 0.0248 -0.0057 0.0018 -0.0275 -0.0358 -0.0414 -0.0128 -0.0205 0.0243 0.0138 0.0054 0.0111 -0.0123 -0.0116 -0.0046 0.0290 0.0029 -0.0058 -0.0016 0.0405 0.0277 -0.0029 0.0273 0.0355 -0.0493 -0.0530 0.0177 -0.0318 0.0164 -0.0108 -0.0047 -0.0199 0.0486 -0.0400 0.0238 -0.0121 0.0295 -0.0336 0.0108 -0.0083 -0.0202 -0.0259 -0.0153 -0.0088 -0.0055 0.0055 -0.0043 0.0274 0.0283 -0.0289 -0.0250 -0.0155 0.0300 -0.0481 0.0283 0.0130 0.0679 0.0101 0.0094 -0.0045 0.0328 0.0098 0.0048 -0.0860 0.0593 -0.0411 -0.0361 -0.0155 -0.0471 -0.0191 0.0564 -0.0064 0.0352 -0.0554 0.0229 -0.0149 0.0288 -0.0028 0.0211 0.0071 0.0030 -0.0602 -0.0536 0.0706 -0.0212 0.0224 -0.0222 -0.0491 0.0469 -0.0137 0.0162 0.0087 -0.0149 -0.0094 0.0040 -0.0016 -0.0392 -0.0383 -0.0710 0.0058 0.0040 0.1177 -0.0161 0.0:\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat  \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.li.300.vec\" | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frysian model has 526167 words and the Limburgish model as 306222 words. Let's load the models and inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_my_fasttext_model = \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.fy.300.vec\"\n",
    "fasttext_fy = KeyedVectors.load_word2vec_format(path_to_my_fasttext_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 300\n",
      "Vocabulary size = 526167 526167\n",
      "Word index for \"tjalk\" 141358\n",
      "Vecotr for \"tjalk\" [-2.80e-02 -6.10e-03 -3.38e-02 -3.78e-02 -1.51e-02 -4.40e-03 -1.70e-03\n",
      " -3.31e-02  5.80e-03 -2.07e-02 -8.60e-03  2.24e-02  2.50e-03  1.31e-02\n",
      " -1.28e-02 -1.37e-02 -2.20e-02 -2.49e-02 -3.70e-03 -3.46e-02 -9.40e-03\n",
      "  3.30e-02  2.13e-02 -9.80e-03  3.20e-03 -1.81e-02  3.38e-02 -3.87e-02\n",
      " -1.57e-02 -1.38e-02  2.29e-02  5.40e-03  1.81e-02  9.30e-03 -2.04e-02\n",
      "  2.55e-02 -1.91e-02  6.40e-03 -8.00e-04  5.83e-02  1.73e-02  1.26e-02\n",
      "  1.97e-02  4.30e-03 -1.90e-03  1.56e-02  1.08e-02 -3.00e-03  8.40e-03\n",
      " -4.40e-03  2.30e-02  8.10e-03 -9.00e-04  8.80e-03  7.50e-03  3.79e-02\n",
      " -2.03e-02 -1.14e-02 -4.22e-02  5.50e-03  2.41e-02 -2.16e-02 -3.00e-03\n",
      "  1.38e-02  2.00e-03 -2.12e-02  2.50e-03 -1.04e-02 -1.48e-02 -3.29e-02\n",
      "  1.56e-02 -1.04e-02 -1.52e-02  7.40e-03  5.05e-02  5.80e-03  1.09e-02\n",
      "  1.77e-02 -2.80e-03 -5.40e-03 -1.15e-02  1.88e-02 -7.40e-03  5.10e-03\n",
      " -7.50e-03 -1.08e-02  1.90e-03 -1.69e-02 -2.59e-02 -2.00e-04  1.33e-02\n",
      " -1.45e-02 -1.20e-02  7.00e-04 -1.25e-02 -1.00e-03 -5.10e-03 -1.00e-04\n",
      "  8.00e-04  1.90e-02 -2.53e-02 -5.60e-03  3.90e-03 -8.60e-03  6.60e-03\n",
      " -7.80e-03  7.60e-03 -2.16e-02  3.26e-02  9.00e-04 -2.86e-02  5.90e-03\n",
      "  2.42e-02  5.70e-03  7.90e-03 -1.90e-03  4.70e-03  2.42e-02  1.34e-02\n",
      " -2.47e-02  4.00e-03  1.70e-03 -8.90e-03 -5.00e-04 -2.20e-03 -1.90e-03\n",
      " -6.70e-03  1.78e-02  1.06e-02 -3.48e-02 -6.63e-02 -2.00e-04  1.92e-02\n",
      " -3.08e-02  7.30e-03 -2.00e-02 -2.32e-02 -1.14e-02  1.00e-04 -7.60e-03\n",
      "  7.30e-03  0.00e+00  5.40e-03  8.80e-03 -2.80e-03  1.89e-02  2.37e-02\n",
      " -2.98e-02  2.50e-03  1.90e-03 -2.70e-02 -2.60e-03  2.39e-02 -1.43e-02\n",
      " -2.87e-02 -1.92e-02 -1.59e-02  1.08e-02 -2.00e-03 -5.70e-03  3.12e-02\n",
      " -1.40e-03  1.94e-02  2.00e-04  2.13e-02 -1.35e-02  1.68e-02 -9.50e-03\n",
      "  3.68e-02 -2.23e-02  8.08e-02  1.15e-02  1.88e-02  1.89e-02  1.32e-02\n",
      "  1.45e-02 -1.45e-02 -2.07e-02  2.77e-02  5.90e-03 -1.35e-02  2.47e-02\n",
      " -2.02e-02 -2.29e-02 -1.39e-02 -3.14e-02  1.82e-02 -1.87e-02  1.69e-02\n",
      "  2.79e-02  9.00e-03  3.60e-02 -3.80e-03 -1.00e-04 -1.92e-02  1.81e-02\n",
      "  1.33e-02 -2.11e-02 -3.16e-02 -2.03e-02 -9.00e-04  1.14e-02  1.04e-02\n",
      " -7.06e-02  1.71e-02 -1.05e-02 -2.01e-02  1.07e-02  6.50e-03  2.60e-03\n",
      " -6.40e-03 -1.00e-04  2.30e-03 -3.40e-03  5.80e-03  5.90e-03  6.50e-03\n",
      "  1.83e-02 -2.73e-02  1.91e-02  4.00e-04  5.40e-03  2.24e-02 -1.40e-03\n",
      "  2.58e-02 -1.77e-02  1.87e-02  2.51e-02  1.51e-02 -6.00e-04 -2.50e-03\n",
      "  1.70e-03 -6.40e-03  1.27e-02 -1.41e-02  2.22e-02 -3.30e-03 -1.91e-02\n",
      " -3.92e-02  8.80e-03 -1.10e-02 -2.52e-02  1.86e-02 -7.50e-03 -3.30e-03\n",
      " -6.80e-03 -1.77e-02 -9.70e-03 -1.11e-02  2.04e-02 -1.41e-02  3.30e-03\n",
      "  4.20e-02  4.02e-02 -2.00e-02 -1.49e-02 -4.10e-03 -1.31e-02 -3.70e-03\n",
      "  3.04e-02 -2.40e-02 -8.40e-03  1.61e-02 -4.59e-02 -1.04e-02  2.55e-02\n",
      "  1.08e-02  1.71e-02  2.70e-02 -7.20e-03 -8.50e-03 -2.80e-02 -8.00e-04\n",
      "  5.00e-04 -1.95e-02  2.48e-02 -9.70e-03 -3.79e-02  2.86e-02 -1.30e-02\n",
      "  3.50e-03  3.08e-02 -2.77e-02 -9.60e-03  2.72e-02 -2.30e-03  2.30e-03\n",
      " -1.00e-04 -1.81e-02  2.80e-03 -4.13e-02  3.52e-02  2.42e-02 -2.50e-03\n",
      " -1.13e-02  5.70e-03  3.10e-03  9.60e-03  3.60e-02 -2.77e-02]\n",
      "Most similar tjalk [('Koftjalk', 0.7346436381340027), ('tsjalk', 0.62446129322052), ('Dektsjalk', 0.5733082294464111), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.519596517086029), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "similar_by_word tjalk [('Koftjalk', 0.7346436381340027), ('tsjalk', 0.62446129322052), ('Dektsjalk', 0.5733082294464111), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.519596517086029), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "similar_by_vector tjalk [('Koftjalk', 0.7346436381340027), ('tsjalk', 0.62446129322052), ('Dektsjalk', 0.5733082294464111), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.519596517086029), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "doesnt_match ['sneek', 'joure', 'tjalk'] tjalk\n",
      "\n",
      "similarity tjalk sneek 0.35242274\n",
      "\n",
      "n_similarity ['sneek', 'joure', 'tjalk'] ['aap', 'noot', 'mies'] 0.3253808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', fasttext_fy.vector_size)\n",
    "print('Vocabulary size =', len(fasttext_fy.key_to_index), len(fasttext_fy))\n",
    "word_idx = fasttext_fy.key_to_index[\"tjalk\"]\n",
    "print('Word index for \\\"tjalk\\\"', word_idx)\n",
    "vector = fasttext_fy.get_vector(\"tjalk\")\n",
    "print('Vecotr for \\\"tjalk\\\"', vector)\n",
    "\n",
    "wordA=\"tjalk\"\n",
    "wordB=\"sneek\"\n",
    "wordlistA=[\"sneek\", \"joure\", \"tjalk\"]\n",
    "wordlistB=[\"aap\", \"noot\", \"mies\"]\n",
    "print(\"Most similar\", wordA, fasttext_fy.most_similar(wordA))  # üëç\n",
    "print()\n",
    "print(\"similar_by_word\", wordA, fasttext_fy.similar_by_word(wordA))  # üëç\n",
    "print()\n",
    "print(\"similar_by_vector\", wordA, fasttext_fy.similar_by_vector(wordA))  # üëç\n",
    "print()\n",
    "print(\"doesnt_match\", wordlistA, fasttext_fy.doesnt_match(wordlistA))  # üëç\n",
    "print()\n",
    "print(\"similarity\", wordA, wordB, fasttext_fy.similarity(wordA, wordB))  # üëç\n",
    "print()\n",
    "print(\"n_similarity\", wordlistA, wordlistB, fasttext_fy.n_similarity(wordlistA, wordlistB))  # üëç\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_my_fasttext_model = \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.li.300.vec\"\n",
    "fasttext_li = KeyedVectors.load_word2vec_format(path_to_my_fasttext_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 300\n",
      "Vocabulary size = 306222 306222\n",
      "Word index for \"stroat\" 240260\n",
      "Vector for \"stroat\" 240260\n",
      "[ 0.0136 -0.0423  0.0041  0.0326 -0.0135 -0.0034 -0.0137 -0.0132  0.0041\n",
      " -0.0108 -0.0007 -0.0127 -0.0194 -0.0115 -0.0003 -0.001  -0.0127 -0.0061\n",
      " -0.0295  0.0258  0.0062 -0.0114  0.0376 -0.0237  0.0093 -0.009   0.0071\n",
      " -0.015   0.0196 -0.0079 -0.0127 -0.0194  0.0105 -0.009   0.0105 -0.035\n",
      "  0.0254  0.0177 -0.0182 -0.0077 -0.0028  0.0133 -0.0063 -0.0225  0.0073\n",
      "  0.0186 -0.0089  0.0334  0.015  -0.0091 -0.0188  0.0133  0.0074  0.0185\n",
      "  0.009   0.0018  0.0021 -0.0166 -0.0023 -0.0148 -0.01   -0.0066  0.0169\n",
      " -0.0052 -0.0095 -0.0342 -0.0152  0.0082  0.0168  0.0042  0.0101  0.0535\n",
      "  0.0059 -0.0028 -0.0021  0.0243  0.0035 -0.0022  0.006  -0.0381 -0.0024\n",
      "  0.0085  0.0036 -0.003  -0.0064 -0.0116  0.0019  0.0009  0.0049  0.0268\n",
      "  0.0118  0.0047 -0.025  -0.0103 -0.0034  0.0168 -0.0158  0.0035 -0.0132\n",
      " -0.0107 -0.0058 -0.0013 -0.0123 -0.0176  0.0092  0.0178  0.0097 -0.0012\n",
      "  0.0371  0.0325 -0.0069  0.019   0.0336  0.004  -0.0067 -0.0036 -0.0057\n",
      " -0.0202  0.0366  0.0066 -0.0147  0.0099 -0.0064 -0.0088 -0.0014 -0.0194\n",
      " -0.0396  0.0277  0.0008  0.0073 -0.0039 -0.0063 -0.0141  0.0046 -0.0206\n",
      "  0.0228  0.043  -0.0133  0.0265 -0.0015  0.0016 -0.0217 -0.0035 -0.007\n",
      "  0.0114  0.0186 -0.0163  0.0091 -0.0236  0.0003 -0.002  -0.0034  0.0067\n",
      "  0.0115  0.0535  0.0137 -0.0014  0.0051  0.0034 -0.0041 -0.003   0.0268\n",
      "  0.0321 -0.0008 -0.0314  0.024   0.0094 -0.0164  0.0154  0.0046  0.0025\n",
      " -0.0333  0.0118  0.0071  0.0067  0.0211  0.0013 -0.0261 -0.0162  0.0512\n",
      "  0.0116 -0.0117  0.0003 -0.0174  0.0026  0.0093 -0.0159 -0.0243  0.0802\n",
      " -0.0066 -0.027   0.0254  0.      0.0226  0.038  -0.0082 -0.0007 -0.0116\n",
      "  0.0101  0.056   0.0059 -0.0154 -0.0055 -0.0058 -0.0024 -0.0037  0.0143\n",
      " -0.0052 -0.0059 -0.0196  0.0032 -0.0031  0.0186 -0.0177  0.0014  0.0035\n",
      "  0.0166  0.0032 -0.0599 -0.0019 -0.0058 -0.0161  0.011   0.0031  0.0094\n",
      " -0.0039 -0.002  -0.0192 -0.0071 -0.0136  0.0038 -0.0387  0.0141 -0.0023\n",
      " -0.0301 -0.013  -0.0037 -0.0134  0.0147 -0.0273 -0.0201  0.0113  0.0155\n",
      " -0.0145 -0.0098  0.0129  0.005  -0.0132 -0.0013 -0.0172  0.0036 -0.0076\n",
      "  0.0558 -0.0032 -0.004  -0.0182 -0.0261  0.0167  0.0192 -0.027   0.0093\n",
      "  0.0244 -0.0043  0.0113 -0.0005  0.0032  0.0212 -0.0039  0.0147 -0.0086\n",
      "  0.0146 -0.0112 -0.0205 -0.0086  0.001  -0.0126  0.0099 -0.0099  0.0458\n",
      " -0.0039 -0.0253 -0.0058  0.0075 -0.004  -0.0117  0.0023 -0.0342  0.0054\n",
      "  0.0656 -0.0128 -0.0183 -0.0117  0.0144  0.0027  0.0097 -0.0146 -0.0233\n",
      "  0.0021 -0.0089 -0.0194]\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', fasttext_li.vector_size)\n",
    "print('Vocabulary size =', len(fasttext_li.key_to_index), len(fasttext_li))\n",
    "word_idx = fasttext_li.key_to_index[\"stroat\"]\n",
    "print('Word index for \\\"stroat\\\"', word_idx)\n",
    "vector = fasttext_li.get_vector(\"stroat\")\n",
    "print('Vector for \\\"stroat\\\"', word_idx)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar stroat [('d√∂rpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460540890693665), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679943084717), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274199485778809), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.6118245124816895)]\n",
      "\n",
      "Most similar poet [('Hoemel', 0.863980233669281), ('foemel', 0.8294392228126526), ('loemelekriem√®rsj', 0.8049798607826233), ('Sjtoemel', 0.7726760506629944), ('dremel', 0.7404617071151733), ('Bieemel', 0.7326873540878296), ('loemele', 0.727192759513855), ('kloemele', 0.7036441564559937), ('tasemel', 0.6926780939102173), ('Loemel', 0.683782696723938)]\n",
      "\n",
      "Most similar loemel [('fine-boned', 0.5196540355682373), ('laureate', 0.4606165885925293), ('k√∂lsje', 0.4573061764240265), ('face', 0.4414013922214508), ('pipe', 0.3875519335269928), ('Nut.', 0.38502728939056396), ('Jenniches', 0.3829471468925476), ('gepoet', 0.3810572624206543), ('z√†√®n', 0.37500694394111633), ('walv√®sechtege', 0.37120863795280457)]\n",
      "\n",
      "similar_by_word stroat [('d√∂rpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460540890693665), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679943084717), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274199485778809), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.6118245124816895)]\n",
      "\n",
      "similar_by_vector stroat [('d√∂rpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460540890693665), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679943084717), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274199485778809), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.6118245124816895)]\n",
      "\n",
      "doesnt_match ['stroat', 'poet', 'loemel'] poet\n",
      "\n",
      "similarity stroat poet 0.054689743\n",
      "\n",
      "n_similarity ['stroat', 'poet', 'loemel'] ['aap', 'noot', 'mies'] 0.20698142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordA=\"stroat\"\n",
    "wordB=\"poet\"\n",
    "wordC=\"loemel\"\n",
    "wordlistA=[\"stroat\", \"poet\", \"loemel\"]\n",
    "wordlistB=[\"aap\", \"noot\", \"mies\"]\n",
    "print(\"Most similar\", wordA, fasttext_li.most_similar(wordA))  # üëç\n",
    "print()\n",
    "\n",
    "print(\"Most similar\", wordB, fasttext_li.most_similar(wordC))  # üëç\n",
    "print()\n",
    "\n",
    "print(\"Most similar\", wordC, fasttext_li.most_similar(wordB))  # üëç\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"similar_by_word\", wordA, fasttext_li.similar_by_word(wordA))  # üëç\n",
    "print()\n",
    "print(\"similar_by_vector\", wordA, fasttext_li.similar_by_vector(wordA))  # üëç\n",
    "print()\n",
    "print(\"doesnt_match\", wordlistA, fasttext_li.doesnt_match(wordlistA))  # üëç\n",
    "print()\n",
    "print(\"similarity\", wordA, wordB, fasttext_li.similarity(wordA, wordB))  # üëç\n",
    "print()\n",
    "print(\"n_similarity\", wordlistA, wordlistB, fasttext_li.n_similarity(wordlistA, wordlistB))  # üëç\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Links to existing models available for download\n",
    "\n",
    "Follow the links to browse available models. The sources listed below contain English models trained using different algorithms, data with different degrees of preprocessing and varying hyperparameter settings. Some resources also include models in other languages.\n",
    "\n",
    "### Large and commonly used models (English):\n",
    "\n",
    "* Google word2vec: can be downloaded from here (follow link in instructions): http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "* GloVe (trained on various corpora): https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "* FastText embeddings (Facebook): https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "* Models with different algorithms, hyperparamters, dimensions and degrees of preprocessing (e.g. dependency parsing windows):  https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html\n",
    "\n",
    "\n",
    "### Various models in English & other languages:\n",
    "\n",
    "* Various algorithms and parameters for English and other languages: http://vectors.nlpl.eu/repository/#\n",
    "\n",
    "### Cross lingual embeddings:\n",
    "* https://ruder.io/cross-lingual-embeddings/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
