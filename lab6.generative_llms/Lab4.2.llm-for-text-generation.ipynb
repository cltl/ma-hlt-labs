{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 Use generative Language Models as text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of you have used ChatGPT from OpenAI and noticed that it can respond to your input in a very natural way. ChatGPT is build on top of GPT (Generative Pretrained Transformer) which is a model trained to generate text given a preceding input, so-called prompt (Brown et al 2020). It can do this repetitively up to a certain length, likewise generating short stories.\n",
    "\n",
    "Another generative model is T5 (Text to Text Transfer Transformer, Raffel et al. 2019). T5 models many tasks as a text generation task, ranging from plain translation, sentiment annotation, question-answering, similarity, to summarisation. Tasks are differentiated through prompt prefixes.\n",
    "\n",
    "<img src=\"T5.gif\">\n",
    "\n",
    "Models such as GPT3,4 and T5, although having good performance, are by far too large model to work with locally. Therefore in this notebook, we will use Llama3 which is publicly available: https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "### References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n",
    "\n",
    "OpenAI, 2023. GPT-4 Technical Report. arXiv:2303.08774\n",
    "\n",
    "Llama: [The Llama 3 Herd of Models](https://scontent-ams2-1.xx.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=t6egZJ8QdI4Q7kNvgEka7Z4&_nc_ht=scontent-ams2-1.xx&oh=00_AYBPnjp-CQn7YnUQU_P-yJATmlaN6oRuEHZ1VrXshBoBwQ&oe=66A6EB8D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama3 client-server calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use Llama3 in the same way as we have seen for BERT and XLM-RoBERTa using a pipeline. This is however a bit more difficult because you need to obtain an access key from Meta and login through a Huggingface account. \n",
    "\n",
    "We are therefore going to make use of the server version of Llama as we did for our chat conversation at the beginning of the course. For this, we need to use the OpenAI client package again. If you have not yet installed it, you can i nstall the OpenAI client using the following command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.51.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from openai) (4.6.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.5.0-cp310-cp310-macosx_10_12_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: sniffio in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.51.0-py3-none-any.whl (383 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.5.0-cp310-cp310-macosx_10_12_x86_64.whl (284 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp310-cp310-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.5.0 openai-1.51.0 pydantic-2.9.2 pydantic-core-2.23.4\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the openAI package we can import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a client that connects to the server version, either locally or to the CLTL server that was given to you during the course. In the next cell, we show how to access a local server that runs on port 9001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://localhost:9001/v1\", api_key=\"not-needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The openAI client has various functions which are explained here: https://pypi.org/project/openai/\n",
    "\n",
    "We will use the ```chat``` module with the function ```completions.create```. This function needs several arguments:\n",
    "\n",
    "* model: this can be a local model or the name of one of the openAI models\n",
    "* messages: this is the input prompt\n",
    "* temperature: a value between 0 and 1 that defines how creative the model should be, 0 means best answer only and 1 a less liklely alternative maximally creative.\n",
    "* stream: this must be set to True so that we can get the complete answer from the server\n",
    "\n",
    "You can experiment with the temperature and the prompt to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt is given as a list of **dict** strings defining the role of the **system** (the model) and the input given by the **user**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Bach sat down at his organ and played\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "                model=\"local-model\", # this field is currently unused\n",
    "                messages=prompt,\n",
    "                temperature=0.3,\n",
    "                stream=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 alternative sentences:\n",
      "\n",
      "1. Bach sat down at his organ and played a soothing melody.\n",
      "2. Bach sat down at his organ and played with precision and passion.\n",
      "3. Bach sat down at his organ and played a complex fugue.\n",
      "4. Bach sat down at his organ and played a lively dance tune.\n",
      "5. Bach sat down at his organ and played a somber dirge.\n",
      "6. Bach sat down at his organ and played a beautiful chorale.\n",
      "7. Bach sat down at his organ and played with reckless abandon.\n",
      "8. Bach sat down at his organ and played a stately processional.\n",
      "9. Bach sat down at his organ and played a whimsical piece.\n",
      "10. Bach sat down at his organ and played a majestic cantata.\n"
     ]
    }
   ],
   "source": [
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        response += chunk.choices[0].delta.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to consider about these sentences. Are they all grammatical? Is the input modified and still semantically valid? Are the completions semantically correct? Do the completions make sense given that it is Bach playing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven sat down at his organ and played\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "                model=\"local-model\", # this field is currently unused\n",
    "                messages=prompt,\n",
    "                temperature=0.3,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        response += chunk.choices[0].delta.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do this all day, let us switch the language of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven ging achter zijn orgel zitten en speelde\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "                model=\"local-model\", # this field is currently unused\n",
    "                messages=prompt,\n",
    "                temperature=0.3,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        response += chunk.choices[0].delta.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions in English, completions as Dutch. You can also give the instructions in Dutch if you want or try other languages. Which languages are covered by Llama and how well? Interesing result is that Llama varied the sitting and in a few cases the organ (orgel) but never specified what Beethoven is playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Genereer 10 alternatieve zinnen door de volgende text af te maken.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven ging achter zijn orgel zitten en speelde\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "                model=\"local-model\", # this field is currently unused\n",
    "                messages=prompt,\n",
    "                temperature=0.3,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        response += chunk.choices[0].delta.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Dutch instructions do make a big difference as the quality went down drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use the same client to prompt openAI models but for that you need to obtain an API key from openAI and pass it in to the model. You can pass this key in the api_key parameter when creating a client and next make the same calls as we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "                \n",
    "# completion = client.chat.completions.create(\n",
    "#                 model=\"gpt-4o\",\n",
    "#                 messages=prompt,\n",
    "#                 temperature=0.3,\n",
    "#                 stream=True\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative LLMs sucha s ChatGPT, LLama, Mixtral have been finetuned for various tasks through alignment and possibly reinforcement learning on top of this. These are specific behaviours such as **chat**, **summarize**, **paraphrase**, **translate** or answer questions **Q&A**. You can evoke these functions through the instruction given, as we gave the instruction to complete a text. However, the models could also give useful responses to other instructions for which they were not specifically trained, e.g. to generate Python code, do calculations or a website. These emerging capabilities are fascinating but also risky since there is little control and/or training for this behaviour and therefore no guarantee that it provides the correct answers. So in case of code generation, it may cost more time to check and understand the generated code than to code yourself which also makes you a better coder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
