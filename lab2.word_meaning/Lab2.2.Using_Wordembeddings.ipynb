{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB2.2: Word embeddings for different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce you to word embeddings. Word embeddings are vector representations for words learned by a neural network to predict words that occur in their context. The weights applied to make predictions eventually form the vector that acts as a representation of a word. Usually, vector sizes are limited to 300 to 500 dimensions (weights).\n",
    "\n",
    "For learning these representations a large corpus of text is needed with sufficient occurrences of words in different contexts. Each context word counts as a correct or positive example of a word that needs to be predicted. As negative examples of words that do not occur in the context, random words are chosen.\n",
    "\n",
    "The learning consists of making the weights of the target word more similar to the weights of the correct context words and less similar to the incorrect (random) contexts words. The learning starts with a random initialisation of the weights but gradually the weights get updated after seeing more and more training cases. \n",
    "\n",
    "For example given the following texts, the words ```dog```, ```cat```, and ```mouse``` will get rather similar weights because they partially occur in each other contexts and partially share other words such as ```chase``` and  ```ate```:\n",
    "\n",
    "```The dog chased the cat.\n",
    "The cat chased the mouse.\n",
    "The mouse ate the cheese.\n",
    "The dog ate a bone.\n",
    "The cat ate the fish\n",
    "```\n",
    "\n",
    "They also differ a little bit because they eat different things. Imagine doing this for hundreds-of-thousands of documents and ten-thousands of words. It will create a large semantic space as we have seen for WordNet positioning words that are related in close proximity. The difference from WordNet being, that this space is derived empirically from texts and not from human judgements.\n",
    "\n",
    "![word-embedding](./images/word-embedding.png)\n",
    "\n",
    "Screen dump taken from: https://projector.tensorflow.org\n",
    "\n",
    "Two major disadvantages of word-embeddings are: 1) different meanings of words get conflated (e.g. mouse) and 2) the vectors represent ```relatedness``` and not the precise semantic relations from WordNet.\n",
    "\n",
    "One of the technical advantages of representing word meanings as vectors is that comparing vectors of words always gives a result for these certain dimensions: i.e. the vectors are dense vectors. Vectors match most strongly when words occur in similar contexts. Obviously, words that do not have a representation, such as domain specific terminology, cannot be matched either.\n",
    "\n",
    "**Reference**: Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems 26 (2013).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to load prebuilt word embedding and learn how to explore these. Although there are many packages and data sets with embeddings, we focus on publicly available and trainable embeddings, especially for multiple languages. Concretely, we will use embeddings from:\n",
    "\n",
    "* [Wikipedia2Vec](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/)\n",
    "* [Fasttext](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md)\n",
    "\n",
    "Wikipedia2Vec created word embeddings for 9 languages built from the Wikipedia pages in these languages. Fasttext, created by Facebook, created embeddings for 157 languages from web data collected by the [Common Crawl project](https://commoncrawl.org).\n",
    "\n",
    "The embeddings are created through different Pythn packages, but they are also available in a common Word2Vec text format. This makes it possible to load the embeddings regardless of how they have been created. We will use the *GenSim* package to load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Gensim: a package for handling embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is an open source and free python package that is often used for building and using word embeddings:\n",
    "\n",
    "* https://radimrehurek.com/gensim/#\n",
    "* https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here\n",
    "\n",
    "You need to install Gensim version 4 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Gensim on your local machine from the command line do:\n",
    "\n",
    "    * conda install -c conda-forge gensim\n",
    "    \n",
    "    OR\n",
    "    \n",
    "    * pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge gensim\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If gensim is succefully installed you can use existing models models in text (txt) format that are compatable with gensim. To do so, we first import gensim and the package *numpy*, which is often used to represent vectors and arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word embeddings built from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is often used to build and test language models. The reasons are obvious: it is large, freely available and there are Wikipedias in many languages, often also linked to each other and partly covering similar content.\n",
    "\n",
    "You can download pre-trained models in various languages from: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "There are different variants trained for 100, 300, and even 500 dimensions. If your computer has limited capacity, it is better to start with the 100 dimensions. For this notebook, we will download enwiki_20180420_100d.txt.bz2, which is a compressed version of the 100 dimensions embeddings model built from the English Wikipedia. You need to decompress the \"txt.bz2\" file to a file with the extension \".txt\", using a  file (de)compression utility that can handle \"bz2\" format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to peek into the file that you have downloaded. It is too big to load it in a text editor so we will try to print part of the file to the screen of the terminal.\n",
    "\n",
    "We can use the command line commands **cat** (use **type** on Windows) and **more** to inspect the beginning of the text file to see what it contains. Make sure it is decompressed.\n",
    "\n",
    "Specify the path to your own download after the command and do not forget to add \" | more\" to the end, \n",
    "otherwise it will list a gigabyte large file in your notebook. We only want to inspect the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4530030 100\n",
      "the -0.0811 0.5145 0.0368 0.0544 -0.0662 0.2121 0.1636 -0.1011 0.0607 -0.1858 -0.1965 -0.0595 0.0703 -0.1013 0.3130 -0.1717 0.0847 -0.1222 0.1024 0.0753 -0.1384 0.0435 -0.0371 0.1932 -0.1226 -0.2227 -0.1530 -0.2890 0.2371 0.2699 0.2693 -0.1666 0.0240 0.1053 -0.1475 -0.3232 0.0236 -0.2056 0.2847 -0.2817 0.1197 0.0314 -0.1215 0.0782 -0.2850 -0.1316 -0.0844 0.1483 -0.2192 -0.0462 -0.2151 0.0582 0.0372 0.0127 -0.3074 -0.1582 -0.1393 0.0361 -0.2519 -0.0305 -0.1532 -0.0286 -0.0955 0.3037 0.5632 -0.1120 -0.0319 -0.2223 -0.2612 -0.2254 -0.1593 0.1807 0.1205 0.3695 -0.2652 -0.0490 -0.2556 0.0130 -0.0898 0.0322 0.0021 -0.2692 0.3129 0.0179 0.3913 0.5415 -0.0049 0.0884 0.1605 0.0878 0.0004 0.1465 0.1872 0.0521 -0.1492 -0.0882 0.1696 0.1894 -0.0866 0.1184\n",
      "in 0.1245 0.4200 0.2936 0.0924 -0.0669 0.0252 0.1407 -0.0729 0.0680 -0.2951 -0.2720 0.0785 0.0780 0.0248 0.0427 -0.1497 0.1013 -0.0257 0.0364 0.2647 0.0330 0.1047 0.0382 0.0138 -0.0162 -0.0733 0.0960 -0.2090 0.0561 0.1030 0.2898 -0.1914 -0.0927 0.1237 -0.0023 -0.4792 0.0523 -0.0819 0.3551 -0.2274 0.3301 0.0547 -0.1707 0.2304 -0.2599 -0.1389 -0.0106 0.1921 -0.3615 0.0077 -0.2439 0.1056 -0.0010 -0.2522 -0.2321 -0.1604 -0.2652 -0.0134 -0.3000 0.1215 0.0737 0.0215 -0.1647 0.2799 0.5886 -0.0189 -0.1250 -0.2438 -0.1621 -0.3960 -0.1078 0.2162 -0.1173 0.6267 -0.0788 0.0086 -0.1317 0.1440 -0.2035 -0.0742 -0.0536 -0.2773 0.1026 0.0356 0.3384 0.4606 -0.1893 0.0142 0.2269 0.1136 0.0962 0.1648 0.1065 0.0051 -0.1777 -0.0971 0.3434 0.2423 -0.1432 -0.1808\n",
      "of 0.0018 0.3950 0.0662 0.1279 -0.2606 0.3275 -0.0161 -0.1740 0.0530 -0.1168 -0.1781 0.0897 0.0423 -0.1886 0.2358 -0.1118 0.1510 -0.2601 0.0367 -0.0689 -0.1016 :\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat /Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.txt | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *cat* command remains active untill you stop the cell in the notebook (see the ```[*]``` before the cell). Please stop running the cell manually (the square next to the play symbol in the top menu of the notebook) to proceed.\n",
    "\n",
    "If things worked out you see the beginning of the embedding text file. It starts with two digits on the first line.\n",
    "In my case this is \"4530030\" and \"100\". The first is the length of the vocabulary and the second the number of dimensions.\n",
    "\n",
    "The following lines show a word from the learned vocabulary and a list of 100 digits, which are the weights for the dimensions that form the embedding. The first line shows the word \"the\" followed by 100 digits. The rest of the file consists of lines for each word with its embeddings. This is the so-called word2vec format for representing word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the package used, embeddings may be represented differently in memory but it always boils down to three components:\n",
    "\n",
    "* vocabulary: a list of observed words\n",
    "* matrix of learned weights as word vectors\n",
    "* mapping from the words in the vocabulary to the vectors in the matrix\n",
    "\n",
    "A vector is a list of digits, such that each digit is a value for a dimension in an n-dimensional space. If you have a list of these vectors you have a matrix of n-columns and m-rows. Each row corresponds to the vector of a word in the vocabulary.\n",
    "\n",
    "The matrix of 3 rows and 3 columns\n",
    "```\n",
    "[[.34, .56, ,12],\n",
    " [.12, .39, ,05],\n",
    " [.78, .37, ,01]]\n",
    "```\n",
    "\n",
    "The vocabulary with the word as a key and the matrix (list of lists) index that points to the row with the embedding for the word:\n",
    "\n",
    "```{\"dog\": 0, \"cat\" : 1, \"car\" : 2}```\n",
    "\n",
    "For this data, a simple lookup function for *dog* will give the embedding *[.34, .56, ,12]*.\n",
    "\n",
    "Now let's see how this is implemented in GenSim for the Wikipedia derived word embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading a model in Gensim\n",
    "\n",
    "We will now load the downloaded embedding file using the gensim class \"KeyedVectors\", which has a function \"load_word2vec_format\" that can load such a text file. As a parameter, you give it the path to your local file. As an additional parameter you can set a limit for how many words should be read. If you have limited memory on your computer it is wise to set a limit. Note that less words will be read from the file and included in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the local copy of a model built from wikipedia\n",
    "MODEL_FILE='/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.txt'\n",
    "\n",
    "### If you have a small computer you may want to limit the number of embeddings loaded as shown below:\n",
    "## wiki2vec = KeyedVectors.load_word2vec_format(MODEL_FILE, limit=5000) \n",
    "\n",
    "### To load the full model you should drop the limit.\n",
    "# Loading the full model can take a while.\n",
    "wiki2vec = KeyedVectors.load_word2vec_format(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type of object **gensim** created by loading the data and what the properties and functions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_mean_vector', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_for_all', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "print(type(wiki2vec))\n",
    "print(dir(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \"wiki2vec\" is an instance of a **KeyedVectors** object.  A KeyedVectors object is essentially a mapping from keys to vectors. Each vector is identified by its lookup key, most often a short string token.\n",
    "\n",
    "One of the functions is the \"load_word2vec_format\" function we used to load the data from a file, but we also see interesting functions such as: 'get_vector', 'cosine_similarities', 'distance', 'most_similar', 'vocab'. We will look into a few of them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a dictionary that contains words. Let's check how big the vocabulary is of the model derived from English Wikipedia. Note that you will have a different result if you loaded only part of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 100\n",
      "Vocabulary size = 4530030 4530030\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', wiki2vec.vector_size)\n",
    "print('Vocabulary size =', len(wiki2vec.key_to_index), len(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four-and-a-haf millions words are present in this model. That is a lot more than in the English WordNet. Let's check if the word \"man\" is in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a man?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a man?\")\n",
    "\"man\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlikely that there is a cultural specific Dutch word such as \"tjalk\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a tjalk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a tjalk\")\n",
    "\"tjalk\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is rich, so this one is in there too. Let's go wild:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a tjalk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a tjalk\")\n",
    "\"Is there a tjalk\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, whole sentences are not in there. We can also get the vocabulary as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model vocabulary is represented internally as a...\n",
      "<class 'dict'>\n",
      "['the', 'in', 'of', 'a', 'and', 'is', 'to', 'was', 'by', 'for', 'on', 'as', 'at', 'from', 'with', 'an', 'it', 'that', 'also', 'which', 'first', 'this', 'has', 'he', 'one', 'his', 'are', 'after', 'who', 'were', 'two', 'its', 'new', 'be', 'or', 'but', 'had', 'their', 'been', 'born', 'not', 'other', 'all', 'have', 'during', 'time', 'when', 'may', 'they', 'into']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = wiki2vec.key_to_index\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(vocabulary))\n",
    "print((list(vocabulary))[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here are the first 50 items in the list. As you can see these are very common and frequent English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  The embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the embedding representation for a specific word that is in the vocabulary using the **get_vector** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information stored in the vocabulary for the word \"man\":\n",
      "<class 'numpy.ndarray'>\n",
      "Distributional meaning of \"man\" in Wikipedia: [-2.377e-01  2.686e-01 -9.620e-02  2.707e-01 -2.241e-01 -2.489e-01\n",
      "  1.065e-01  4.120e-02 -5.349e-01 -1.445e-01 -8.700e-02 -1.877e-01\n",
      "  1.985e-01 -1.643e-01  1.021e-01 -1.783e-01 -5.520e-02  2.190e-02\n",
      " -2.180e-01  1.569e-01 -2.835e-01 -3.299e-01 -6.780e-02  3.505e-01\n",
      " -3.241e-01 -9.000e-04 -1.234e-01 -3.452e-01 -4.523e-01  7.449e-01\n",
      "  1.470e-01 -1.258e-01 -1.073e-01  4.019e-01  1.120e-01  2.230e-02\n",
      " -3.720e-01  2.026e-01  3.160e-02  2.910e-02 -2.406e-01  1.368e-01\n",
      " -1.750e-02  1.020e-01  8.340e-02  5.012e-01 -3.973e-01  4.010e-02\n",
      " -1.653e-01 -1.892e-01 -1.441e-01  6.290e-02 -5.185e-01 -2.638e-01\n",
      "  3.170e-02 -6.030e-02  1.012e-01 -5.408e-01 -3.528e-01 -1.281e-01\n",
      " -2.617e-01 -2.607e-01 -9.150e-02  3.094e-01  4.468e-01 -2.526e-01\n",
      " -2.842e-01 -9.303e-01 -3.270e-02 -4.669e-01  4.064e-01  2.045e-01\n",
      "  2.223e-01  2.501e-01 -4.577e-01  4.089e-01  1.261e-01 -2.000e-01\n",
      " -8.700e-02 -2.701e-01  3.453e-01  7.210e-02  1.277e-01 -2.570e-02\n",
      "  2.319e-01  3.712e-01  5.400e-02  1.502e-01  2.973e-01  7.630e-02\n",
      " -8.790e-02  2.386e-01  3.562e-01 -6.440e-02  2.086e-01 -3.865e-01\n",
      " -1.940e-02  5.608e-01 -6.439e-01  2.104e-01]\n"
     ]
    }
   ],
   "source": [
    "print('Information stored in the vocabulary for the word \"man\":')\n",
    "man_vector=wiki2vec.get_vector('man')\n",
    "print(type(man_vector))\n",
    "print('Distributional meaning of \"man\" in Wikipedia:', man_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get exotic numbers for 100 dimensions. What else did you expect? \n",
    "\n",
    "A vector is a sorted bunch of numbers, each representing a dimension. These numbers are the weights learned by the neural network when predicting the context words of 'man'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data object is of the type ```numpy.ndarray```. This is a special array defined in the **numpy** package. Numpy is a package for efficiently processing numerical data and is used a lot in machine learning. For those interested, here is the description:\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many dimensions do we have in this vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy data shape (100,)\n",
      "Number of vector dimensions: 100\n"
     ]
    }
   ],
   "source": [
    "print('Numpy data shape', man_vector.shape)\n",
    "print('Number of vector dimensions:', len(man_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a surprise: we loaded a model with 100 dimensions based on a layer with 100 weights. This is true for all words so also for 'dog'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector4dog = wiki2vec['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector4dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013   0.6476  0.1045 -0.3117  0.1475  0.0809 -0.1523  0.265  -0.6462\n",
      " -0.2058  0.1564 -0.2072  0.4179  0.0386 -0.0194 -0.2241  0.2227 -0.3452\n",
      " -0.426   0.1028 -0.2136 -0.0267  0.1946  0.3652 -0.2265 -0.2736  0.0326\n",
      " -0.0279 -0.2359  0.5077  0.3759 -0.2207 -0.0506  0.7909  0.1344 -0.079\n",
      " -0.4099  0.1559 -0.0066  0.1236 -0.5474 -0.0877 -0.3738 -0.253  -0.4688\n",
      " -0.1184 -0.0501  0.3267 -0.1799 -0.2662  0.0968  0.2891 -0.4816 -0.3374\n",
      "  0.2488  0.1744  0.0889 -0.1873 -0.3312 -0.1903  0.0547 -0.6149 -0.427\n",
      " -0.1079  0.137  -0.1445  0.0521 -0.5711 -0.3859 -0.6626  0.2417 -0.0141\n",
      "  0.3974  0.1331 -0.6726 -0.227   0.1793  0.2454  0.1545 -0.0923 -0.0247\n",
      " -0.4611 -0.1317 -0.2194  0.2143  0.491  -0.2186  0.2463  0.0843  0.1324\n",
      " -0.4565  0.008   0.6242 -0.0217 -0.084  -0.4722  0.1191  0.3299 -0.9191\n",
      "  0.0963]\n"
     ]
    }
   ],
   "source": [
    "print(vector4dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the representations are compatible across the words, we can compare two vector representations through the cosine similarity function:\n",
    "\n",
    "![Cosine similarity](./images/cosine-full.png \"Logo Title Text 1\")\n",
    "\n",
    "So suppose we have two vectors A and B, each with 100 slots, this formula (taken from the Wikipedia page) tells you to sum the results of multiplying each slot across A and B:\n",
    "\n",
    "A[0]\\*B[0]+A[1]\\*B[1]+....A[99]\\*B[99]\n",
    "\n",
    "We divide this sum by the square-root of the total sum of the slots of A, multiplied by the square-root of the total sum of the slots of B. Dividing it that way normalises the value between 0 and 1 and it makes the sum of the products of the numerator relative to the product of the sums of the individual vectors.\n",
    "\n",
    "Embedding software uses such measures to obtain the most similar words. We can now use the *most_similar_by_word* function from Gensim to ask for the words that are most similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dogs', 0.8637314438819885), ('cat', 0.8286387324333191), ('puppy', 0.8150733709335327), ('rabbit', 0.8042253851890564), ('montarges', 0.7981182932853699), ('poodle', 0.7949738502502441), ('barfy', 0.7915432453155518), ('cockapoo', 0.7834540009498596), ('pekapoos', 0.7828510999679565), ('pollicle', 0.7826345562934875), ('hound', 0.782395601272583), ('cow', 0.7820497155189514), ('potcake', 0.7794346809387207), ('dweeble', 0.7784105539321899), ('animohš', 0.7776063084602356), ('qimugta', 0.7706390023231506), ('purries', 0.7704073786735535), ('fluppy', 0.7676477432250977), ('rottweiler', 0.7663619518280029), ('retriever', 0.765354573726654), ('puppies', 0.7636122107505798), ('otterhound', 0.7632703185081482), ('labradoodle', 0.7611411213874817), ('dachshund', 0.7609491944313049), ('pig', 0.7596056461334229), ('fuchsl', 0.7588311433792114), ('yorkipoo', 0.7583247423171997), ('maltipoo', 0.7578954100608826), ('löwchen', 0.7569964528083801), ('sheepdog', 0.7556124329566956), ('malamute', 0.7555465698242188), ('laughy', 0.7523866891860962), ('pbgv', 0.7518571019172668), ('feists', 0.7496235966682434), ('smousje', 0.7470435500144958), ('gaspode', 0.745922863483429), ('goat', 0.7454589009284973), ('fluffykins', 0.7454375624656677), ('хьэм', 0.7453168630599976), ('quiltro', 0.7444002628326416), ('toadkiller', 0.7438957691192627), ('bullmastiff', 0.7435868382453918), ('gigglepaws', 0.7423607707023621), ('sled', 0.740258514881134), ('pony', 0.7395642399787903), ('coonhound', 0.7394067645072937), ('turnspit', 0.7392607927322388), ('chupadogra', 0.7374231815338135), ('sharkdog', 0.7367580533027649), ('telomian', 0.7365846037864685)]\n"
     ]
    }
   ],
   "source": [
    "dog_sim = wiki2vec.similar_by_word(\"dog\", topn=50)\n",
    "print(dog_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at this list and compare it with the *dogs* we got from the WordNet hiearchy. Which is richer, which is more precise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **similarity** function directly gives the cosine similarity score across the vectors of two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8286388\n",
      "0.4113313\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.similarity(\"dog\", \"cat\"))\n",
    "print(wiki2vec.similarity(\"dog\", \"coffee\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine sets of vectors by providing a list of positive words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dogggz', 0.825171947479248), ('souljers', 0.8060130476951599), ('hampsterdance', 0.8026869297027588), ('tweetit', 0.7962952852249146), ('pollicle', 0.7949056029319763), ('crazy', 0.7939842939376831), ('skumfuk', 0.7907962203025818), ('trashville', 0.7886988520622253), ('montarges', 0.7879376411437988), ('sugalumps', 0.7846031785011292), ('muppaphones', 0.7845421433448792), ('미친', 0.7824394702911377), ('swampblood', 0.7788825631141663), ('chooo', 0.7775618433952332), ('popdance', 0.7752609848976135), ('spacewalkin', 0.7737489342689514), ('squaredance', 0.773613691329956), ('unshockable', 0.7721720933914185), ('megachic', 0.771091639995575), ('octopad', 0.7708858251571655), ('wheresthelove', 0.7708709239959717), ('rabbittland', 0.7700871825218201), ('breaktek', 0.76993328332901), ('woggy', 0.7697234749794006), ('phidelity', 0.7695780992507935), ('부르는', 0.7692602872848511), ('fucky', 0.7685251235961914), ('donque', 0.7681090235710144), ('sinequanon', 0.7679200768470764), ('poopie', 0.7676936388015747), ('tiajuana', 0.7675125002861023), ('cockadoodle', 0.7656272649765015), ('crewz', 0.7655825018882751), ('feldons', 0.7646614909172058), ('thedude35', 0.7636573314666748), ('cerbee', 0.7635172605514526), ('dueces', 0.7629543542861938), ('waspman', 0.7625081539154053), ('ghettomusick', 0.7613077759742737), ('untanglin', 0.7600138783454895), ('chalypso', 0.7597859501838684), ('lifening', 0.7596178650856018), ('supertzar', 0.7594419121742249), ('doodley', 0.7591357231140137), ('beatlesongs', 0.7588974237442017), ('charmels', 0.7584806680679321), ('buzzstyle', 0.7580109238624573), ('inbetweenies', 0.7577478885650635), ('gabbling', 0.7572817206382751), ('patooties', 0.7571721076965332)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=['dog', 'song'], topn=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining the vectors of **dog** and **song**, we created a new derived vector, a cocktail of both. Next, getting the most similar to this cocktail, we get a very different list of rap-related concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another operation is to get the word that matches the **least** with others, as in an odd-one-out task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'dog']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add and substract vectors to obtain a target vector in space, assuming that there is parallelism in the semantic space. T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('canine', 0.5311753153800964), ('ENTITY/Guard_dog', 0.48210182785987854), ('dogs', 0.4805601239204407), ('ENTITY/Dog_bite_prevention', 0.4798755943775177), ('tollers', 0.47770798206329346), ('ENTITY/Kangaroo', 0.466862291097641), ('ENTITY/Greyhound_adoption', 0.46052291989326477), ('ENTITY/Housebreaking', 0.45895230770111084), ('ENTITY/Hunting_dog', 0.45722559094429016), ('ENTITY/Fox_Terrier', 0.4570063352584839)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=[\"dog\"], negative=[\"song\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most famous example in this respect is not about dogs and cats but about kings and queens (see the original Word2Vec paper Mikolov et al 2013). By adding **woman** to **king** and subtracting **man**, you should end up somewhere in semantic space close to **queen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.830649197101593), ('monarch', 0.7416260838508606), ('ENTITY/Queen_consort', 0.7348716855049133), ('laungshe', 0.7347308993339539), ('regnant', 0.7243735194206238), ('chelna', 0.7236213088035583), ('consort', 0.720160722732544), ('indlovukati', 0.7181541919708252), ('kamamalu', 0.7178552150726318), ('indlovukazi', 0.714848518371582)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Wiki2vec has special prefixes ```ENTITY/``` for words that are actual hyperlinked names. This is an idiosyncracy of Wiki2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec analogy example has been critisized because **king** and **queen** are already very close without doing anything. Nevertheless, adding and substracting did move **queen** closer as it moved from position 12 to 1 and scored 0.83 instead of 0.75 due to our vector manipulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7570435\n",
      "[('meldryn', 0.7807800769805908), ('domnal', 0.7773880362510681), ('buwanekabahu', 0.771612286567688), ('boromakot', 0.7671722173690796), ('borommatrailokanat', 0.764298677444458), ('silamegha', 0.763579249382019), ('parâkramabâhu', 0.7631061673164368), ('kings', 0.7624024152755737), ('monarch', 0.7603241205215454), ('haraald', 0.7582810521125793), ('saysethathirath', 0.7575438618659973), ('queen', 0.7570434808731079), ('13281350', 0.754815399646759), ('15721610', 0.7541301846504211), ('14061454', 0.7522280812263489), ('19101925', 0.7507657408714294), ('panduvasudeva', 0.7499051094055176), ('bhadrasena', 0.7497343420982361), ('13801422', 0.7493346929550171), ('13901406', 0.749112606048584), ('malikum', 0.7482700347900391), ('borommarachathirat', 0.7469587326049805), ('perekule', 0.7465206384658813), ('duttabaung', 0.7459855079650879), ('pālaka', 0.7454712390899658), ('hiranyakasyapa', 0.7443112730979919), ('suliyavongsa', 0.7442653775215149), ('15471553', 0.743290901184082), ('puntite', 0.7425240278244019), ('15401571', 0.7405396699905396), ('gwylliam', 0.7402992844581604), ('11631172', 0.7398092150688171), ('silihwangi', 0.7394959330558777), ('11821202', 0.7393442392349243), ('tsarkon', 0.7393054366111755), ('thazata', 0.7389993667602539), ('athittayawong', 0.7378371953964233), ('pedroiv', 0.7374894618988037), ('16031625', 0.7369816303253174), ('algaut', 0.7369042634963989), ('goshposh', 0.7365431189537048), ('12911327', 0.735938549041748), ('16051639', 0.7356316447257996), ('bhumipol', 0.735440731048584), ('19251935', 0.7352972626686096), ('æðelstan', 0.7352105379104614), ('buvanekabahu', 0.7351621389389038), ('sulakrama', 0.7350161671638489), ('15681592', 0.7342566847801208), ('19471972', 0.7336845397949219)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.similarity(\"king\", \"queen\"))\n",
    "print(wiki2vec.most_similar(\"king\", topn=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disambiguating word meanings in word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the addition and subtraction to separate different meanings of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mice', 0.7914096117019653), ('merized', 0.7884306907653809), ('rabbit', 0.7779645919799805), ('desmarestianus', 0.760723888874054), ('batmouse', 0.7604084610939026), ('cuppedius', 0.7602810263633728), ('hamster', 0.7569741606712341), ('rat', 0.7562689185142517), ('urartensis', 0.7485920190811157), ('ochrotomys', 0.7427321672439575)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.similar_by_word(\"mouse\", topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the animal meaning of mouse is dominant in Wikipedia. We can now add **computer** and subtract **rat** to manipulate the similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('computers', 0.7617183327674866), ('hardware', 0.7281700968742371), ('software', 0.7272316217422485), ('mscape', 0.7164640426635742), ('probeware', 0.716342031955719), ('systemsthe', 0.7032573819160461), ('graphics', 0.702555239200592), ('programmers', 0.7021493315696716), ('programmable', 0.7009457945823669), ('pdp11', 0.6991949677467346)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=[\"mouse\", \"computer\"], negative=[\"rat\"], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rabbit', 0.6957606673240662), ('mystromys', 0.690538227558136), ('lophiomys', 0.6903327107429504), ('ochrotomys', 0.6902164220809937), ('conilurus', 0.6892696619033813), ('imhausi', 0.6819950938224792), ('culturatus', 0.6803641319274902), ('rufodorsalis', 0.6792685985565186), ('shrew', 0.6781168580055237), ('orangiae', 0.676617443561554)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=[\"mouse\", \"rat\"], negative=[\"computer\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Word Embeddings from Facebook's Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research lab of Facebook, created embeddings in 157 languages from Common Crawl. Common Crawl is a project that scrapes the web for texts in as many languages that are present. Check out the Github of Fasttext and Common Crawl for more details:\n",
    "\n",
    "* https://commoncrawl.org/the-data/get-started/\n",
    "* https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\n",
    "\n",
    "You should download the text version of the embeddings, which are following the same word2vec format as the Wike2Vec models.\n",
    "I downloaded the embeddings for Frysian and Limburgish, which are two local, regional languages. We can inpsect these two text files in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526167 300\n",
      ". 0.0139 -0.0210 -0.0328 -0.0229 0.2410 -0.0260 0.0071 -0.0160 0.0308 0.1494 0.0172 0.0276 -0.0017 0.0205 -0.0072 -0.0089 -0.0154 -0.0288 0.0217 -0.0085 0.0114 0.0119 -0.0284 -0.0170 0.0035 -0.0065 0.0191 -0.0085 0.0190 0.0197 0.0497 -0.0014 0.0060 -0.0069 0.0119 0.0168 0.0049 -0.0049 -0.0081 0.0194 0.0187 -0.0013 -0.0073 0.0139 0.0211 0.0138 -0.0148 0.0042 0.0004 0.0085 -0.0077 -0.0274 -0.0114 -0.0073 -0.0042 0.0038 0.0009 0.1392 0.0047 -0.0014 -0.0010 0.0031 -0.0005 -0.0238 -0.0040 0.0103 0.0276 0.0052 -0.0244 0.0189 0.0208 0.0128 0.0100 -0.0016 0.0019 0.0015 0.0308 -0.0299 -0.0125 0.0094 0.0671 0.0361 -0.0079 -0.0272 0.0130 -0.0070 -0.0004 0.0052 -0.0046 -0.0036 -0.0083 0.0083 0.0192 -0.0097 0.0201 -0.0088 -0.0143 0.0176 -0.0161 -0.0087 0.0108 0.0016 -0.0036 0.0272 0.0440 -0.0102 0.0194 0.0037 0.0198 -0.0054 -0.0010 -0.0030 0.0221 -0.0367 0.0004 0.0175 0.0520 -0.0069 -0.0036 0.0293 -0.0043 -0.0152 -0.0093 0.0255 0.0003 -0.0035 0.0348 0.0176 -0.0236 0.0035 0.0583 -0.0857 -0.0101 0.0213 0.0190 0.0008 -0.0181 0.0081 0.0209 0.0074 -0.0044 -0.0183 -0.0441 -0.0121 -0.0296 -0.0273 -0.0183 -0.0292 -0.0293 0.0112 -0.0128 -0.0079 0.0020 0.0033 -0.0029 0.0103 -0.0195 0.0056 0.0107 -0.0087 -0.0199 0.0067 -0.0024 0.0161 0.0111 -0.0433 -0.0014 -0.0258 -0.0063 -0.0218 -0.1612 -0.0027 0.0227 0.0085 -0.0051 -0.0226 -0.0177 0.0192 0.0071 -0.0080 -0.0211 -0.0169 0.0163 0.1091 0.0094 0.0165 -0.0095 0.0034 -0.0841 0.0296 -0.0232 0.0172 0.0243 -0.1293 -0.0063 -0.0101 -0.0104 -0.0151 0.0037 -0.0299 -0.0078 -0.0068 0.0014 0.2565 -0.0084 -0.0145 0.0246 0.0013 -0.0019 -0.0284 -0.0003 -0.0118 -0.0512 -0.0023 -0.0297 0.0337 0.0002 0.0059 -0.0148 -0.0421 -0.0625 0.0101 -0.0099 0.0114 -0.0115 0.0051 0.0757 -0.0320 0.0172 0.0067 -0.0087 -0.0019 -0.0144 0.020:\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat  \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.fy.300.vec\" | more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306222 300\n",
      "' -0.0150 -0.0400 0.0501 -0.0717 -0.0137 0.0211 0.0416 0.0129 0.0141 0.0216 0.0250 0.0593 -0.0445 0.0193 -0.0003 -0.0169 0.0217 0.0115 -0.0477 -0.0079 -0.0130 0.0283 -0.0274 0.0024 -0.0774 -0.0215 -0.0345 -0.0077 0.0146 -0.0027 -0.0087 0.0011 -0.0105 0.1293 -0.0385 0.0089 -0.0080 -0.0460 0.0075 0.0241 0.0128 0.0006 0.0730 -0.0214 0.1288 0.0327 -0.0051 -0.0403 0.0103 0.0162 -0.0266 -0.0041 0.0075 -0.0028 0.0516 -0.0068 0.0191 -0.0159 0.0393 0.0053 -0.0347 0.0049 0.0063 -0.0582 0.0186 -0.0151 -0.0245 0.0054 0.0352 0.0268 0.0313 0.0166 -0.0037 0.0444 -0.0140 0.0595 0.0168 0.0309 -0.0075 -0.0360 0.0133 -0.0097 0.0309 -0.0339 -0.0262 -0.0637 -0.0074 -0.0029 0.0032 0.0123 -0.0102 -0.0100 -0.0129 -0.0051 -0.0047 -0.0834 -0.0273 -0.0322 -0.0355 -0.0005 -0.0230 0.0571 -0.0147 0.0053 -0.0271 0.0189 -0.0605 0.0250 0.0149 0.0120 -0.0225 -0.0752 -0.0751 0.0189 -0.0105 0.0276 -0.0156 -0.0079 -0.0502 -0.0214 0.0635 0.0127 -0.0148 0.0152 0.0049 -0.0183 0.0351 0.0391 -0.0167 0.0224 0.0248 -0.0057 0.0018 -0.0275 -0.0358 -0.0414 -0.0128 -0.0205 0.0243 0.0138 0.0054 0.0111 -0.0123 -0.0116 -0.0046 0.0290 0.0029 -0.0058 -0.0016 0.0405 0.0277 -0.0029 0.0273 0.0355 -0.0493 -0.0530 0.0177 -0.0318 0.0164 -0.0108 -0.0047 -0.0199 0.0486 -0.0400 0.0238 -0.0121 0.0295 -0.0336 0.0108 -0.0083 -0.0202 -0.0259 -0.0153 -0.0088 -0.0055 0.0055 -0.0043 0.0274 0.0283 -0.0289 -0.0250 -0.0155 0.0300 -0.0481 0.0283 0.0130 0.0679 0.0101 0.0094 -0.0045 0.0328 0.0098 0.0048 -0.0860 0.0593 -0.0411 -0.0361 -0.0155 -0.0471 -0.0191 0.0564 -0.0064 0.0352 -0.0554 0.0229 -0.0149 0.0288 -0.0028 0.0211 0.0071 0.0030 -0.0602 -0.0536 0.0706 -0.0212 0.0224 -0.0222 -0.0491 0.0469 -0.0137 0.0162 0.0087 -0.0149 -0.0094 0.0040 -0.0016 -0.0392 -0.0383 -0.0710 0.0058 0.0040 0.1177 -0.0161 0.0:\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat  \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.li.300.vec\" | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frysian model has 526167 words and the Limburgish model as 306222 words. Let's load the models and inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_my_fasttext_model = \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.fy.300.vec\"\n",
    "fasttext_fy = KeyedVectors.load_word2vec_format(path_to_my_fasttext_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 300\n",
      "Vocabulary size = 526167 526167\n",
      "Word index for \"tjalk\" 141358\n",
      "Vecotr for \"tjalk\" [-2.80e-02 -6.10e-03 -3.38e-02 -3.78e-02 -1.51e-02 -4.40e-03 -1.70e-03\n",
      " -3.31e-02  5.80e-03 -2.07e-02 -8.60e-03  2.24e-02  2.50e-03  1.31e-02\n",
      " -1.28e-02 -1.37e-02 -2.20e-02 -2.49e-02 -3.70e-03 -3.46e-02 -9.40e-03\n",
      "  3.30e-02  2.13e-02 -9.80e-03  3.20e-03 -1.81e-02  3.38e-02 -3.87e-02\n",
      " -1.57e-02 -1.38e-02  2.29e-02  5.40e-03  1.81e-02  9.30e-03 -2.04e-02\n",
      "  2.55e-02 -1.91e-02  6.40e-03 -8.00e-04  5.83e-02  1.73e-02  1.26e-02\n",
      "  1.97e-02  4.30e-03 -1.90e-03  1.56e-02  1.08e-02 -3.00e-03  8.40e-03\n",
      " -4.40e-03  2.30e-02  8.10e-03 -9.00e-04  8.80e-03  7.50e-03  3.79e-02\n",
      " -2.03e-02 -1.14e-02 -4.22e-02  5.50e-03  2.41e-02 -2.16e-02 -3.00e-03\n",
      "  1.38e-02  2.00e-03 -2.12e-02  2.50e-03 -1.04e-02 -1.48e-02 -3.29e-02\n",
      "  1.56e-02 -1.04e-02 -1.52e-02  7.40e-03  5.05e-02  5.80e-03  1.09e-02\n",
      "  1.77e-02 -2.80e-03 -5.40e-03 -1.15e-02  1.88e-02 -7.40e-03  5.10e-03\n",
      " -7.50e-03 -1.08e-02  1.90e-03 -1.69e-02 -2.59e-02 -2.00e-04  1.33e-02\n",
      " -1.45e-02 -1.20e-02  7.00e-04 -1.25e-02 -1.00e-03 -5.10e-03 -1.00e-04\n",
      "  8.00e-04  1.90e-02 -2.53e-02 -5.60e-03  3.90e-03 -8.60e-03  6.60e-03\n",
      " -7.80e-03  7.60e-03 -2.16e-02  3.26e-02  9.00e-04 -2.86e-02  5.90e-03\n",
      "  2.42e-02  5.70e-03  7.90e-03 -1.90e-03  4.70e-03  2.42e-02  1.34e-02\n",
      " -2.47e-02  4.00e-03  1.70e-03 -8.90e-03 -5.00e-04 -2.20e-03 -1.90e-03\n",
      " -6.70e-03  1.78e-02  1.06e-02 -3.48e-02 -6.63e-02 -2.00e-04  1.92e-02\n",
      " -3.08e-02  7.30e-03 -2.00e-02 -2.32e-02 -1.14e-02  1.00e-04 -7.60e-03\n",
      "  7.30e-03  0.00e+00  5.40e-03  8.80e-03 -2.80e-03  1.89e-02  2.37e-02\n",
      " -2.98e-02  2.50e-03  1.90e-03 -2.70e-02 -2.60e-03  2.39e-02 -1.43e-02\n",
      " -2.87e-02 -1.92e-02 -1.59e-02  1.08e-02 -2.00e-03 -5.70e-03  3.12e-02\n",
      " -1.40e-03  1.94e-02  2.00e-04  2.13e-02 -1.35e-02  1.68e-02 -9.50e-03\n",
      "  3.68e-02 -2.23e-02  8.08e-02  1.15e-02  1.88e-02  1.89e-02  1.32e-02\n",
      "  1.45e-02 -1.45e-02 -2.07e-02  2.77e-02  5.90e-03 -1.35e-02  2.47e-02\n",
      " -2.02e-02 -2.29e-02 -1.39e-02 -3.14e-02  1.82e-02 -1.87e-02  1.69e-02\n",
      "  2.79e-02  9.00e-03  3.60e-02 -3.80e-03 -1.00e-04 -1.92e-02  1.81e-02\n",
      "  1.33e-02 -2.11e-02 -3.16e-02 -2.03e-02 -9.00e-04  1.14e-02  1.04e-02\n",
      " -7.06e-02  1.71e-02 -1.05e-02 -2.01e-02  1.07e-02  6.50e-03  2.60e-03\n",
      " -6.40e-03 -1.00e-04  2.30e-03 -3.40e-03  5.80e-03  5.90e-03  6.50e-03\n",
      "  1.83e-02 -2.73e-02  1.91e-02  4.00e-04  5.40e-03  2.24e-02 -1.40e-03\n",
      "  2.58e-02 -1.77e-02  1.87e-02  2.51e-02  1.51e-02 -6.00e-04 -2.50e-03\n",
      "  1.70e-03 -6.40e-03  1.27e-02 -1.41e-02  2.22e-02 -3.30e-03 -1.91e-02\n",
      " -3.92e-02  8.80e-03 -1.10e-02 -2.52e-02  1.86e-02 -7.50e-03 -3.30e-03\n",
      " -6.80e-03 -1.77e-02 -9.70e-03 -1.11e-02  2.04e-02 -1.41e-02  3.30e-03\n",
      "  4.20e-02  4.02e-02 -2.00e-02 -1.49e-02 -4.10e-03 -1.31e-02 -3.70e-03\n",
      "  3.04e-02 -2.40e-02 -8.40e-03  1.61e-02 -4.59e-02 -1.04e-02  2.55e-02\n",
      "  1.08e-02  1.71e-02  2.70e-02 -7.20e-03 -8.50e-03 -2.80e-02 -8.00e-04\n",
      "  5.00e-04 -1.95e-02  2.48e-02 -9.70e-03 -3.79e-02  2.86e-02 -1.30e-02\n",
      "  3.50e-03  3.08e-02 -2.77e-02 -9.60e-03  2.72e-02 -2.30e-03  2.30e-03\n",
      " -1.00e-04 -1.81e-02  2.80e-03 -4.13e-02  3.52e-02  2.42e-02 -2.50e-03\n",
      " -1.13e-02  5.70e-03  3.10e-03  9.60e-03  3.60e-02 -2.77e-02]\n",
      "Most similar tjalk [('Koftjalk', 0.7346436977386475), ('tsjalk', 0.6244613528251648), ('Dektsjalk', 0.5733082890510559), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.5195964574813843), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "similar_by_word tjalk [('Koftjalk', 0.7346436977386475), ('tsjalk', 0.6244613528251648), ('Dektsjalk', 0.5733082890510559), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.5195964574813843), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "similar_by_vector tjalk [('Koftjalk', 0.7346436977386475), ('tsjalk', 0.6244613528251648), ('Dektsjalk', 0.5733082890510559), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.5195964574813843), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "doesnt_match ['sneek', 'joure', 'tjalk'] tjalk\n",
      "\n",
      "similarity tjalk sneek 0.35242274\n",
      "\n",
      "n_similarity ['sneek', 'joure', 'tjalk'] ['aap', 'noot', 'mies'] 0.3253808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', fasttext_fy.vector_size)\n",
    "print('Vocabulary size =', len(fasttext_fy.key_to_index), len(fasttext_fy))\n",
    "word_idx = fasttext_fy.key_to_index[\"tjalk\"]\n",
    "print('Word index for \\\"tjalk\\\"', word_idx)\n",
    "vector = fasttext_fy.get_vector(\"tjalk\")\n",
    "print('Vecotr for \\\"tjalk\\\"', vector)\n",
    "\n",
    "wordA=\"tjalk\"\n",
    "wordB=\"sneek\"\n",
    "wordlistA=[\"sneek\", \"joure\", \"tjalk\"]\n",
    "wordlistB=[\"aap\", \"noot\", \"mies\"]\n",
    "print(\"Most similar\", wordA, fasttext_fy.most_similar(wordA))  # 👍\n",
    "print()\n",
    "print(\"similar_by_word\", wordA, fasttext_fy.similar_by_word(wordA))  # 👍\n",
    "print()\n",
    "print(\"similar_by_vector\", wordA, fasttext_fy.similar_by_vector(wordA))  # 👍\n",
    "print()\n",
    "print(\"doesnt_match\", wordlistA, fasttext_fy.doesnt_match(wordlistA))  # 👍\n",
    "print()\n",
    "print(\"similarity\", wordA, wordB, fasttext_fy.similarity(wordA, wordB))  # 👍\n",
    "print()\n",
    "print(\"n_similarity\", wordlistA, wordlistB, fasttext_fy.n_similarity(wordlistA, wordlistB))  # 👍\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_my_fasttext_model = \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.li.300.vec\"\n",
    "fasttext_li = KeyedVectors.load_word2vec_format(path_to_my_fasttext_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 300\n",
      "Vocabulary size = 306222 306222\n",
      "Word index for \"stroat\" 240260\n",
      "Vector for \"stroat\" 240260\n",
      "[ 0.0136 -0.0423  0.0041  0.0326 -0.0135 -0.0034 -0.0137 -0.0132  0.0041\n",
      " -0.0108 -0.0007 -0.0127 -0.0194 -0.0115 -0.0003 -0.001  -0.0127 -0.0061\n",
      " -0.0295  0.0258  0.0062 -0.0114  0.0376 -0.0237  0.0093 -0.009   0.0071\n",
      " -0.015   0.0196 -0.0079 -0.0127 -0.0194  0.0105 -0.009   0.0105 -0.035\n",
      "  0.0254  0.0177 -0.0182 -0.0077 -0.0028  0.0133 -0.0063 -0.0225  0.0073\n",
      "  0.0186 -0.0089  0.0334  0.015  -0.0091 -0.0188  0.0133  0.0074  0.0185\n",
      "  0.009   0.0018  0.0021 -0.0166 -0.0023 -0.0148 -0.01   -0.0066  0.0169\n",
      " -0.0052 -0.0095 -0.0342 -0.0152  0.0082  0.0168  0.0042  0.0101  0.0535\n",
      "  0.0059 -0.0028 -0.0021  0.0243  0.0035 -0.0022  0.006  -0.0381 -0.0024\n",
      "  0.0085  0.0036 -0.003  -0.0064 -0.0116  0.0019  0.0009  0.0049  0.0268\n",
      "  0.0118  0.0047 -0.025  -0.0103 -0.0034  0.0168 -0.0158  0.0035 -0.0132\n",
      " -0.0107 -0.0058 -0.0013 -0.0123 -0.0176  0.0092  0.0178  0.0097 -0.0012\n",
      "  0.0371  0.0325 -0.0069  0.019   0.0336  0.004  -0.0067 -0.0036 -0.0057\n",
      " -0.0202  0.0366  0.0066 -0.0147  0.0099 -0.0064 -0.0088 -0.0014 -0.0194\n",
      " -0.0396  0.0277  0.0008  0.0073 -0.0039 -0.0063 -0.0141  0.0046 -0.0206\n",
      "  0.0228  0.043  -0.0133  0.0265 -0.0015  0.0016 -0.0217 -0.0035 -0.007\n",
      "  0.0114  0.0186 -0.0163  0.0091 -0.0236  0.0003 -0.002  -0.0034  0.0067\n",
      "  0.0115  0.0535  0.0137 -0.0014  0.0051  0.0034 -0.0041 -0.003   0.0268\n",
      "  0.0321 -0.0008 -0.0314  0.024   0.0094 -0.0164  0.0154  0.0046  0.0025\n",
      " -0.0333  0.0118  0.0071  0.0067  0.0211  0.0013 -0.0261 -0.0162  0.0512\n",
      "  0.0116 -0.0117  0.0003 -0.0174  0.0026  0.0093 -0.0159 -0.0243  0.0802\n",
      " -0.0066 -0.027   0.0254  0.      0.0226  0.038  -0.0082 -0.0007 -0.0116\n",
      "  0.0101  0.056   0.0059 -0.0154 -0.0055 -0.0058 -0.0024 -0.0037  0.0143\n",
      " -0.0052 -0.0059 -0.0196  0.0032 -0.0031  0.0186 -0.0177  0.0014  0.0035\n",
      "  0.0166  0.0032 -0.0599 -0.0019 -0.0058 -0.0161  0.011   0.0031  0.0094\n",
      " -0.0039 -0.002  -0.0192 -0.0071 -0.0136  0.0038 -0.0387  0.0141 -0.0023\n",
      " -0.0301 -0.013  -0.0037 -0.0134  0.0147 -0.0273 -0.0201  0.0113  0.0155\n",
      " -0.0145 -0.0098  0.0129  0.005  -0.0132 -0.0013 -0.0172  0.0036 -0.0076\n",
      "  0.0558 -0.0032 -0.004  -0.0182 -0.0261  0.0167  0.0192 -0.027   0.0093\n",
      "  0.0244 -0.0043  0.0113 -0.0005  0.0032  0.0212 -0.0039  0.0147 -0.0086\n",
      "  0.0146 -0.0112 -0.0205 -0.0086  0.001  -0.0126  0.0099 -0.0099  0.0458\n",
      " -0.0039 -0.0253 -0.0058  0.0075 -0.004  -0.0117  0.0023 -0.0342  0.0054\n",
      "  0.0656 -0.0128 -0.0183 -0.0117  0.0144  0.0027  0.0097 -0.0146 -0.0233\n",
      "  0.0021 -0.0089 -0.0194]\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', fasttext_li.vector_size)\n",
    "print('Vocabulary size =', len(fasttext_li.key_to_index), len(fasttext_li))\n",
    "word_idx = fasttext_li.key_to_index[\"stroat\"]\n",
    "print('Word index for \\\"stroat\\\"', word_idx)\n",
    "vector = fasttext_li.get_vector(\"stroat\")\n",
    "print('Vector for \\\"stroat\\\"', word_idx)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar stroat [('dörpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460541486740112), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679347038269), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274198889732361), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.611824631690979)]\n",
      "\n",
      "Most similar poet [('Hoemel', 0.863980233669281), ('foemel', 0.8294392228126526), ('loemelekriemèrsj', 0.8049797415733337), ('Sjtoemel', 0.7726760506629944), ('dremel', 0.7404616475105286), ('Bieemel', 0.7326872944831848), ('loemele', 0.727192759513855), ('kloemele', 0.7036440968513489), ('tasemel', 0.6926780939102173), ('Loemel', 0.683782696723938)]\n",
      "\n",
      "Most similar loemel [('fine-boned', 0.5196540355682373), ('laureate', 0.4606165885925293), ('kölsje', 0.4573061466217041), ('face', 0.4414014220237732), ('pipe', 0.3875519335269928), ('Nut.', 0.38502728939056396), ('Jenniches', 0.3829471170902252), ('gepoet', 0.3810572028160095), ('zàèn', 0.37500694394111633), ('walvèsechtege', 0.3712086081504822)]\n",
      "\n",
      "similar_by_word stroat [('dörpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460541486740112), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679347038269), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274198889732361), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.611824631690979)]\n",
      "\n",
      "similar_by_vector stroat [('dörpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460541486740112), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679347038269), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274198889732361), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.611824631690979)]\n",
      "\n",
      "doesnt_match ['stroat', 'poet', 'loemel'] poet\n",
      "\n",
      "similarity stroat poet 0.054689746\n",
      "\n",
      "n_similarity ['stroat', 'poet', 'loemel'] ['aap', 'noot', 'mies'] 0.20698144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordA=\"stroat\"\n",
    "wordB=\"poet\"\n",
    "wordC=\"loemel\"\n",
    "wordlistA=[\"stroat\", \"poet\", \"loemel\"]\n",
    "wordlistB=[\"aap\", \"noot\", \"mies\"]\n",
    "print(\"Most similar\", wordA, fasttext_li.most_similar(wordA))  # 👍\n",
    "print()\n",
    "\n",
    "print(\"Most similar\", wordB, fasttext_li.most_similar(wordC))  # 👍\n",
    "print()\n",
    "\n",
    "print(\"Most similar\", wordC, fasttext_li.most_similar(wordB))  # 👍\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"similar_by_word\", wordA, fasttext_li.similar_by_word(wordA))  # 👍\n",
    "print()\n",
    "print(\"similar_by_vector\", wordA, fasttext_li.similar_by_vector(wordA))  # 👍\n",
    "print()\n",
    "print(\"doesnt_match\", wordlistA, fasttext_li.doesnt_match(wordlistA))  # 👍\n",
    "print()\n",
    "print(\"similarity\", wordA, wordB, fasttext_li.similarity(wordA, wordB))  # 👍\n",
    "print()\n",
    "print(\"n_similarity\", wordlistA, wordlistB, fasttext_li.n_similarity(wordlistA, wordlistB))  # 👍\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Links to existing models available for download\n",
    "\n",
    "Follow the links to browse available models. The sources listed below contain English models trained using different algorithms, data with different degrees of preprocessing and varying hyperparameter settings. Some resources also include models in other languages.\n",
    "\n",
    "### Large and commonly used models (English):\n",
    "\n",
    "* Google word2vec: can be downloaded from here (follow link in instructions): http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "* GloVe (trained on various corpora): https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "* FastText embeddings (Facebook): https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "* Models with different algorithms, hyperparamters, dimensions and degrees of preprocessing (e.g. dependency parsing windows):  https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html\n",
    "\n",
    "\n",
    "### Various models in English & other languages:\n",
    "\n",
    "* Various algorithms and parameters for English and other languages: http://vectors.nlpl.eu/repository/#\n",
    "\n",
    "### Cross lingual embeddings:\n",
    "* https://ruder.io/cross-lingual-embeddings/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hltenv",
   "language": "python",
   "name": "hltenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
