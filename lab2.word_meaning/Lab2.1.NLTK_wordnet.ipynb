{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2.1: Words, concepts, semantic relations in Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you are going to work with wordnet databases that have been incorporated in the NLTK package.\n",
    "Detailed information how to access and use wordnet can be found here: http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "Study the documentation and make yourself familiar with the different commands. Some of documentation is also explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an abstract model, WordNet provides a mapping of the vocabulary of a language to a set of concepts with semantic relations between these concepts. The concepts and relations form a huge semantic graph or space, in which one concept is related to another. In the next picture, you only see a fragment of the WordNet graph for all concepts related to communication, which is the concept in the center. This graph only shows the `hyponymy` relations, which are most dominant in WordNet. You can travel from the edge of the graph to the center and take another branch back the the edge. In this way, you could reach any communication related concept. \n",
    "\n",
    "![wordnet](./images/wn-communication.png)\n",
    "\n",
    "The graph is so dense that you cannot read the words that map to each concept. You can imagine how the graph would look like if you project the complete WordNet space as a universe.\n",
    "\n",
    "The mapping of the words of a language to the concepts is complex. Synonymous words form a so-called `synset`, e.g. `{board, surf board}`, which represents a single concept. However, a word from a synset such as `board` can also have other meanings (`polysemy`), and therefore occur in other synsets as well. There is therefore a many-to-many relation between words and concepts through the synset mappings. Finally, it is important to realize that nouns, verbs and adjectives form different subgraphs in WordNet and exhibit different relations.\n",
    "\n",
    "In this notebook, we will explain how to access the graph structure and explore it as well as how to measure how close or distant concepts and words are according to the graph. We first need to import the *wordnet* module from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From words to synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look up a word using the \"wn.synsets()\" function. This will give you a list of synsets in which the lookup string is matched with a lemma (synonym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of synsets with \"dog\" as a synonym: 8\n",
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "dog_synsets = wn.synsets(lemma='dog')\n",
    "print('Number of synsets with \"dog\" as a synonym:', len(dog_synsets))\n",
    "print(dog_synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"dog\" thus occurs as a synonym in 8 synsets (concepts), which means it has 8 different meanings in WordNet and can be located in 8 different places in the graph.\n",
    "\n",
    "If we print the synsets, we get a shorthand representation in which the first synonym is shown from the complete synset. In some cases this is the word \"dog\" but we see also other words such as \"cad\", \"chase\". So \"dog\" is always one of the synonyms but not always the first one listed. You can also observe that the word is followed by a digit (the sense number of the word) and a part-of-speech tag. We see that \"dog\" can also be a verb in English according to WordNet because \"chase.v.01\" is a verb and synsets have only one part-of-speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of useful functions that you can call for each synset to get more information:\n",
    "\n",
    "* the synonyms\n",
    "* the definition or gloss\n",
    "* the depth in the hierarchy\n",
    "* semantic relations to other synsets\n",
    "* examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The synset = Synset('dog.n.01')\n",
      "The synonyms =  [Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n",
      "The definition = a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "The depth of its hypernym chain is =  13\n",
      "Hypernyms: [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "Hyponyms: [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
      "Part holonyms: []\n",
      "Member holonyms: [Synset('canis.n.01'), Synset('pack.n.06')]\n",
      "Substance holonyms: []\n",
      "Part meronyms: [Synset('flag.n.07')]\n",
      "Member meronyms: []\n",
      "Substance meronyms: []\n",
      "Examples: ['the dog barked all night']\n",
      "\n",
      "The synset = Synset('frump.n.01')\n",
      "The synonyms =  [Lemma('frump.n.01.frump'), Lemma('frump.n.01.dog')]\n",
      "The definition = a dull unattractive unpleasant girl or woman\n",
      "The depth of its hypernym chain is =  10\n",
      "Hypernyms: [Synset('unpleasant_woman.n.01')]\n",
      "Hyponyms: []\n",
      "Part holonyms: []\n",
      "Member holonyms: []\n",
      "Substance holonyms: []\n",
      "Part meronyms: []\n",
      "Member meronyms: []\n",
      "Substance meronyms: []\n",
      "Examples: ['she got a reputation as a frump', \"she's a real dog\"]\n",
      "\n",
      "The synset = Synset('dog.n.03')\n",
      "The synonyms =  [Lemma('dog.n.03.dog')]\n",
      "The definition = informal term for a man\n",
      "The depth of its hypernym chain is =  9\n",
      "Hypernyms: [Synset('chap.n.01')]\n",
      "Hyponyms: []\n",
      "Part holonyms: []\n",
      "Member holonyms: []\n",
      "Substance holonyms: []\n",
      "Part meronyms: []\n",
      "Member meronyms: []\n",
      "Substance meronyms: []\n",
      "Examples: ['you lucky dog']\n",
      "\n",
      "The synset = Synset('cad.n.01')\n",
      "The synonyms =  [Lemma('cad.n.01.cad'), Lemma('cad.n.01.bounder'), Lemma('cad.n.01.blackguard'), Lemma('cad.n.01.dog'), Lemma('cad.n.01.hound'), Lemma('cad.n.01.heel')]\n",
      "The definition = someone who is morally reprehensible\n",
      "The depth of its hypernym chain is =  9\n",
      "Hypernyms: [Synset('villain.n.01')]\n",
      "Hyponyms: [Synset('perisher.n.01')]\n",
      "Part holonyms: []\n",
      "Member holonyms: []\n",
      "Substance holonyms: []\n",
      "Part meronyms: []\n",
      "Member meronyms: []\n",
      "Substance meronyms: []\n",
      "Examples: ['you dirty dog']\n",
      "\n",
      "The synset = Synset('frank.n.02')\n",
      "The synonyms =  [Lemma('frank.n.02.frank'), Lemma('frank.n.02.frankfurter'), Lemma('frank.n.02.hotdog'), Lemma('frank.n.02.hot_dog'), Lemma('frank.n.02.dog'), Lemma('frank.n.02.wiener'), Lemma('frank.n.02.wienerwurst'), Lemma('frank.n.02.weenie')]\n",
      "The definition = a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n",
      "The depth of its hypernym chain is =  7\n",
      "Hypernyms: [Synset('sausage.n.01')]\n",
      "Hyponyms: [Synset('vienna_sausage.n.01')]\n",
      "Part holonyms: [Synset('hotdog.n.02')]\n",
      "Member holonyms: []\n",
      "Substance holonyms: []\n",
      "Part meronyms: []\n",
      "Member meronyms: []\n",
      "Substance meronyms: []\n",
      "Examples: []\n",
      "\n",
      "The synset = Synset('pawl.n.01')\n",
      "The synonyms =  [Lemma('pawl.n.01.pawl'), Lemma('pawl.n.01.detent'), Lemma('pawl.n.01.click'), Lemma('pawl.n.01.dog')]\n",
      "The definition = a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
      "The depth of its hypernym chain is =  9\n",
      "Hypernyms: [Synset('catch.n.06')]\n",
      "Hyponyms: []\n",
      "Part holonyms: [Synset('ratchet.n.01')]\n",
      "Member holonyms: []\n",
      "Substance holonyms: []\n",
      "Part meronyms: []\n",
      "Member meronyms: []\n",
      "Substance meronyms: []\n",
      "Examples: []\n",
      "\n",
      "The synset = Synset('andiron.n.01')\n",
      "The synonyms =  [Lemma('andiron.n.01.andiron'), Lemma('andiron.n.01.firedog'), Lemma('andiron.n.01.dog'), Lemma('andiron.n.01.dog-iron')]\n",
      "The definition = metal supports for logs in a fireplace\n",
      "The depth of its hypernym chain is =  8\n",
      "Hypernyms: [Synset('support.n.10')]\n",
      "Hyponyms: []\n",
      "Part holonyms: []\n",
      "Member holonyms: []\n",
      "Substance holonyms: []\n",
      "Part meronyms: []\n",
      "Member meronyms: []\n",
      "Substance meronyms: []\n",
      "Examples: ['the andirons were too hot to touch']\n",
      "\n",
      "The synset = Synset('chase.v.01')\n",
      "The synonyms =  [Lemma('chase.v.01.chase'), Lemma('chase.v.01.chase_after'), Lemma('chase.v.01.trail'), Lemma('chase.v.01.tail'), Lemma('chase.v.01.tag'), Lemma('chase.v.01.give_chase'), Lemma('chase.v.01.dog'), Lemma('chase.v.01.go_after'), Lemma('chase.v.01.track')]\n",
      "The definition = go after with the intent to catch\n",
      "The depth of its hypernym chain is =  2\n",
      "Hypernyms: [Synset('pursue.v.02')]\n",
      "Hyponyms: [Synset('hound.v.01'), Synset('quest.v.02'), Synset('run_down.v.07'), Synset('tree.v.03')]\n",
      "Part holonyms: []\n",
      "Member holonyms: []\n",
      "Substance holonyms: []\n",
      "Part meronyms: []\n",
      "Member meronyms: []\n",
      "Substance meronyms: []\n",
      "Examples: ['The policeman chased the mugger down the alley', 'the dog chased the rabbit']\n"
     ]
    }
   ],
   "source": [
    "for doggy_synset in dog_synsets:\n",
    "    print()\n",
    "    print('The synset =', doggy_synset)\n",
    "    print('The synonyms = ', doggy_synset.lemmas())\n",
    "    print('The definition =', doggy_synset.definition())\n",
    "    print('The depth of its hypernym chain is = ', doggy_synset.max_depth())\n",
    "\n",
    "    print('Hypernyms:', doggy_synset.hypernyms())\n",
    "    print('Hyponyms:', doggy_synset.hyponyms())\n",
    "\n",
    "    #### Part - to -  whole relations:\n",
    "    print('Part holonyms:',doggy_synset.part_holonyms())\n",
    "    print('Member holonyms:',doggy_synset.member_holonyms())\n",
    "    print('Substance holonyms:',doggy_synset.substance_holonyms())\n",
    "\n",
    "    ### Whole - to - part relations\n",
    "    print('Part meronyms:',doggy_synset.part_meronyms())\n",
    "    print('Member meronyms:',doggy_synset.member_meronyms())\n",
    "    print('Substance meronyms:',doggy_synset.substance_meronyms())\n",
    "\n",
    "    #### Examples\n",
    "    print('Examples:', doggy_synset.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all relations make sense for all parts-of-speech. Verbs have ```causes``` and ```entailments``` instead of the meronymy relations of nouns. Below we show the relationas for the verbal synset ```bake```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The synset = Synset('bake.v.01')\n",
      "The synonyms =  [Lemma('bake.v.01.bake')]\n",
      "The definition = cook and make edible by putting in a hot oven\n",
      "The depth of its hypernym chain is =  3\n",
      "Hypernyms: [Synset('cook.v.03')]\n",
      "Hyponyms: [Synset('fire.v.03'), Synset('ovenbake.v.01'), Synset('shirr.v.01')]\n",
      "Caused: []\n",
      "Entailments: []\n",
      "Examples: ['bake the potatoes']\n",
      "\n",
      "The synset = Synset('bake.v.02')\n",
      "The synonyms =  [Lemma('bake.v.02.bake')]\n",
      "The definition = prepare with dry heat in an oven\n",
      "The depth of its hypernym chain is =  2\n",
      "Hypernyms: [Synset('create_from_raw_material.v.01')]\n",
      "Hyponyms: []\n",
      "Caused: []\n",
      "Entailments: []\n",
      "Examples: ['bake a cake']\n",
      "\n",
      "The synset = Synset('broil.v.02')\n",
      "The synonyms =  [Lemma('broil.v.02.broil'), Lemma('broil.v.02.bake')]\n",
      "The definition = heat by a natural force\n",
      "The depth of its hypernym chain is =  2\n",
      "Hypernyms: [Synset('heat.v.01')]\n",
      "Hyponyms: []\n",
      "Caused: []\n",
      "Entailments: []\n",
      "Examples: ['The sun broils the valley in the summer']\n",
      "\n",
      "The synset = Synset('bake.v.04')\n",
      "The synonyms =  [Lemma('bake.v.04.bake'), Lemma('bake.v.04.broil')]\n",
      "The definition = be very hot, due to hot weather or exposure to the sun\n",
      "The depth of its hypernym chain is =  1\n",
      "Hypernyms: [Synset('be.v.01')]\n",
      "Hyponyms: []\n",
      "Caused: []\n",
      "Entailments: []\n",
      "Examples: ['The town was broiling in the sun', 'the tourists were baking in the heat']\n"
     ]
    }
   ],
   "source": [
    "bake_synsets = wn.synsets(lemma=\"bake\", pos=\"v\")\n",
    "\n",
    "for bake_synset in bake_synsets:\n",
    "    print()\n",
    "    print('The synset =', bake_synset)\n",
    "    print('The synonyms = ', bake_synset.lemmas())\n",
    "    print('The definition =', bake_synset.definition())\n",
    "    print('The depth of its hypernym chain is = ', bake_synset.max_depth())\n",
    "\n",
    "    print('Hypernyms:', bake_synset.hypernyms())\n",
    "    print('Hyponyms:', bake_synset.hyponyms())\n",
    "    \n",
    "    #### Relations for verbal synsets\n",
    "    print('Caused:', bake_synset.causes())\n",
    "    print('Entailments:',bake_synset.entailments())\n",
    "\n",
    "    #### Examples\n",
    "    print('Examples:', bake_synset.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjectives in WordNet do not have hyponymy and hypernym relations between synsets but only similar-to. Below, we only look for adjectival synsets of the lemma ```sad```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The synset = Synset('sad.a.01')\n",
      "The synonyms =  [Lemma('sad.a.01.sad')]\n",
      "The definition = experiencing or showing sorrow or unhappiness; ; - Christina Rossetti\n",
      "Similar to [Synset('bittersweet.s.01'), Synset('doleful.s.01'), Synset('heavyhearted.s.01'), Synset('melancholy.s.01'), Synset('pensive.s.02'), Synset('tragic.s.01'), Synset('tragicomic.s.02')]\n",
      "Examples: ['feeling sad because his dog had died', 'Better by far that you should forget and smile / Than that you should remember and be sad']\n",
      "\n",
      "The synset = Synset('sad.s.02')\n",
      "The synonyms =  [Lemma('sad.s.02.sad')]\n",
      "The definition = of things that make you feel sad; ; ; ; - Christina Rossetti\n",
      "Similar to [Synset('sorrowful.a.01')]\n",
      "Examples: ['sad news', \"she doesn't like sad movies\", 'it was a very sad story', 'When I am dead, my dearest, / Sing no sad songs for me']\n",
      "\n",
      "The synset = Synset('deplorable.s.01')\n",
      "The synonyms =  [Lemma('deplorable.s.01.deplorable'), Lemma('deplorable.s.01.distressing'), Lemma('deplorable.s.01.lamentable'), Lemma('deplorable.s.01.pitiful'), Lemma('deplorable.s.01.sad'), Lemma('deplorable.s.01.sorry')]\n",
      "The definition = bad; unfortunate\n",
      "Similar to [Synset('bad.a.01')]\n",
      "Examples: ['my finances were in a deplorable state', 'a lamentable decision', 'her clothes were in sad shape', 'a sorry state of affairs']\n"
     ]
    }
   ],
   "source": [
    "sad_synsets = wn.synsets(lemma=\"sad\", pos=\"a\")\n",
    "\n",
    "for sad_synset in sad_synsets:\n",
    "    print()\n",
    "    print('The synset =', sad_synset)\n",
    "    print('The synonyms = ', sad_synset.lemmas())\n",
    "    print('The definition =', sad_synset.definition())\n",
    "\n",
    "    #### Relations for adjectival synsets\n",
    "    print('Similar to', sad_synset.similar_tos())\n",
    "\n",
    "    #### Examples\n",
    "    print('Examples:', sad_synset.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to synset-to-synset relations, some relations are expressed between lemmas, specifically derivational relations and antonymy relations. These relations are relevant for all parts-of-speech.\n",
    "Below we show what we get for the adjectival meanings of ```sweet```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The definition = having or denoting the characteristic taste of sugar\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = having a sweet nature befitting an angel or cherub\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = pleasing to the ear\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = pleasing to the senses\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = pleasing to the mind or feeling\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = having a natural fragrance\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = (used of wines) having a high residual sugar content\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = not containing or composed of salt water\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = not soured or preserved\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "\n",
      "The definition = with sweetening added\n",
      "Synonym Lemma('sugared.s.01.sugared')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweetened')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n",
      "Synonym Lemma('sugared.s.01.sweet')\n",
      "\tAntonymns []\n",
      "\tDerivations [Lemma('sweetness.n.02.sweetness')]\n",
      "Synonym Lemma('sugared.s.01.sweet-flavored')\n",
      "\tAntonymns []\n",
      "\tDerivations []\n"
     ]
    }
   ],
   "source": [
    "sweet_synsets = wn.synsets(lemma=\"sweet\", pos=\"a\")\n",
    "\n",
    "for sweet_synset in sweet_synsets:\n",
    "    print()\n",
    "    print('The definition =', sweet_synset.definition())\n",
    "    for lemma in bad_synset.lemmas():\n",
    "        print(\"Synonym\", lemma)\n",
    "        # Antonyms\n",
    "        print('\\tAntonymns', lemma.antonyms())\n",
    "        print('\\tDerivations', lemma.derivationally_related_forms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ```antonyms``` and ```derivations``` are only occasionally given for synonyms of \"sweet\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hypernym classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **hypernym_paths** gives all the `hypernym` relations upward starting from a given synset untill there are no more hypernyms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('animal.n.01'), Synset('chordate.n.01'), Synset('vertebrate.n.01'), Synset('mammal.n.01'), Synset('placental.n.01'), Synset('carnivore.n.01'), Synset('feline.n.01'), Synset('cat.n.01')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_synsets = wn.synsets(lemma=\"cat\", pos=\"n\")\n",
    "first_cat_synset = cat_synsets[0]\n",
    "for path in first_cat_synset.hypernym_paths():\n",
    "    print(path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code makes it easier to see the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Synset('entity.n.01')\n",
      "    Synset('physical_entity.n.01')\n",
      "       Synset('object.n.01')\n",
      "          Synset('whole.n.02')\n",
      "             Synset('living_thing.n.01')\n",
      "                Synset('organism.n.01')\n",
      "                   Synset('animal.n.01')\n",
      "                      Synset('chordate.n.01')\n",
      "                         Synset('vertebrate.n.01')\n",
      "                            Synset('mammal.n.01')\n",
      "                               Synset('placental.n.01')\n",
      "                                  Synset('carnivore.n.01')\n",
      "                                     Synset('feline.n.01')\n",
      "                                        Synset('cat.n.01')\n"
     ]
    }
   ],
   "source": [
    "for path in first_cat_synset.hypernym_paths():\n",
    "    indent = \"\"\n",
    "    for hyper in path:\n",
    "        print(indent, hyper)\n",
    "        indent += \"   \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there can be multiple hypernym chains for a synset because occasionally WordNet gives multiple hypernyms for a synset. \n",
    "\n",
    "If you examine the output of the **hypernym_paths** function more closely, you will see it is actually a list of lists.\n",
    "\n",
    "In the case of the first sense of dog, we get two different paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Synset('entity.n.01')\n",
      "    Synset('physical_entity.n.01')\n",
      "       Synset('object.n.01')\n",
      "          Synset('whole.n.02')\n",
      "             Synset('living_thing.n.01')\n",
      "                Synset('organism.n.01')\n",
      "                   Synset('animal.n.01')\n",
      "                      Synset('chordate.n.01')\n",
      "                         Synset('vertebrate.n.01')\n",
      "                            Synset('mammal.n.01')\n",
      "                               Synset('placental.n.01')\n",
      "                                  Synset('carnivore.n.01')\n",
      "                                     Synset('canine.n.02')\n",
      "                                        Synset('dog.n.01')\n",
      " Synset('entity.n.01')\n",
      "    Synset('physical_entity.n.01')\n",
      "       Synset('object.n.01')\n",
      "          Synset('whole.n.02')\n",
      "             Synset('living_thing.n.01')\n",
      "                Synset('organism.n.01')\n",
      "                   Synset('animal.n.01')\n",
      "                      Synset('domestic_animal.n.01')\n",
      "                         Synset('dog.n.01')\n"
     ]
    }
   ],
   "source": [
    "first_dog_synset = dog_synsets[0]\n",
    "for path in first_dog_synset.hypernym_paths():\n",
    "    indent = \"\"\n",
    "    for hyper in path:\n",
    "        print(indent, hyper)\n",
    "        indent += \"   \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that dog in its first sense has two pathes due to the hypernyms **canine.n.02** and **domestic_animal.n.01**. Two different ways of classifying dogs that eventually end up in the same top nodes from **animal.n.01** onwards but provide different routes to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from the lecture that the verbs have hypernyms but they do not have an augmented top layer in WordNet but the verb subnetwork consists of many isolated subgraphs as islands. So let's see what the hypernym path is for a verbal synset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Synset('travel.v.01') change location; move, travel, or proceed, also metaphorically\n"
     ]
    }
   ],
   "source": [
    "first_travel_synset = wn.synsets(lemma=\"travel\", pos=\"v\")[0]\n",
    "for path in first_travel_synset.hypernym_paths():\n",
    "    indent = \"\"\n",
    "    for hyper in path:\n",
    "        print(indent, hyper, hyper.definition())\n",
    "        indent += \"   \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path is very short and not so informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypernym_path function only looks upwards. We can also traverse the network downwards by getting all the hyponyms. We can get more insight in the coverage of specific areas or semantic fields by collecting all the hyponyms \"below\" a synset, e.g. all types of dogs, cats, horses, ships, cars, etc. The next simple \"for\" loop iterates over all first sense dog-hyponyms in the English wordnet and prints the definition and next the synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dogs: 18\n",
      "\n",
      "Synset('basenji.n.01')\n",
      "small smooth-haired breed of African origin having a tightly curled tail and the inability to bark\n",
      "English: ['basenji']\n",
      "\n",
      "Synset('corgi.n.01')\n",
      "either of two Welsh breeds of long-bodied short-legged dogs with erect ears and a fox-like head\n",
      "English: ['corgi', 'Welsh_corgi']\n",
      "\n",
      "Synset('cur.n.01')\n",
      "an inferior dog or one of mixed breed\n",
      "English: ['cur', 'mongrel', 'mutt']\n",
      "\n",
      "Synset('dalmatian.n.02')\n",
      "a large breed having a smooth white coat with black or brown spots; originated in Dalmatia\n",
      "English: ['dalmatian', 'coach_dog', 'carriage_dog']\n",
      "\n",
      "Synset('great_pyrenees.n.01')\n",
      "bred of large heavy-coated white dogs resembling the Newfoundland\n",
      "English: ['Great_Pyrenees']\n",
      "\n",
      "Synset('griffon.n.02')\n",
      "breed of various very small compact wiry-coated dogs of Belgian origin having a short bearded muzzle\n",
      "English: ['griffon', 'Brussels_griffon', 'Belgian_griffon']\n",
      "\n",
      "Synset('hunting_dog.n.01')\n",
      "a dog used in hunting game\n",
      "English: ['hunting_dog']\n",
      "\n",
      "Synset('lapdog.n.01')\n",
      "a dog small and tame enough to be held in the lap\n",
      "English: ['lapdog']\n",
      "\n",
      "Synset('leonberg.n.01')\n",
      "a large dog (usually with a golden coat) produced by crossing a St Bernard and a Newfoundland\n",
      "English: ['Leonberg']\n",
      "\n",
      "Synset('mexican_hairless.n.01')\n",
      "any of an old breed of small nearly hairless dogs of Mexico\n",
      "English: ['Mexican_hairless']\n",
      "\n",
      "Synset('newfoundland.n.01')\n",
      "a breed of very large heavy dogs with a thick coarse usually black coat; highly intelligent dogs and vigorous swimmers; developed in Newfoundland\n",
      "English: ['Newfoundland', 'Newfoundland_dog']\n",
      "\n",
      "Synset('pooch.n.01')\n",
      "informal terms for dogs\n",
      "English: ['pooch', 'doggie', 'doggy', 'barker', 'bow-wow']\n",
      "\n",
      "Synset('poodle.n.01')\n",
      "an intelligent dog with a heavy curly solid-colored coat that is usually clipped; an old breed sometimes trained as sporting dogs or as performing dogs\n",
      "English: ['poodle', 'poodle_dog']\n",
      "\n",
      "Synset('pug.n.01')\n",
      "small compact smooth-coated breed of Asiatic origin having a tightly curled tail and broad flat wrinkled muzzle\n",
      "English: ['pug', 'pug-dog']\n",
      "\n",
      "Synset('puppy.n.01')\n",
      "a young dog\n",
      "English: ['puppy']\n",
      "\n",
      "Synset('spitz.n.01')\n",
      "any of various stocky heavy-coated breeds of dogs native to northern regions having pointed muzzles and erect ears with a curled furry tail\n",
      "English: ['spitz']\n",
      "\n",
      "Synset('toy_dog.n.01')\n",
      "any of several breeds of very small dogs kept purely as pets\n",
      "English: ['toy_dog', 'toy']\n",
      "\n",
      "Synset('working_dog.n.01')\n",
      "any of several breeds of usually large powerful dogs bred to work as draft animals and guard and guide dogs\n",
      "English: ['working_dog']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dogs = first_dog_synset.hyponyms()\n",
    "print('Number of dogs:', len(dogs))\n",
    "print()\n",
    "for s in dogs:\n",
    "    print(s)\n",
    "    print(s.definition())\n",
    "    print('English:', s.lemma_names())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code only gives the direct hyponyms as dogs but maybe there are more dogs as hyponyms of these hyponyms or even deeper down. To get these, we need a recursive function. A recursive function is a function that calls itself inside. These functions are extremely powerful and also elegant. But this comes with a risk: if you do not built in a way to stop the calling, it will run for ever and may consume all your memory.\n",
    "\n",
    "We define a function that takes a synset and first gets the hyponyms as children and next for each child calls the function again to get the grand children. If there are any grand children these are also added to the family, if not the call is done and we return to the next child.\n",
    "\n",
    "The function terminates if all children have been processed. Because it is recursive, the same applies for the function applied to the children, and the children of the children, etc. Since the graph ends, the function also ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponym_descendants (parent):\n",
    "    descendants=[]\n",
    "    children = parent.hyponyms()\n",
    "    if children:\n",
    "        descendants = descendants + children\n",
    "        for child in children:\n",
    "            grand_children = get_hyponym_descendants(child)\n",
    "            if grand_children:\n",
    "                descendants = descendants + grand_children\n",
    "    return descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_descendants = get_hyponym_descendants(first_dog_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('dog.n.01')\n",
      "Number of dog descendants: 189\n"
     ]
    }
   ],
   "source": [
    "print(first_dog_synset)\n",
    "print('Number of dog descendants:', len(dog_descendants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another function that just gets the synonyms from all these synsets. This function iterates over all the synsets in the \"descendants\", gets the Wordnet Lemma objects and finally gets the lemmas for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas_from_synsets(synsets):\n",
    "    lemmas = []\n",
    "    for synset in synsets:\n",
    "        slemmas = synset.lemma_names()\n",
    "        for slemma in slemmas:\n",
    "            lemmas.append(slemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are so many dogs in WordNet: 279\n",
      "['basenji', 'corgi', 'Welsh_corgi', 'cur', 'mongrel', 'mutt', 'dalmatian', 'coach_dog', 'carriage_dog', 'Great_Pyrenees', 'griffon', 'Brussels_griffon', 'Belgian_griffon', 'hunting_dog', 'lapdog', 'Leonberg', 'Mexican_hairless', 'Newfoundland', 'Newfoundland_dog', 'pooch', 'doggie', 'doggy', 'barker', 'bow-wow', 'poodle', 'poodle_dog', 'pug', 'pug-dog', 'puppy', 'spitz', 'toy_dog', 'toy', 'working_dog', 'Cardigan', 'Cardigan_Welsh_corgi', 'Pembroke', 'Pembroke_Welsh_corgi', 'feist', 'fice', 'pariah_dog', 'pye-dog', 'pie-dog', 'liver-spotted_dalmatian', 'Brabancon_griffon', 'courser', 'dachshund', 'dachsie', 'badger_dog', 'hound', 'hound_dog', 'Rhodesian_ridgeback', 'sporting_dog', 'gun_dog', 'terrier', 'sausage_dog', 'sausage_hound', 'Afghan_hound', 'Afghan', 'basset', 'basset_hound', 'beagle', 'bloodhound', 'sleuthhound', 'bluetick', 'boarhound', 'coonhound', 'foxhound', 'greyhound', 'harrier', 'Ibizan_hound', 'Ibizan_Podenco', 'Norwegian_elkhound', 'elkhound', 'otterhound', 'otter_hound', 'Plott_hound', 'redbone', 'Saluki', 'gazelle_hound', 'Scottish_deerhound', 'deerhound', 'staghound', 'Weimaraner', 'wolfhound', 'black-and-tan_coonhound', 'coondog', 'American_foxhound', 'English_foxhound', 'Walker_hound', 'Walker_foxhound', 'Italian_greyhound', 'whippet', 'borzoi', 'Russian_wolfhound', 'Irish_wolfhound', 'bird_dog', 'griffon', 'wire-haired_pointing_griffon', 'pointer', 'Spanish_pointer', 'retriever', 'setter', 'spaniel', 'water_dog', 'German_short-haired_pointer', 'vizsla', 'Hungarian_pointer', 'Chesapeake_Bay_retriever', 'curly-coated_retriever', 'flat-coated_retriever', 'golden_retriever', 'Labrador_retriever', 'English_setter', 'Gordon_setter', 'Irish_setter', 'red_setter', 'Brittany_spaniel', 'clumber', 'clumber_spaniel', 'cocker_spaniel', 'English_cocker_spaniel', 'cocker', 'field_spaniel', 'springer_spaniel', 'springer', 'Sussex_spaniel', 'water_spaniel', 'English_springer', 'English_springer_spaniel', 'Welsh_springer_spaniel', 'American_water_spaniel', 'Irish_water_spaniel', 'Airedale', 'Airedale_terrier', 'Australian_terrier', 'Bedlington_terrier', 'Border_terrier', 'Boston_bull', 'Boston_terrier', 'bullterrier', 'bull_terrier', 'cairn', 'cairn_terrier', 'Dandie_Dinmont', 'Dandie_Dinmont_terrier', 'fox_terrier', 'Irish_terrier', 'Kerry_blue_terrier', 'Lhasa', 'Lhasa_apso', 'Norfolk_terrier', 'Norwich_terrier', 'rat_terrier', 'ratter', 'schnauzer', 'Scotch_terrier', 'Scottish_terrier', 'Scottie', 'silky_terrier', 'Sydney_silky', 'Skye_terrier', 'soft-coated_wheaten_terrier', 'Tibetan_terrier', 'chrysanthemum_dog', 'West_Highland_white_terrier', 'wirehair', 'wirehaired_terrier', 'wire-haired_terrier', 'Yorkshire_terrier', 'American_Staffordshire_terrier', 'Staffordshire_terrier', 'American_pit_bull_terrier', 'pit_bull_terrier', 'Staffordshire_bullterrier', 'Staffordshire_bull_terrier', 'smooth-haired_fox_terrier', 'wire-haired_fox_terrier', 'Manchester_terrier', 'black-and-tan_terrier', 'toy_Manchester', 'toy_Manchester_terrier', 'giant_schnauzer', 'miniature_schnauzer', 'standard_schnauzer', 'Clydesdale_terrier', 'Lakeland_terrier', 'Welsh_terrier', 'Sealyham_terrier', 'Sealyham', 'large_poodle', 'miniature_poodle', 'standard_poodle', 'toy_poodle', 'chow', 'chow_chow', 'keeshond', 'Pomeranian', 'Samoyed', 'Samoyede', 'Chihuahua', 'Japanese_spaniel', 'Maltese_dog', 'Maltese_terrier', 'Maltese', 'Pekinese', 'Pekingese', 'Peke', 'Shih-Tzu', 'toy_spaniel', 'toy_terrier', 'English_toy_spaniel', 'King_Charles_spaniel', 'papillon', 'Blenheim_spaniel', 'boxer', 'bull_mastiff', 'bulldog', 'English_bulldog', 'Eskimo_dog', 'husky', 'Great_Dane', 'guide_dog', 'hearing_dog', 'mastiff', 'police_dog', 'Saint_Bernard', 'St_Bernard', 'seizure-alert_dog', 'Sennenhunde', 'shepherd_dog', 'sheepdog', 'sheep_dog', 'sled_dog', 'sledge_dog', 'watchdog', 'guard_dog', 'French_bulldog', 'Seeing_Eye_dog', 'Tibetan_mastiff', 'Appenzeller', 'Bernese_mountain_dog', 'EntleBucher', 'Greater_Swiss_Mountain_dog', 'Belgian_sheepdog', 'Belgian_shepherd', 'Border_collie', 'Bouvier_des_Flandres', 'Bouviers_des_Flandres', 'briard', 'collie', 'German_shepherd', 'German_shepherd_dog', 'German_police_dog', 'alsatian', 'kelpie', 'komondor', 'Old_English_sheepdog', 'bobtail', 'Rottweiler', 'Shetland_sheepdog', 'Shetland_sheep_dog', 'Shetland', 'groenendael', 'malinois', 'malamute', 'malemute', 'Alaskan_malamute', 'Siberian_husky', 'attack_dog', 'housedog', 'kuvasz', 'pinscher', 'schipperke', 'affenpinscher', 'monkey_pinscher', 'monkey_dog', 'Doberman', 'Doberman_pinscher', 'miniature_pinscher']\n"
     ]
    }
   ],
   "source": [
    "### Get the words for dogs in English at any level of specificity\n",
    "dog_lemmas = get_lemmas_from_synsets(dog_descendants)\n",
    "print('There are so many dogs in WordNet:', len(dog_lemmas))\n",
    "print(dog_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if the WordNet builders made a mistake and made a hyponym also the hypernym of a concept?\n",
    "\n",
    "dog -is-a-> working_dog -is-a-> dog\n",
    "\n",
    "This would create a cycle and our function will never terminate. This is the danger of recursive functions that do not have a clear stopping condition. Your memory will get loaded and at some point everythiong slows down. Eventually, the application crashes and you may have to restart your computer. Not a real disaster, since you will not destroy anything but you may not have saved your work in other application or in this notebook. Let's hope that the WordNet builders did not make a mistake. To pervent this from happening, you should check if a synset was already added and stop if this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would apply the above to the first synset of \"person\", you get an enormous amount of words that can be used to refer to a person in English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person direct hyponyms: 402 synsets, 639 lemmas\n",
      "Person descendant hyponyms: 7981 synsets, 13229 lemmas\n"
     ]
    }
   ],
   "source": [
    "first_person_synset = wn.synsets(\"person\")[0]\n",
    "person_hyponyms = first_person_synset.hyponyms() \n",
    "person_hyponyms_words = get_lemmas_from_synsets(person_hyponyms)\n",
    "person_descendants = get_hyponym_descendants(first_person_synset)\n",
    "person_descendants_words = get_lemmas_from_synsets(person_descendants)\n",
    "print('Person direct hyponyms:', len(person_hyponyms), 'synsets,',  len(person_hyponyms_words), 'lemmas')\n",
    "print('Person descendant hyponyms:', len(person_descendants), 'synsets,', len(person_descendants_words), 'lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abator', 'abjurer', 'abomination', 'abstainer', 'abstinent', 'nondrinker', 'achiever', 'winner', 'success', 'succeeder', 'acquaintance', 'friend', 'acquirer', 'active', 'actor', 'doer', 'worker', 'adjudicator', 'admirer', 'adoptee', 'adult', 'grownup', 'adventurer', 'venturer', 'adversary', 'antagonist', 'opponent', 'opposer', 'resister', 'advisee', 'advocate', 'advocator', 'proponent', 'exponent', 'affiant', 'African', 'agnostic', 'doubter', 'amateur', 'Amerindian', 'Native_American', 'ancient', 'anomaly', 'unusual_person', 'anti-American', 'anti', 'applicant', 'applier', 'appointee', 'appointment', 'appreciator', 'apprehender', 'Aquarius', 'Water_Bearer', 'archaist', 'Aries', 'Ram', 'arrogator', 'assessee', 'asthmatic', 'authority', 'autodidact', 'baby_boomer', 'boomer', 'baby_buster', 'buster', 'bad_guy', 'bad_person', 'baldhead', 'baldpate', 'baldy', 'balker', 'baulker', 'noncompliant', 'bather', 'beard', 'bedfellow', 'bereaved', 'bereaved_person', 'best', 'topper', 'birth', 'biter', 'Black', 'Black_person', 'blackamoor', 'Negro', 'Negroid', 'blogger', 'blond', 'blonde', 'bluecoat', 'bodybuilder', 'muscle_builder', 'muscle-builder', 'musclebuilder', 'muscleman', 'bomber', 'brunet', 'brunette', 'bullfighter', 'toreador', 'buster', 'Cancer', 'Crab', 'candidate', 'prospect', 'capitalist', 'Capricorn', 'Goat', 'captor', 'capturer', 'case', 'cashier', 'celebrant', 'celebrator', 'celebrater', 'censor', 'chameleon', 'changer', 'modifier', 'charmer', 'beguiler', 'child', 'baby', 'chutzpanik', 'closer', 'clumsy_person', 'collector', 'aggregator', 'color-blind_person', 'combatant', 'battler', 'belligerent', 'fighter', 'scrapper', 'commoner', 'common_man', 'common_person', 'communicator', 'complexifier', 'compulsive', 'computer_user', 'contemplative', 'contestant', 'convert', 'copycat', 'imitator', 'emulator', 'ape', 'aper', 'counter', 'counterterrorist', 'coward', 'crawler', 'creeper', 'creator', 'creature', 'wight', 'creditor', 'cripple', 'dancer', 'social_dancer', 'dead_person', 'dead_soul', 'deceased_person', 'deceased', 'decedent', 'departed', 'deaf_person', 'debaser', 'degrader', 'debtor', 'debitor', 'defecator', 'voider', 'shitter', 'delayer', 'deliverer', 'demander', 'dieter', 'differentiator', 'discriminator', 'disentangler', 'unraveler', 'unraveller', 'disputant', 'controversialist', 'eristic', 'dissenter', 'dissident', 'protester', 'objector', 'contestant', 'divider', 'domestic_partner', 'significant_other', 'spousal_equivalent', 'spouse_equivalent', 'double']\n"
     ]
    }
   ],
   "source": [
    "print(person_descendants_words[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wordnet Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of wordnet as a graph can be used to measure the similarity across concepts. The basic idea is that concepts can be connected by going up and down through the relations. By counting the steps, we can measure the distance between for example **car**, **train**, **man**, **woman**. In the image below, we can infer that \"train#1\" and \"bus#1\" are similar because they share the same hypernym \"public_transport\", but to get to e.g. \"knowledge\" requires many steps through the graph which makes \"knowledge\" very dissimilar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![distance_in_wordnet](./images/wordnet_sim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A whole series of similarity functions have been added to NLTK that measure the distances in different ways but use the same basic strategy that exploits the relations between synsets. See the documentation for the other methods. \n",
    "\n",
    "We show here how it works for the most basic method **path** that counts the steps. We first obtain a few synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "car = wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the full hypernym path for these synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs are a type of:\n",
      " Synset('entity.n.01')\n",
      "    Synset('physical_entity.n.01')\n",
      "       Synset('object.n.01')\n",
      "          Synset('whole.n.02')\n",
      "             Synset('living_thing.n.01')\n",
      "                Synset('organism.n.01')\n",
      "                   Synset('animal.n.01')\n",
      "                      Synset('chordate.n.01')\n",
      "                         Synset('vertebrate.n.01')\n",
      "                            Synset('mammal.n.01')\n",
      "                               Synset('placental.n.01')\n",
      "                                  Synset('carnivore.n.01')\n",
      "                                     Synset('canine.n.02')\n",
      "                                        Synset('dog.n.01')\n",
      " Synset('entity.n.01')\n",
      "    Synset('physical_entity.n.01')\n",
      "       Synset('object.n.01')\n",
      "          Synset('whole.n.02')\n",
      "             Synset('living_thing.n.01')\n",
      "                Synset('organism.n.01')\n",
      "                   Synset('animal.n.01')\n",
      "                      Synset('domestic_animal.n.01')\n",
      "                         Synset('dog.n.01')\n",
      "cats are a type of:\n",
      " Synset('entity.n.01')\n",
      "    Synset('physical_entity.n.01')\n",
      "       Synset('object.n.01')\n",
      "          Synset('whole.n.02')\n",
      "             Synset('living_thing.n.01')\n",
      "                Synset('organism.n.01')\n",
      "                   Synset('animal.n.01')\n",
      "                      Synset('chordate.n.01')\n",
      "                         Synset('vertebrate.n.01')\n",
      "                            Synset('mammal.n.01')\n",
      "                               Synset('placental.n.01')\n",
      "                                  Synset('carnivore.n.01')\n",
      "                                     Synset('feline.n.01')\n",
      "                                        Synset('cat.n.01')\n"
     ]
    }
   ],
   "source": [
    "print('dogs are a type of:')\n",
    "for path in dog.hypernym_paths():\n",
    "    indent = \"\"\n",
    "    for hyper in path:\n",
    "        print(indent, hyper)\n",
    "        indent += \"   \"\n",
    "\n",
    "print('cats are a type of:')\n",
    "for path in cat.hypernym_paths():\n",
    "    indent = \"\"\n",
    "    for hyper in path:\n",
    "        print(indent, hyper)\n",
    "        indent += \"   \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we see that **cat** has only one path and we have seen that **dog** has two. The **cat** path is most similar to the **canine** path of **dog**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `path_similarity` function of a synset, which requires as input another synset. So let's get the score for dog and cat, where the function will use the shortest path from alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(dog.path_similarity(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this very similar? The only way to find out is to compare this with something else such as a **car**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "car = wn.synset('car.n.01')\n",
    "print(dog.path_similarity(car))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, this score is a lot lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this also works for verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n",
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "hit = wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')\n",
    "print(hit.path_similarity(slap))\n",
    "print(wn.path_similarity(hit, slap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work well but......\n",
    "\n",
    "If you did some readings on WordNet, you know that the noun hierarchy has a single top-node synset 'entity-n-01'. All nominal synsets decent from this synset. This is not the case for verbs nor for adjectives. The verb part of WordNet therefore consists of '559' islands of disconnected synsets with 559 rootnodes. The English WordNet editors decided not to connect these islands in an artificial way as was done for nouns. We can see this when we get the hypernym path for each of the above synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit is a type of:\n",
      " Synset('move.v.02')\n",
      "    Synset('propel.v.01')\n",
      "       Synset('hit.v.01')\n",
      "slap is a type of:\n",
      " Synset('touch.v.01')\n",
      "    Synset('strike.v.01')\n",
      "       Synset('slap.v.01')\n"
     ]
    }
   ],
   "source": [
    "print('hit is a type of:')\n",
    "for path in hit.hypernym_paths():\n",
    "    indent = \"\"\n",
    "    for hyper in path:\n",
    "        print(indent, hyper)\n",
    "        indent += \"   \"\n",
    "print('slap is a type of:')\n",
    "for path in slap.hypernym_paths():\n",
    "    indent = \"\"\n",
    "    for hyper in path:\n",
    "        print(indent, hyper)\n",
    "        indent += \"   \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the top synsets **move** and **touch** are not connected in any way and there is no overlap between the pathes for the two verb synsets.\n",
    "\n",
    "We can also see this using the NLTK root_hypernyms() function for the above noun and verb synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root for dog: [Synset('entity.n.01')]\n",
      "Root for cat: [Synset('entity.n.01')]\n",
      "Root for slap: [Synset('touch.v.01')]\n",
      "Root for hit: [Synset('move.v.02')]\n"
     ]
    }
   ],
   "source": [
    "print('Root for dog:', dog.root_hypernyms())\n",
    "print('Root for cat:', cat.root_hypernyms())\n",
    "print('Root for slap:', slap.root_hypernyms())\n",
    "print('Root for hit:', hit.root_hypernyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is it still possible to get a value for similarity if the subgraphs are not connected? Well, the package imposes a simulated root node by grouping all the subgraph top-nodes under a single node. This is the default setting. If you do not want to use this, you can turn it off by setting the parameter *simulate_root* to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(hit.path_similarity(slap, simulate_root=False))\n",
    "print(wn.path_similarity(hit, slap, simulate_root=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the simulated root there is no path from 'hit' to 'slap'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using WordNet similarity for words instead of synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we also determine the similarity of words?\n",
    "\n",
    "Yes we can but we first need to obtain all the synsets for a word and then compare each synset with the synsets of another word to get the most similar meanings of these words. \n",
    "We therefore need a **for-loop** inside a **for-loop**. The first loop gets the synsets for the first word and the second loop for each synset gets the synsets for the second word to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('dog.n.01') :\n",
      "\t Synset('cat.n.01') : 0.2\n",
      "\t Synset('guy.n.01') : 0.125\n",
      "\t Synset('cat.n.03') : 0.125\n",
      "\t Synset('kat.n.01') : 0.07692307692307693\n",
      "\t Synset('cat-o'-nine-tails.n.01') : 0.08333333333333333\n",
      "\t Synset('caterpillar.n.02') : 0.07692307692307693\n",
      "\t Synset('big_cat.n.01') : 0.2\n",
      "\t Synset('computerized_tomography.n.01') : 0.05263157894736842\n",
      "Synset('frump.n.01') :\n",
      "\t Synset('cat.n.01') : 0.07142857142857142\n",
      "\t Synset('guy.n.01') : 0.125\n",
      "\t Synset('cat.n.03') : 0.125\n",
      "\t Synset('kat.n.01') : 0.1\n",
      "\t Synset('cat-o'-nine-tails.n.01') : 0.07142857142857142\n",
      "\t Synset('caterpillar.n.02') : 0.06666666666666667\n",
      "\t Synset('big_cat.n.01') : 0.07142857142857142\n",
      "\t Synset('computerized_tomography.n.01') : 0.05555555555555555\n",
      "Synset('dog.n.03') :\n",
      "\t Synset('cat.n.01') : 0.07692307692307693\n",
      "\t Synset('guy.n.01') : 0.2\n",
      "\t Synset('cat.n.03') : 0.14285714285714285\n",
      "\t Synset('kat.n.01') : 0.1111111111111111\n",
      "\t Synset('cat-o'-nine-tails.n.01') : 0.07692307692307693\n",
      "\t Synset('caterpillar.n.02') : 0.07142857142857142\n",
      "\t Synset('big_cat.n.01') : 0.07692307692307693\n",
      "\t Synset('computerized_tomography.n.01') : 0.058823529411764705\n",
      "Synset('cad.n.01') :\n",
      "\t Synset('cat.n.01') : 0.07692307692307693\n",
      "\t Synset('guy.n.01') : 0.14285714285714285\n",
      "\t Synset('cat.n.03') : 0.14285714285714285\n",
      "\t Synset('kat.n.01') : 0.1111111111111111\n",
      "\t Synset('cat-o'-nine-tails.n.01') : 0.07692307692307693\n",
      "\t Synset('caterpillar.n.02') : 0.07142857142857142\n",
      "\t Synset('big_cat.n.01') : 0.07692307692307693\n",
      "\t Synset('computerized_tomography.n.01') : 0.058823529411764705\n",
      "Synset('frank.n.02') :\n",
      "\t Synset('cat.n.01') : 0.05263157894736842\n",
      "\t Synset('guy.n.01') : 0.08333333333333333\n",
      "\t Synset('cat.n.03') : 0.08333333333333333\n",
      "\t Synset('kat.n.01') : 0.09090909090909091\n",
      "\t Synset('cat-o'-nine-tails.n.01') : 0.06666666666666667\n",
      "\t Synset('caterpillar.n.02') : 0.0625\n",
      "\t Synset('big_cat.n.01') : 0.05263157894736842\n",
      "\t Synset('computerized_tomography.n.01') : 0.05555555555555555\n",
      "Synset('pawl.n.01') :\n",
      "\t Synset('cat.n.01') : 0.058823529411764705\n",
      "\t Synset('guy.n.01') : 0.07692307692307693\n",
      "\t Synset('cat.n.03') : 0.07692307692307693\n",
      "\t Synset('kat.n.01') : 0.07142857142857142\n",
      "\t Synset('cat-o'-nine-tails.n.01') : 0.14285714285714285\n",
      "\t Synset('caterpillar.n.02') : 0.1\n",
      "\t Synset('big_cat.n.01') : 0.058823529411764705\n",
      "\t Synset('computerized_tomography.n.01') : 0.05\n",
      "Synset('andiron.n.01') :\n",
      "\t Synset('cat.n.01') : 0.0625\n",
      "\t Synset('guy.n.01') : 0.08333333333333333\n",
      "\t Synset('cat.n.03') : 0.08333333333333333\n",
      "\t Synset('kat.n.01') : 0.07692307692307693\n",
      "\t Synset('cat-o'-nine-tails.n.01') : 0.16666666666666666\n",
      "\t Synset('caterpillar.n.02') : 0.1111111111111111\n",
      "\t Synset('big_cat.n.01') : 0.0625\n",
      "\t Synset('computerized_tomography.n.01') : 0.05263157894736842\n"
     ]
    }
   ],
   "source": [
    "word1='dog'\n",
    "word2='cat'\n",
    "for s1 in wn.synsets(word1, 'n'):\n",
    "    print(s1,':')\n",
    "    for s2 in wn.synsets(word2, 'n'):\n",
    "        print('\\t', s2,':', s1.path_similarity(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the highest similarity from all pairs to find the strongest association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar are Synset('dog.n.01') Synset('cat.n.01') : 0.2\n",
      "Most similar are Synset('frump.n.01') Synset('guy.n.01') : 0.125\n",
      "Most similar are Synset('dog.n.03') Synset('guy.n.01') : 0.2\n",
      "Most similar are Synset('cad.n.01') Synset('guy.n.01') : 0.14285714285714285\n",
      "Most similar are Synset('frank.n.02') Synset('kat.n.01') : 0.09090909090909091\n",
      "Most similar are Synset('pawl.n.01') Synset('cat-o'-nine-tails.n.01') : 0.14285714285714285\n",
      "Most similar are Synset('andiron.n.01') Synset('cat-o'-nine-tails.n.01') : 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "word1='dog'\n",
    "word2='cat'\n",
    "for s1 in wn.synsets(word1, 'n'):\n",
    "    top_sim_score = 0    \n",
    "    top_sim_synset_word1 = \"\"\n",
    "    top_sim_synset_word2 = \"\"\n",
    "    for s2 in wn.synsets(word2, 'n'):\n",
    "        sim = s1.path_similarity(s2)\n",
    "        if sim>top_sim_score:\n",
    "            top_sim_score = sim\n",
    "            top_sim_synset_word2 = s2\n",
    "    print('Most similar are', s1, top_sim_synset_word2,':', top_sim_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to use this code more often, it is convenient to define a function for it and always call this function instead of re-typing this code. Here is the function to measure shortest distance through the path function for any pair of words. This function assumes you imported NLTK wordnet as wn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def word_similarity_wordnet_path(word1, word2):\n",
    "    top_sim_score = 0    \n",
    "    top_sim_synset_word1 = \"\"\n",
    "    top_sim_synset_word2 = \"\"\n",
    "    for s1 in wn.synsets(word1, 'n'):\n",
    "        for s2 in wn.synsets(word2, 'n'):\n",
    "            sim = s1.path_similarity(s2)\n",
    "            if sim>top_sim_score:\n",
    "                top_sim_score = sim\n",
    "                top_sim_synset_word1 = s1\n",
    "                top_sim_synset_word2 = s2\n",
    "    return top_sim_synset_word1, top_sim_synset_word2, top_sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n",
      "Synset('keyboard.n.01') device consisting of a set of keys on a piano or organ or typewriter or typesetting machine or computer or the like\n",
      "Similarity 0.25\n"
     ]
    }
   ],
   "source": [
    "s1, s2, sim = word_similarity_wordnet_path(\"mouse\", \"keyboard\")\n",
    "print(s1, s1.definition())\n",
    "print(s2, s2.definition())\n",
    "print(\"Similarity\", sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet uses explicit representations of the meanings of words. It is debatable what these meanings are even when they were created by trained experts. In the next notebook, we look at word embeddings which do not distinguish between meanings at all. They plot words directly into a semantic space instead of words mapped to concepts first through their meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnets in other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are wordnets in many different languages and many are linked to English. The ones that are freely available in the Open Multilingual Wordnet platform are also available in NLTK. You can use \"wn.langs\" to get the full list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get an error regarding NLTK not finding a 'omw' dataset. You can download it just like you did in lab1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /Users/piek/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out which languages have wordnets in the OMW package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['als', 'arb', 'bul', 'cat', 'cmn', 'dan', 'ell', 'eng', 'eus', 'fas', 'fin', 'fra', 'glg', 'heb', 'hrv', 'ind', 'ita', 'jpn', 'nld', 'nno', 'nob', 'pol', 'por', 'qcn', 'slv', 'spa', 'swe', 'tha', 'zsm']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(wn.langs()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listed language wordnets are created by translating the English synsets, following the so-called `Expand Method` (Vossen (ed.) 1998). This means that the concepts of the English wordnet are re-used and only the synonyms in the synsets are translated. Another approach is the merge method in which a wordnet is built independently from English and mapped to English afterwards. Only few wordnets are built following a merge approach and, often, they are not freely available as they started from existing dictionaries. The main reason for this is that building a wordnet from scratch is very expensive and labour-intensive.\n",
    "\n",
    "    Vossen, Piek. \"Introduction to eurowordnet.\" In EuroWordNet: A multilingual database with lexical semantic networks, pp. 1-17. Springer, Dordrecht, 1998.\n",
    "\n",
    "In NLTK, you only find wordnets built following the `Expand Method`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the English wordnet, you can  ask for any language lemmas linked to any English-based synset.\n",
    "\n",
    "Are there any Japanese lemmas linked to English dog sense 1? \n",
    "\n",
    "For this we need to use the function **lemma_names** on the synset and pass in the  3-letter language tag as a parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are there any Japanese lemmas linked to English dog sense 1\n",
    "wn.synset('dog.n.01').lemma_names('jpn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hond', 'joekel']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same for Dutch\n",
    "wn.synset('dog.n.01').lemma_names('nld')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is great but can we also get the synset directly through a Dutch or Japanese synonym?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog.n.01.hond')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately not. You cannot directly get the synsets in Wordnet through the same interface we have used before for 'dog'. The next call therefore also does not work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of synsets with \"hond\" as a synonym: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "all_dog_synsets = wn.synsets('hond')\n",
    "print('Number of synsets with \"hond\" as a synonym:', len(all_dog_synsets))\n",
    "print(all_dog_synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the synsets for a non-English word, we first have to use the wn.lemmas() function to get the list of lemma objects for a specific language. The next cell shows this for the Dutch lemma *hond*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('dog.n.01.hond'), Lemma('asshole.n.01.hond')]\n"
     ]
    }
   ],
   "source": [
    "dutch_dog_lemmas = wn.lemmas('hond', lang='nld')\n",
    "print(dutch_dog_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.wordnet.Lemma"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dutch_dog_lemmas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lemma* is a specific class defined in the wordnet module with attributes and functions that is slightly different from the *Synsets* we used before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the *Lemma* functions are different from the `Synset` functions such as lang(). The function *.synset()* can be used to get the synsets associated with a lemma. Obviously, the synset information is the same as for the English wordnet because the Open Dutch Wordnet: http://wordpress.let.vupr.nl/odwn/ was created by expanding the English wordnet:\n",
    "\n",
    "    Postma, Marten, Emiel Van Miltenburg, Roxane Segers, Anneleen Schoen, and Piek Vossen. \"Open dutch wordnet.\" In Proceedings of the 8th Global WordNet Conference (GWC), pp. 302-310. 2016\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('dog.n.01.hond') nld\n",
      "Synsets: Synset('dog.n.01')\n",
      "Synonym: hond\n",
      "Hypernyms: [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "Definition: a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n"
     ]
    }
   ],
   "source": [
    "dutch_lemma = dutch_dog_lemmas[0]\n",
    "print(dutch_lemma, dutch_lemma.lang())\n",
    "\n",
    "dutch_dog_synset = dutch_lemma.synset()\n",
    "print('Synsets:', dutch_dog_synset)\n",
    "print('Synonym:', dutch_lemma._name)\n",
    "print('Hypernyms:', dutch_dog_synset.hypernyms())\n",
    "print('Definition:', dutch_dog_synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have many wordnets in different languages. Can we get statistics on their coverage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 117798\n",
      "Dutch: 36896\n",
      "Italian: 31477\n",
      "Japanese: 64797\n",
      "Slovene: 31631\n",
      "Spanish: 28647\n"
     ]
    }
   ],
   "source": [
    "print('English:', len(list(wn.all_lemma_names(pos='n', lang='eng'))))\n",
    "\n",
    "print('Dutch:', len(list(wn.all_lemma_names(pos='n', lang='nld'))))\n",
    "print('Italian:', len(list(wn.all_lemma_names(pos='n', lang='ita'))))\n",
    "print('Japanese:', len(list(wn.all_lemma_names(pos='n', lang='jpn'))))\n",
    "print('Slovene:', len(list(wn.all_lemma_names(pos='n', lang='slv'))))\n",
    "print('Spanish:', len(list(wn.all_lemma_names(pos='n', lang='spa'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that English has a lot more synonyms than the other wordnets. So there is still work to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dogs: 18\n",
      "\n",
      "Synset('basenji.n.01')\n",
      "small smooth-haired breed of African origin having a tightly curled tail and the inability to bark\n",
      "English: ['basenji']\n",
      "Dutch: []\n",
      "Japanese: []\n",
      "Italian: ['basenji', 'cane_del_Congo']\n",
      "Spanish: ['basenji']\n",
      "\n",
      "Synset('corgi.n.01')\n",
      "either of two Welsh breeds of long-bodied short-legged dogs with erect ears and a fox-like head\n",
      "English: ['corgi', 'Welsh_corgi']\n",
      "Dutch: []\n",
      "Japanese: ['']\n",
      "Italian: []\n",
      "Spanish: []\n",
      "\n",
      "Synset('cur.n.01')\n",
      "an inferior dog or one of mixed breed\n",
      "English: ['cur', 'mongrel', 'mutt']\n",
      "Dutch: ['mormel', 'idioot', 'halve_gare', 'bastaard', 'bastaardhond', 'straathond']\n",
      "Japanese: ['', '', '']\n",
      "Italian: ['bastardo']\n",
      "Spanish: ['chucho', 'gozque', 'mestizo']\n",
      "\n",
      "Synset('dalmatian.n.02')\n",
      "a large breed having a smooth white coat with black or brown spots; originated in Dalmatia\n",
      "English: ['dalmatian', 'coach_dog', 'carriage_dog']\n",
      "Dutch: ['dalmatir', 'Dalmatische']\n",
      "Japanese: []\n",
      "Italian: ['dalmata']\n",
      "Spanish: []\n",
      "\n",
      "Synset('great_pyrenees.n.01')\n",
      "bred of large heavy-coated white dogs resembling the Newfoundland\n",
      "English: ['Great_Pyrenees']\n",
      "Dutch: []\n",
      "Japanese: []\n",
      "Italian: ['mastino_dei_Pirenei']\n",
      "Spanish: []\n",
      "\n",
      "Synset('griffon.n.02')\n",
      "breed of various very small compact wiry-coated dogs of Belgian origin having a short bearded muzzle\n",
      "English: ['griffon', 'Brussels_griffon', 'Belgian_griffon']\n",
      "Dutch: []\n",
      "Japanese: ['', '', '']\n",
      "Italian: []\n",
      "Spanish: ['grifn']\n",
      "\n",
      "Synset('hunting_dog.n.01')\n",
      "a dog used in hunting game\n",
      "English: ['hunting_dog']\n",
      "Dutch: []\n",
      "Japanese: ['']\n",
      "Italian: ['cane_da_caccia']\n",
      "Spanish: []\n",
      "\n",
      "Synset('lapdog.n.01')\n",
      "a dog small and tame enough to be held in the lap\n",
      "English: ['lapdog']\n",
      "Dutch: []\n",
      "Japanese: []\n",
      "Italian: ['cagnolino', 'cagnolino_da_salotto']\n",
      "Spanish: []\n",
      "\n",
      "Synset('leonberg.n.01')\n",
      "a large dog (usually with a golden coat) produced by crossing a St Bernard and a Newfoundland\n",
      "English: ['Leonberg']\n",
      "Dutch: []\n",
      "Japanese: []\n",
      "Italian: []\n",
      "Spanish: []\n",
      "\n",
      "Synset('mexican_hairless.n.01')\n",
      "any of an old breed of small nearly hairless dogs of Mexico\n",
      "English: ['Mexican_hairless']\n",
      "Dutch: []\n",
      "Japanese: []\n",
      "Italian: []\n",
      "Spanish: []\n",
      "\n",
      "Synset('newfoundland.n.01')\n",
      "a breed of very large heavy dogs with a thick coarse usually black coat; highly intelligent dogs and vigorous swimmers; developed in Newfoundland\n",
      "English: ['Newfoundland', 'Newfoundland_dog']\n",
      "Dutch: []\n",
      "Japanese: []\n",
      "Italian: ['terranova', 'Terranova']\n",
      "Spanish: []\n",
      "\n",
      "Synset('pooch.n.01')\n",
      "informal terms for dogs\n",
      "English: ['pooch', 'doggie', 'doggy', 'barker', 'bow-wow']\n",
      "Dutch: ['bastaard', 'vuilnisbakkie']\n",
      "Japanese: ['', '', '']\n",
      "Italian: []\n",
      "Spanish: []\n",
      "\n",
      "Synset('poodle.n.01')\n",
      "an intelligent dog with a heavy curly solid-colored coat that is usually clipped; an old breed sometimes trained as sporting dogs or as performing dogs\n",
      "English: ['poodle', 'poodle_dog']\n",
      "Dutch: ['poedel']\n",
      "Japanese: ['']\n",
      "Italian: ['barbone']\n",
      "Spanish: []\n",
      "\n",
      "Synset('pug.n.01')\n",
      "small compact smooth-coated breed of Asiatic origin having a tightly curled tail and broad flat wrinkled muzzle\n",
      "English: ['pug', 'pug-dog']\n",
      "Dutch: ['mops', 'mopshond']\n",
      "Japanese: ['']\n",
      "Italian: []\n",
      "Spanish: ['pug']\n",
      "\n",
      "Synset('puppy.n.01')\n",
      "a young dog\n",
      "English: ['puppy']\n",
      "Dutch: ['hondejong', 'hondenjong', 'pup', 'puppy']\n",
      "Japanese: ['', '', '', '', '', '', '']\n",
      "Italian: ['cucciolo']\n",
      "Spanish: []\n",
      "\n",
      "Synset('spitz.n.01')\n",
      "any of various stocky heavy-coated breeds of dogs native to northern regions having pointed muzzles and erect ears with a curled furry tail\n",
      "English: ['spitz']\n",
      "Dutch: []\n",
      "Japanese: []\n",
      "Italian: []\n",
      "Spanish: ['spitz']\n",
      "\n",
      "Synset('toy_dog.n.01')\n",
      "any of several breeds of very small dogs kept purely as pets\n",
      "English: ['toy_dog', 'toy']\n",
      "Dutch: []\n",
      "Japanese: ['']\n",
      "Italian: []\n",
      "Spanish: []\n",
      "\n",
      "Synset('working_dog.n.01')\n",
      "any of several breeds of usually large powerful dogs bred to work as draft animals and guard and guide dogs\n",
      "English: ['working_dog']\n",
      "Dutch: ['werkhond']\n",
      "Japanese: ['']\n",
      "Italian: ['cane_da_lavoro']\n",
      "Spanish: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dog = wn.synset ('dog.n.01')\n",
    "dogs = dog.hyponyms()\n",
    "print('Number of dogs:', len(dogs))\n",
    "print()\n",
    "for s in dogs:\n",
    "    print(s)\n",
    "    print(s.definition())\n",
    "    print('English:', s.lemma_names('eng'))\n",
    "    print('Dutch:', s.lemma_names('nld'))\n",
    "    print('Japanese:', s.lemma_names('jpn'))\n",
    "    print('Italian:', s.lemma_names('ita'))\n",
    "    print('Spanish:', s.lemma_names('spa'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hltenv",
   "language": "python",
   "name": "hltenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
