{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 Use generative Language Models as text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of you have used ChatGPT from OpenAI and noticed that it can respond to your input in a very natural way. ChatGPT is build on top of GPT (Generative Pretrained Transformer) which is a model trained to generate text given a preceding input, so-called prompt (Brown et al 2020). It can do this repetitively up to a certain length, likewise generating short stories.\n",
    "\n",
    "Another generative model is T5 (Text to Text Transfer Transformer, Raffel et al. 2019). T5 models many tasks as a text generation task, ranging from plain translation, sentiment annotation, question-answering, similarity, to summarisation. Tasks are differentiated through prompt prefixes.\n",
    "\n",
    "<img src=\"T5.gif\">\n",
    "\n",
    "Models such as GPT3,4 and T5, although having good performance, are by far too large model to work with locally. Therefore in this notebook, we will use Llama3 which is publicly available: https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "### References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n",
    "\n",
    "OpenAI, 2023. GPT-4 Technical Report. arXiv:2303.08774\n",
    "\n",
    "Llama: [The Llama 3 Herd of Models](https://scontent-ams2-1.xx.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=t6egZJ8QdI4Q7kNvgEka7Z4&_nc_ht=scontent-ams2-1.xx&oh=00_AYBPnjp-CQn7YnUQU_P-yJATmlaN6oRuEHZ1VrXshBoBwQ&oe=66A6EB8D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama3 client-server calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use Llama3 in the same way as we have seen for BERT and XLM-RoBERTa using a pipeline. This is however a bit more difficult because you need to obtain an access key from Meta and login through a Huggingface account. \n",
    "\n",
    "We are therefore going to make use of the server version of Llama as we did for our chat conversation at the beginning of the course. For this, we need to use the OpenAI client package again. If you have not yet installed it, you can i nstall the OpenAI client using the following command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the openAI package we can import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a client that connects to the server version, either locally or to the CLTL server that was given to you during the course. In the next cell, we show how to access a local server that runs on port 9001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://localhost:9001/v1\", api_key=\"not-needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The openAI client has various functions which are explained here: https://pypi.org/project/openai/\n",
    "\n",
    "We will use the ```chat``` module with the function ```completions.create```. This function needs several arguments:\n",
    "\n",
    "* model: this can be a local model or the name of one of the openAI models\n",
    "* messages: this is the input prompt\n",
    "* temperature: a value between 0 and 1 that defines how creative the model should be, 0 means best answer only and 1 a less liklely alternative maximally creative.\n",
    "* stream: this must be set to True so that we can get the complete answer from the server\n",
    "\n",
    "You can experiment with the temperature and the prompt to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt is given as a list of **dict** strings defining the role of the **system** (the model) and the input given by the **user**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Bach sat down at his organ and played\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "                model=\"local-model\", # this field is currently unused\n",
    "                messages=prompt,\n",
    "                temperature=0.3,\n",
    "                stream=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 alternative sentences:\n",
      "\n",
      "1. Bach sat down at his harpsichord and composed.\n",
      "2. Bach stood up at the piano and improvised.\n",
      "3. Bach lay down on the cello and harmonized.\n",
      "4. Bach stepped out to the street corner and jammed.\n",
      "5. Bach reached for the violin and swelled.\n",
      "6. Bach leaned back in his armchair and accompanied.\n",
      "7. Bach walked into the concert hall and performed.\n",
      "8. Bach sat up at the forte piano and arpeggiated.\n",
      "9. Bach stood tall at the bassoon and oscillated.\n",
      "10. Bach lay low on the contrabass and resonated.\n",
      "\n",
      "Let me know if you'd like me to generate more!\n"
     ]
    }
   ],
   "source": [
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        response += chunk.choices[0].delta.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to consider about these sentences. Are they all grammatical? Is the input modified and still semantically valid? Are the completions semantically correct? Do the completions make sense given that it is Bach playing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 alternative sentences:\n",
      "\n",
      "1. Beethoven's fingers flew across the piano keys as he composed.\n",
      "2. The composer's music swelled, filling every corner of the room.\n",
      "3. With a surge of creative energy, Beethoven began to improvise.\n",
      "4. He expertly manipulated the strings to produce a beautiful melody.\n",
      "5. As the music swirled around him, Beethoven felt his emotions soar.\n",
      "6. In a burst of artistic expression, he poured his heart onto the page.\n",
      "7. The notes danced across the sheet music like tiny ballerinas.\n",
      "8. With each deliberate movement, Beethoven crafted a masterpiece.\n",
      "9. His symphony rose to a crescendo, filling every corner of the room.\n",
      "10. As the final chord faded away, Beethoven smiled with pride and satisfaction.\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven sat down at his organ and played\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "                model=\"local-model\", # this field is currently unused\n",
    "                messages=prompt,\n",
    "                temperature=0.3,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        response += chunk.choices[0].delta.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do this all day, let us switch the language of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 alternative sentences:\n",
      "\n",
      "1. Beethoven ging achter het orgel zitten en speelde.\n",
      "2. Beethoven stond op en begon te spelen op het orgel.\n",
      "3. Hij ging zitten bij het orgel en begon te spelen.\n",
      "4. Beethoven ging voor het orgel staan en begon te zingen.\n",
      "5. Hij ging zitten bij het orgel en begon te schrijven.\n",
      "6. Beethoven ging voor het orgel staan en begon te improviseren.\n",
      "7. Hij ging op het orgel zitten en begon te dansen.\n",
      "8. Beethoven ging voor het orgel staan en begon te componeren.\n",
      "9. Hij ging bij het orgel zitten en begon te vertellen.\n",
      "10. Beethoven ging voor het orgel staan en begon te herinneren.\n",
      "\n",
      "Note: The input text is in Dutch, and it translates to \"Beethoven sat behind his organ and played\".\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven ging achter zijn orgel zitten en speelde\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "                model=\"local-model\", # this field is currently unused\n",
    "                messages=prompt,\n",
    "                temperature=0.3,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        response += chunk.choices[0].delta.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions in English, completions as Dutch. You can also give the instructions in Dutch if you want or try other languages. Which languages are covered by Llama and how well? Interesing result is that Llama varied the sitting and in a few cases the organ (orgel) but never specified what Beethoven is playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 alternative sentences:\n",
      "\n",
      "1. Beethoven ploegde zich neer bij het orgel en begon te spelen.\n",
      "2. Hij nestelde zich in bij het orgel en klopte op een klassieke melodie.\n",
      "3. Beethoven ging zitten voor het orgel en begon zijn muziek te maken.\n",
      "4. Hij liet zich zakken bij het orgel en begon de toetsen te drukken.\n",
      "5. Beethoven kwam aanzitten bij het orgel en begon zijn composities te creëren.\n",
      "6. Hij ging zitten voor het orgel en begon zijn muziekstukjes te spelen.\n",
      "7. Beethoven ging op het orgel zitten en begon zijn klassieke stukken te maken.\n",
      "8. Hij nestelde zich in bij het orgel en begon de toetsen te laten klinken.\n",
      "9. Beethoven kwam aanzitten bij het orgel en begon zijn muziekstukjes te creëren.\n",
      "10. Hij ging zitten voor het orgel en begon zijn klassieke stukken te spelen.\n",
      "\n",
      "Let me know if you'd like me to generate more!\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Genereer 10 alternatieve zinnen door de volgende text af te maken.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven ging achter zijn orgel zitten en speelde\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "                model=\"local-model\", # this field is currently unused\n",
    "                messages=prompt,\n",
    "                temperature=0.3,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "response = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        response += chunk.choices[0].delta.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Dutch instructions do make a big difference as the quality went down drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use the same client to prompt openAI models but for that you need to obtain an API key from openAI and pass it in to the model. You can pass this key in the api_key parameter when creating a client and next make the same calls as we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "                \n",
    "# completion = client.chat.completions.create(\n",
    "#                 model=\"gpt-4o\",\n",
    "#                 messages=prompt,\n",
    "#                 temperature=0.3,\n",
    "#                 stream=True\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative LLMs sucha s ChatGPT, LLama, Mixtral have been finetuned for various tasks through alignment and possibly reinforcement learning on top of this. These are specific behaviours such as **chat**, **summarize**, **paraphrase**, **translate** or answer questions **Q&A**. You can evoke these functions through the instruction given, as we gave the instruction to complete a text. However, the models could also give useful responses to other instructions for which they were not specifically trained, e.g. to generate Python code, do calculations or a website. These emerging capabilities are fascinating but also risky since there is little control and/or training for this behaviour and therefore no guarantee that it provides the correct answers. So in case of code generation, it may cost more time to check and understand the generated code than to code yourself which also makes you a better coder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
