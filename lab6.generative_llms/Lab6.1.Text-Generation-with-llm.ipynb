{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 Use generative Language Models as text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of you have used ChatGPT from OpenAI and noticed that it can respond to your input in a very natural way. ChatGPT is build on top of GPT (Generative Pretrained Transformer) which is a model trained to generate text given a preceding input, so-called prompt (Brown et al 2020). It can do this repetitively up to a certain length, likewise generating short stories.\n",
    "\n",
    "Models such as ChatGPT, although having good performance, are by far too large model to work with locally. Therefore in this notebook, we will use smaller open source models such as Llama and Qwen which are publicly available on huggingface.co.\n",
    "\n",
    "### References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n",
    "\n",
    "OpenAI, 2023. GPT-4 Technical Report. arXiv:2303.08774\n",
    "\n",
    "Llama: Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière et al. \"Llama: Open and efficient foundation language models.\" arXiv preprint arXiv:2302.13971 (2023). [The Llama 3 Herd of Models](https://scontent-ams2-1.xx.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=t6egZJ8QdI4Q7kNvgEka7Z4&_nc_ht=scontent-ams2-1.xx&oh=00_AYBPnjp-CQn7YnUQU_P-yJATmlaN6oRuEHZ1VrXshBoBwQ&oe=66A6EB8D)\n",
    "\n",
    "Qwen: Bai, Jinze, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan et al. \"Qwen technical report.\" arXiv preprint arXiv:2309.16609 (2023).\n",
    "\n",
    "[How to work with Qwen](https://www.codecademy.com/article/qwen-3-ollama-setup-and-fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative LLMs are trained to respond to prompts. The prompt is given as a list of **dict** strings defining the role of the **system** (the model) and the input given by the **user**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Bach sat down at his organ and played\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```transformers``` module can also be used to create a pipeline to generate a text as a response to prompt:\n",
    "\n",
    "```# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-1.7B-Base\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "response = pipe(messages)\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However it takes a lot of memory and time to download models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the pipeline modules, we will use another package ```ChatOllama``` from **langchain** that was also used for your chat before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model=\"qwen3:1.7b\"\n",
    "\n",
    "chat = ChatOllama(\n",
    "            model=model,\n",
    "            temperature= 0.1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking aloud <think>\n",
      "Okay, the user wants me to generate 10 alternative sentences by completing the sentence \"Bach sat down at his organ and played...\" Let me start by understanding the original sentence. It's about Johann Sebastian Bach playing his organ. The key elements here are Bach, sitting down, the organ, and playing.\n",
      "\n",
      "First, I need to think of different ways to describe what he was doing. Maybe vary the verbs or the details. For example, instead of \"played,\" could use \"sang,\" \"composed,\" \"rehearsed,\" etc. Also, the context might change slightly. Like, maybe he was playing a specific piece, or he was in a particular mood.\n",
      "\n",
      "I should make sure each sentence is unique and uses different verbs or phrases. Let me brainstorm some possibilities:\n",
      "\n",
      "1. Bach sat down at his organ and played a sonata.\n",
      "2. Bach sat down at his organ and composed a new piece.\n",
      "3. Bach sat down at his organ and performed a concert.\n",
      "4. Bach sat down at his organ and sang a hymn.\n",
      "5. Bach sat down at his organ and played a fugue.\n",
      "6. Bach sat down at his organ and rehearsed a new arrangement.\n",
      "7. Bach sat down at his organ and played a piece for the first time.\n",
      "8. Bach sat down at his organ and played a piece that had been waiting for him.\n",
      "9. Bach sat down at his organ and played a piece that was part of a larger composition.\n",
      "10. Bach sat down at his organ and played a piece that was inspired by a particular moment in his life.\n",
      "\n",
      "Wait, I need to check if these are all different. Each sentence should have a unique verb and maybe different aspects. Also, ensure that the sentences are grammatically correct and make sense. Let me verify each one. \n",
      "\n",
      "For example, sentence 1 uses \"played a sonata,\" which is correct. Sentence 2 uses \"composed,\" which is good. Sentence 3 \"performed a concert\" is okay. Sentence 4 \"sang a hymn\" is correct. Sentence 5 \"played a fugue\" is accurate. Sentence 6 \"rehearsed a new arrangement\" is good. Sentence 7 \"played a piece that had been waiting for him\" is a bit more creative. Sentence 8 \"played a piece that was part of a larger composition\" is also good. Sentence 9 \"played a piece that was inspired by a particular moment\" is unique. \n",
      "\n",
      "I think these are all valid. Now, I need to present them in a list of 10, making sure each is distinct and uses different elements. Maybe check for any repetition in verbs or phrases. For example, \"played\" is used in all sentences, but the context varies. That's okay. Each sentence has a different aspect of Bach's activity. \n",
      "\n",
      "I think that's covered. Now, format the answer as 10 sentences in a list, each with a different completion.\n",
      "</think>\n",
      "\n",
      "The final answer 1. Bach sat down at his organ and played a sonata.  2. Bach sat down at his organ and composed a new piece.  3. Bach sat down at his organ and performed a concert.  4. Bach sat down at his organ and sang a hymn.  5. Bach sat down at his organ and played a fugue.  6. Bach sat down at his organ and rehearsed a new arrangement.  7. Bach sat down at his organ and played a piece that had been waiting for him.  8. Bach sat down at his organ and played a piece that was part of a larger composition.  9. Bach sat down at his organ and played a piece inspired by a particular moment in his life.  10. Bach sat down at his organ and played a piece that was dedicated to a special friend.\n"
     ]
    }
   ],
   "source": [
    "print(\"Thinking aloud\", think)\n",
    "print()\n",
    "print(\"The final answer\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define the client, we can not only adjust the prompt but also the temperature. Temperatures closer to 1.0 make the model become more creative, closer to 1.0 it will follow the instructions more in a strict way. So let's try again with a high temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_creative = ChatOllama(\n",
    "            model=model,\n",
    "            temperature= 0.9,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Bach played a piece that was particularly moving.  2. Bach played a piece that was in the style of a particular era.  3. Bach played a piece that was a testament to his genius.  4. Bach played a piece that was a masterpiece of Baroque music.  5. Bach played a piece that was a celebration of the human spirit.  6. Bach played a piece that was technically challenging.  7. Bach played a piece that was a tribute to a particular composer.  8. Bach played a piece that was in a specific key.  9. Bach played a piece that was performed in front of an audience.  10. Bach played a piece that was a cornerstone of Western classical music.\n"
     ]
    }
   ],
   "source": [
    "response = chat_creative.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see that it got more expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set the temperature back to 0.1 and try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Beethoven sat down at his organ and composed a symphony.  2. Beethoven sat down at his organ and played a sonata.  3. Beethoven sat down at his organ and performed a piece.  4. Beethoven sat down at his organ and played a nocturne.  5. Beethoven sat down at his organ and played a fugue.  6. Beethoven sat down at his organ and played a prelude.  7. Beethoven sat down at his organ and played a piece that would later become a classic.  8. Beethoven sat down at his organ and played a piece that marked a new era in classical music.  9. Beethoven sat down at his organ and played a composition that inspired a new generation.  10. Beethoven sat down at his organ and played a melody that would forever change the landscape of classical music.\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven sat down at his organ and played\"}]\n",
    "\n",
    "\n",
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us switch the language of the input, as Qwen is a multilingual model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Beethoven ging achter zijn orgel zitten en speelde een lied dat hij had opgesteld.  2. Beethoven ging achter zijn orgel zitten en speelde een symfonie dat hij had opgesteld.  3. Beethoven ging achter zijn orgel zitten en speelde een konzert dat hij had opgesteld.  4. Beethoven ging achter zijn orgel zitten en speelde een motief dat hij had opgesteld.  5. Beethoven ging achter zijn orgel zitten en speelde een klok dat hij had opgesteld.  6. Beethoven ging achter zijn orgel zitten en speelde een opname dat hij had opgesteld.  7. Beethoven ging achter zijn orgel zitten en speelde een kantoor dat hij had opgesteld.  8. Beethoven ging achter zijn orgel zitten en speelde een kantoor dat hij had opgesteld.  9. Beethoven ging achter zijn orgel zitten en speelde een kantoor dat hij had opgesteld.  10. Beethoven ging achter zijn orgel zitten en speelde een kantoor dat hij had opgesteld.\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text in Dutch.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven ging achter zijn orgel zitten en speelde\"}]\n",
    "\n",
    "\n",
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions in English, completions as Dutch. You can also give the instructions in Dutch if you want or try other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Beethoven ging achter zijn orgel zitten en speelde een symfonie.  2. Beethoven ging achter zijn orgel zitten en speelde een lied op een zomeravond.  3. Beethoven ging achter zijn orgel zitten en speelde een nocturne met een zachte muziek.  4. Beethoven ging achter zijn orgel zitten en speelde een konzert op een zonnestijl.  5. Beethoven ging achter zijn orgel zitten en speelde een klokgedeelte met een zachte toon.  6. Beethoven ging achter zijn orgel zitten en speelde een opus dat zijn zonen hoorden.  7. Beethoven ging achter zijn orgel zitten en speelde een lied dat zijn zonen hoorden.  8. Beethoven ging achter zijn orgel zitten en speelde een symfonie die zijn zonen hoorden.  9. Beethoven ging achter zijn orgel zitten en speelde een lied dat zijn zonen hoorden.  10. Beethoven ging achter zijn orgel zitten en speelde een symfonie die zijn zonen hoorden.\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Genereer 10 alternatieve zinnen door de volgende tekst af te maken.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven ging achter zijn orgel zitten en speelde\"}]\n",
    "\n",
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Dutch instructions do make a big difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hltenv",
   "language": "python",
   "name": "hltenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
