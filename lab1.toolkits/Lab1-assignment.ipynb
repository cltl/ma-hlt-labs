{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1-Assignment\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the assignment for Lab 1. \n",
    "\n",
    "**Points**: each exercise is prefixed with the number of points you can obtain for the exercise. However, these points are just an indication of what parts we value more. The assignment itself is assessed as PASS/NO-PASS. In general, we value a critical analysis of running code more than just showing tat you can create or run the code. So if you succesfully carried out the instructed commands in a notebook you are not done yet. We want you to analyse, understand what the code is doing with language and text. Be critical, think about how to explain what you observe and write this down in the notebook after running the code. It will be clear in the assignment when we expect this from you.\n",
    "\n",
    "You can make the assignment as a group but make sure that you understand and can carry out the coding yourself. You need these skills for your final assignment that is graded. Feedback will be given at the group level.\n",
    "\n",
    "We assume you have worked through the following notebooks:\n",
    "* **Lab1.1-introduction**\n",
    "* **Lab1.2-introduction-to-NLTK**\n",
    "* **Lab1.3-introduction-to-spaCy** \n",
    "\n",
    "In this assignment, you will process an English text (**Lab1-apple-samsung-example.txt**) with both NLTK and spaCy and discuss the similarities and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who to contact for questions\n",
    "* Piek Vossen (piek.vossen@vu.nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip: how to read a file from disk\n",
    "Let's open the file **Lab1-apple-samsung-example.txt** from disk. It should be located in the same folder as this notebook. The most simple way is to specify the full path to the file, e.g.:\n",
    "\n",
    "```\n",
    "path_to_file='/Users/piek/Desktop/t-ONDERWIJS/2021-2022/t-MA-HLT-introduction-2021/ma-hlt-labs/lab1.toolkitst'\n",
    "```\n",
    "\n",
    "This may work for me but not for you as it is unlikely that the file has the same path on your machine.\n",
    "\n",
    "We can use the Path module to find the directory of this notebook. Once we have that, we only need to concatenate the name of the text file to this path. This is how you do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory of this notebook: /Users/piek/Desktop/t-ONDERWIJS/2021-2022/t-MA-HLT-introduction-2021/ma-hlt-labs/lab1.toolkits\n",
      "Path to the text file: /Users/piek/Desktop/t-ONDERWIJS/2021-2022/t-MA-HLT-introduction-2021/ma-hlt-labs/lab1.toolkits/Lab1-apple-samsung-example.txt\n"
     ]
    }
   ],
   "source": [
    "cur_dir = Path().resolve() # this should provide you with the folder in which this notebook is placed\n",
    "print('Current directory of this notebook:', cur_dir)\n",
    "\n",
    "## We can now stick the name of the file to the end of the Path using the *joinpath* function:\n",
    "path_to_file = Path.joinpath(cur_dir, 'Lab1-apple-samsung-example.txt')\n",
    "print('Path to the text file:', path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unsure whether the path is correct, you can check if the file exist on that location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does path exist? -> True\n"
     ]
    }
   ],
   "source": [
    "print('does path exist? ->', Path.exists(path_to_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output from the code cell above states that **does path exist? -> False**, please check that the file **Lab1-apple-samsung-example.txt** is in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can open the file and access its content. Lets read the complete content and ask for it length using the 'len' function, which will tell us how many characters a string has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters 1139\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as infile:\n",
    "    text = infile.read()\n",
    "\n",
    "print('number of characters', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.telegraph.co.uk/technology/apple/9702716/Apple-Samsung-lawsuit-six-more-products-under-scrutiny.html\n",
      "\n",
      "Documents filed to the San Jose federal court in California on November 23 list six Samsung products running the \"Jelly Bean\" and \"Ice Cream Sandwich\" operating systems, which Apple claims infringe its patents.\n",
      "The six phones and tablets affected are the Galaxy S III, running the new Jelly Bean system, the Galaxy Tab 8.9 Wifi tablet, the Galaxy Tab 2 10.1, Galaxy Rugby Pro and Galaxy S III mini.\n",
      "Apple stated it had “acted quickly and diligently\" in order to \"determine that these newly released products do infringe many of the same claims already asserted by Apple.\"\n",
      "In August, Samsung lost a US patent case to Apple and was ordered to pay its rival $1.05bn (£0.66bn) in damages for copying features of the iPad and iPhone in its Galaxy range of devices. Samsung, which is the world's top mobile phone maker, is appealing the ruling.\n",
      "A similar case in the UK found in Samsung's favour and ordered Apple to publish an apology making clear that the South Korean firm had not copied its iPad when designing its own devices.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now created a string object with the name 'text' that we can use for the assignment below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason, you see weird characters in the text you may have a problem with the character encoding. Computers use different systems to represent scripts. For most languages UTF-8 will work as it has representations for many different characters. In some cases, especially older texts, Latin encodings only have been used which works for English and some other languages but cannot represent characters in others. For non-Western scripts special encodings have been defined. You never know for sure what encoding a text is using but now-adays most data is in UTF-8.\n",
    "\n",
    "## What to do if you see weird tokens?\n",
    "First check if you are really using Python 3.x and not Python 2.x when running the notebook. You can do this using: \n",
    "\n",
    "    import platform\n",
    "    print(platform.python_version())\n",
    "\n",
    "If your are running 3.x and still have encoding problems try to open the file as utf-8:\n",
    "\n",
    "    with open(path_to_file, encoding=‘utf-8') as infile:\n",
    "    \n",
    "Note that when you open a text file in a plain text editor, you never know how it loads the file. The weird characters may still be there or disappear. In some cases, you can try to save the text file again using UTF-8 but this can also corrupt your file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 4] Exercise 1: NLTK\n",
    "In this exercise, we use NLTK to apply **Part-of-speech (POS) tagging**, **Named Entity Recognition (NER)**, and **Constituency parsing**. The following code snippet already performs sentence splitting and tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_nltk = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_sentence = [] # this will become a list of lists!!!\n",
    "\n",
    "#Below you find a so-called for loop\n",
    "for sentence_nltk in sentences_nltk:\n",
    "    sent_tokens = word_tokenize(sentence_nltk)\n",
    "    # We append the tokens of this sentence to the result list\n",
    "    tokens_per_sentence.append(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use lists to keep track of the output of the NLP tasks. We can hence inspect the output for each task using the index of the sentence. Lets look at the first sentence. Since the text starts with the URL to the source, our first real sentence is index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE The six phones and tablets affected are the Galaxy S III, running the new Jelly Bean system, the Galaxy Tab 8.9 Wifi tablet, the Galaxy Tab 2 10.1, Galaxy Rugby Pro and Galaxy S III mini.\n",
      "TOKENS ['The', 'six', 'phones', 'and', 'tablets', 'affected', 'are', 'the', 'Galaxy', 'S', 'III', ',', 'running', 'the', 'new', 'Jelly', 'Bean', 'system', ',', 'the', 'Galaxy', 'Tab', '8.9', 'Wifi', 'tablet', ',', 'the', 'Galaxy', 'Tab', '2', '10.1', ',', 'Galaxy', 'Rugby', 'Pro', 'and', 'Galaxy', 'S', 'III', 'mini', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_id = 1\n",
    "print('SENTENCE', sentences_nltk[sentence_id])\n",
    "print('TOKENS', tokens_per_sentence[sentence_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sentence=1 we get the second sentence from the text. Next you can define any value to sentence_id to carry out the assignment. Also try out sentence_id=0. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the assignment, pick a sentence you think has interesting properties and define the value for sentence_id correspondingly. In this notebook it is set to sentence '2' which is the third sentence. Change it to get your sentence and continue with the assignment in which you will use this value for sentence_id from now on. So do not pick out a short sentence without punctuation or any named entities!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_id=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain here in words why you selected this sentence! Why do you expect it to give interesting results?\n",
    "[your explanation goes here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [point: 1] Exercise 1a: Part-of-speech (POS) tagging\n",
    "Use *nltk.pos_tag* to perform part-of-speech tagging on a single sentence.\n",
    "\n",
    "Use **print** to show the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = tokens_per_sentence[sentence_id]\n",
    "pos_tagged_sentence_tokens= [] \n",
    "#put here the call to nltk pos tagger and to assign the result to the variable 'pos_tagged_sentence_tokens'\n",
    "print(pos_tagged_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [point: 1] Exercise 1b: Named Entity Recognition (NER)\n",
    "Use *nltk.chunk.ne_chunk* to perform Named Entity Recognition (NER) on your selected sentence.\n",
    "\n",
    "Use **print** to show the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "tokens_pos_tagged_and_named_entities = []\n",
    "#put here the call to nltk named entity chunker and to assign the result to the variable 'tokens_pos_tagged_and_named_entities'\n",
    "print(tokens_pos_tagged_and_named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 2] Exercise 1c: Constituency parsing\n",
    "Use the *nltk.RegexpParser* to perform constituency parsing on your selected sentence. Think about what the parsing expects as input.\n",
    "\n",
    "Use **print** to show the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_parser_v1 = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_v1_output_for_sentence = []\n",
    "#add here your code to assign the output of the parser 'constituent_parser_v1' to the variable name 'constituency_v1_output_for_sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(constituency_v1_output_for_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the RegexpParser so that it also detects Named Entity Phrases (NEP), e.g., that it detects phrases such as *Galaxy S III* and *Ice Cream Sandwich* as entity phrases, which we give the label 'NEP'. Below you see an empty structure for NEP and ??? as comments. Fill in the empty structure and write your comment in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_parser_v2 = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*\n",
    "NEP: {}             # ???''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_v2_output_for_sentence = []\n",
    "#add here your code to assign the output of the parser 'constituent_parser_v2' to the variable name 'constituency_v2_output_for_sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(constituency_v2_output_for_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 1] Exercise 2: spaCy\n",
    "Use Spacy to process the same text as you analyzed with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text) # insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: You can use **sents = list(doc.sents)** to be able to use the index to access a sentence like **sents[2]** for the third sentence. Use the previsouly defined \"sentence_id\" to get the output for the same NLTK sentence. Note that we assume that the sentences are split in the same way. So lets get the spaCy sentence for sentence_id and use this to compare against the NLTK output we had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple stated it had “acted quickly and diligently\" in order to \"determine that these newly released products do infringe many of the same claims already asserted by Apple.\n"
     ]
    }
   ],
   "source": [
    "sents=list(doc.sents)\n",
    "spacy_sentence=sents[sentence_id]\n",
    "print(spacy_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 5] Exercise 3: Comparison NLTK and spaCy\n",
    "We will now compare the output of NLTK and spaCy for the same sentence identified with sentence_id and which you just processed with the NLTK, i.e. in what do they differ? So here we expect you to critically think about at the differences in output and not just run the code and describe the differences. What is good and what is bad and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 2] Exercise 3a: Part of speech tagging\n",
    "Compare the output from NLTK and Spacy regarding part of speech tagging for the selected sentence. You already had the PoS Tags from NLTK. Get the tokens and their PoS tags (**token.tag**) from spaCy. Print both and describe any differences. This is not a trick question, it is possible that there are no differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple stated it had “acted quickly and diligently\" in order to \"determine that these newly released products do infringe many of the same claims already asserted by Apple.\n"
     ]
    }
   ],
   "source": [
    "sentence=sents[sentence_id]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 1] Exercise 3b: Named Entity Recognition (NER)\n",
    "* For the same sentence, describe differences between the output from NLTK and spaCy for Named Entity Recognition. Which one do you think performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 1] Exercise 3c: Constituency/dependency parsing\n",
    "For the same sentence from the text, run constituency parsing using NLTK and dependency parsing using spaCy.\n",
    "* describe briefly the difference between constituency parsing and dependency parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_parser = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the sentence here with NLTK and draw the tree. Also use Spacy to show the parsed tree. What is different about these trees? Are there also differences in the syntactic analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the assignment 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
