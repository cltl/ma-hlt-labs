{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4.3 How to use fine-tuned crosslinglual transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer models such as BERT and RoBERTa can easily be fine-tuned for downstream tasks. The Huggingface hub lists many of these models already trained for specific tasks. New fine-tuned transformer models are published regularly on the huggingface platform: https://huggingface.co/models\n",
    "\n",
    "In this notebook, we show two examples of fine-tuned models for xlm-roberta. Because the language model is cross-lingual, also the fine-tuned model works for all the 100 languages that xlm-roberta supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pipelines for transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface transfomers provides an option to create an **pipeline** to perform a specific NLP task with a pretrained model:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "The pipelines are abstractions from specific tasks such as sentiment-analysis and entity recognition. In the case of sentiment-analysis, the complete sentence representation of the model is taken as the input and classified with the defined labels. The sentence information is packaged in the special ```[CLS]``` token that was trained in BERT for next sentence prediction. Alternatively, token embeddings of sentences can be summed and averaged,\n",
    "\n",
    "In the case of entity recognition, each token in a sentence is classified separately in a sequence, i.e. a sequence labelling or token classification task. Whether a finetuned model can be used for a specific task depends on the way it was fine-tuned with labeled data, i.e. what type of classification head as added to the model. \n",
    "\n",
    "In this notebook, we will demonstrate two differently fine-tuned models. We will use the ```sentiment-analysis``` pipeline to demonstrate text classification and the ```ner``` pipeline to demonstrate token classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search on the Model Hub of Huggingface for a fine-tuned xlm-roberta model for sentiment classification, e.g.:\n",
    "\n",
    "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment\n",
    "\n",
    "This model was trained on ~198M tweets and finetuned for sentiment analysis. According to the Huggingface model card text:\n",
    "\n",
    "\"The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\"\n",
    "\n",
    "    Barbieri, Francesco, Luis Espinosa Anke, and Jose Camacho-Collados. \"Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond.\" arXiv preprint arXiv:2104.12250 (2021).\n",
    "\n",
    "The cross-lingual capabilities come from XLM-roberta that has a vocabulary for 100 languages and represents texts in any of these in a language-agnostic way. Fine-tuning it with example in the 8 languages transfers to all 100 languages.\n",
    "\n",
    "Sentiment analysis is modeled as text classification. This means that the label is associated with a text as a whole and not to individual tokens (as is done for sequence classification). When using the model, we need to choose the same classification type as was set for training. Since we will use the **pipeline** API to the transformer models, we need to select a pipeline name that matches the training settings. You can check the Huggingface model card and training configuration of the model to check the details.\n",
    "\n",
    "In this case, this is easy as the pipeline name is also called \"sentiment-analysis\". We thus initialise a sentiment_task module by the **pipeline** constructor by giving it the task name \"sentiment-analysis\" and the name of the model on the Huggingface site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is loaded, you can pass in any text into the **sentiment_task** instance of the model to get the prediction. You can try out any of the 100 languages of XLM-RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.9273003935813904}]\n",
      "[{'label': 'negative', 'score': 0.8501129746437073}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_task(\"What an awful movie!\"))\n",
    "print(sentiment_task(\"Wat een waardeloze film!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity-recognition and classification (NERC) is typically conceived as a sequence labelling or token classification task. This means that every word (token) in the input text will receive a label as it occurs in a sequence. In the case of NERC, the labels mark each token in a text separately as either the beginning of a named-entity expression, being inside such an expression or being outside such an expression. This type of annotation is called **BIO** or **IOB** annotation, where B=beginning, I=inside and O=outside. In addition to the B and I tag, the type of entity is added as a suffix, e.g. B-PER=beginning of an expression that names a person, whereas B-LOC=beginning of an expression that names a location.\n",
    "\n",
    "Transformer models that are fine-tuned for NERC typically are set to do sequence labelling. To use such a model, we can create a pipeline for the task \"ner\". Always check the model card on Huggingface which pipeline task it was designed for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nerc_task = pipeline(\"ner\", model=\"Davlan/xlm-roberta-base-ner-hrl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.9998417, 'index': 1, 'word': '▁Na', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-PER', 'score': 0.88056296, 'index': 2, 'word': 'der', 'start': 2, 'end': 5}\n",
      "{'entity': 'I-PER', 'score': 0.999816, 'index': 3, 'word': '▁Jo', 'start': 5, 'end': 8}\n",
      "{'entity': 'I-PER', 'score': 0.9998022, 'index': 4, 'word': 'kha', 'start': 8, 'end': 11}\n",
      "{'entity': 'I-PER', 'score': 0.99975294, 'index': 5, 'word': 'dar', 'start': 11, 'end': 14}\n",
      "{'entity': 'B-LOC', 'score': 0.99962485, 'index': 8, 'word': '▁Syria', 'start': 24, 'end': 30}\n"
     ]
    }
   ],
   "source": [
    "example = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\n",
    "nerc_results = nerc_task(example)\n",
    "for result in nerc_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tokens in the output do not correspond to the words from the input. Remember that XLM-RoBERTa captures 100 languages and although the vocabulary is more than 250K items, this is by far not enough to represent all words of these languages. Therefore, the tokenizer of the model breaks down these words to smaller pieces in order to represent the complete sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.9998752, 'index': 1, 'word': '▁Mark', 'start': 0, 'end': 4}\n",
      "{'entity': 'I-PER', 'score': 0.99985504, 'index': 2, 'word': '▁Rut', 'start': 4, 'end': 8}\n",
      "{'entity': 'I-PER', 'score': 0.99987614, 'index': 3, 'word': 'te', 'start': 8, 'end': 10}\n",
      "{'entity': 'B-ORG', 'score': 0.999185, 'index': 9, 'word': '▁VVD', 'start': 29, 'end': 33}\n",
      "{'entity': 'B-ORG', 'score': 0.99986625, 'index': 13, 'word': '▁Google', 'start': 54, 'end': 61}\n",
      "{'entity': 'B-ORG', 'score': 0.999859, 'index': 15, 'word': '▁Facebook', 'start': 62, 'end': 71}\n",
      "{'entity': 'B-ORG', 'score': 0.9998468, 'index': 17, 'word': '▁Apple', 'start': 74, 'end': 80}\n"
     ]
    }
   ],
   "source": [
    "example = \"Mark Rutte kondigt aan dat de VVD tech bedrijven zoals Google, Facebook en Apple zwaarder gaat belasten.\"\n",
    "nerc_results = nerc_task(example)\n",
    "for result in nerc_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
