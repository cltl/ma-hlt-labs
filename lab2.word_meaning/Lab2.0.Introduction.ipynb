{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB2: Word meaning, similarity and relatedness\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nobody can tell how many words there are in a language nor what different meanings these words have or can have in different contexts. Nobody knows how to define these meanings correctly either. \n",
    "\n",
    "**WordNet** is a semantic lexical database that can be seen as a proxy for the words of the English languages with their meanings defined through lexical semantic relations. The core unit of WordNet is the synset: a set of synonymous words that represents a single concept. For example, the English words *car, auto, automobile, machine, motorcar* form a synset in the English WordNet that represents a single concept. \n",
    "\n",
    "Semantic relations are defined between synsets. Likewise the *car* synset is related to the synset *motor vehicle, automotive vehicle* through a hyponymy relation. As WordNet contains over a hundred-thousand words and synsets, it forms a huge network of concepts and a mapping of the English vocabulary to these concepts. Through the semantic relations, WordNet represents a large semantic space in the form of a graph.\n",
    "\n",
    "Wordnets have been built for many other languages as well, mostly by *translating* the synsets from English to these languages and maintaining the meanings of the synsets and their relations. Wordnets can be used to calculate the similarity of word pairs through different methods traversing the network of relations.\n",
    "\n",
    "**Word embeddings** are vector representations for words derived from large volumes of text through unsupervised machine learning. These vector representations tyically have a few hundred dimensions. Each word has different values for these dimensions but similar words have similar values. The values are learned by predicting context words observed in a large text collection. A neural network adapts a small set of weights to make them more similar to context words and less similar to words that do not occur in its context (usually picked randomly from the vocabulary). After training (many examples and many iterations), the weights of the hidden layer form dense vector. Since semantically related words occur in similar contexts, their weights (vectors) also tend to be similar. \n",
    "\n",
    "Word embeddings are a powerful resource that can be learned without supervision from text. Whereas wordnets have been built manually on the basis of human intuition, embeddings are derived empirically from data.\n",
    "\n",
    "In this LAB, you are going to work with:\n",
    "\n",
    "* Wordnet in English as provided in NLTK\n",
    "* Embeddings built from Wikipedia for different languages\n",
    "* Create you own embeddings from text corpora provided by the Leibzig Corpora Collection\n",
    "* Extract a lexicon of emotion words from wordnet and the embeddings and compare these.\n",
    "\n",
    "LAB2 contains three notebooks, excluding this introduction:\n",
    "\n",
    "* Lab2.1.NLTK_wordnet.ipynb\n",
    "* Lab2.2.Using_Wordembeddings.ipynb\n",
    "* Lab2.3.Creating_Wordembeddings.ipynb\n",
    "* Lab2.4.Assignment.ipynb\n",
    "\n",
    "You are supposed to work through the first three notebooks in this order before making the assignment. \n",
    "\n",
    "In the other folder yo find two more notebooks in case you are interested:\n",
    "\n",
    "* Lab2.5.NLTK_concordances.ipynb (optional)\n",
    "* Lab2.6.NLTK_framenet.ipynb (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
