{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3482f58f-bece-4a5e-a9b9-090c39d2c278",
   "metadata": {},
   "source": [
    "# How to install your own Llama server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a1276-2584-46c2-9b36-ad1108defbfc",
   "metadata": {},
   "source": [
    "### Installation of a local Llama server\n",
    "\n",
    "This notebook explains how to install a llama server locally. The smallest Llama3 model (8B parameters) is almost 5GB in size. You need sufficient memory to load this model.\n",
    "\n",
    "* create a folder e.g. ```llama``` to install the server\n",
    "* open a terminal, navigate to the ```llama``` folder and install a new virtual environment ```venv```(see the [documentation](https://docs.python.org/3/library/venv.html):\n",
    "    * python -m venv venv\n",
    "* activate the virtual environment:\n",
    "    * On Linux/Mac\n",
    "        * source ./venv/bin/activate\n",
    "    * On Windows:\n",
    "        * .\\venv\\Scripts\\activate.bat\n",
    "\n",
    "* install the following dependencies in the ```llama``` folder:\n",
    "    * pip install --upgrade --quiet llama-cpp-python (it make take a while before all dependencies are installed)\n",
    "    * pip install sse_starlette \n",
    "    * pip install starlette_context \n",
    "    * pip install pydantic_settings \n",
    "\n",
    "* Download a Llama model from [huggingface](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF) and place it in a local folder, e.g. \"models\". You can also download the model from the next [Google-drive](https://drive.google.com/file/d/1gIYCP-3zCjVlD8EXm5OggKXaYH4MLqTB/view?usp=share_link).\n",
    "\n",
    "To launch llama_server specify the path to the local model and the port. The script ```llama_cpp_server``` can be found at the ```llama``` folder next to the ```venv``` folder:\n",
    "\n",
    "    llama> python -m llama_cpp.server --host 0.0.0.0 --model ./models/Meta-Llama-3-8B-Instruct.Q2_K.gguf --n_ctx 2048 --port 9001\n",
    "\n",
    "Note that we assume here that you put the model in a folder ```models```. On a Windows machine use \".\\models\\/Meta-Llama-3-8B-Instruct.Q2_K.gguf\".\n",
    "\n",
    "If there are no error messages, the server is up and running and you can make calls to this port 9001 using a Llama client using: http://localhost:9001/v1. See the notebook **llama-chat.ipynb** how to run a chatbot that uses this server.\n",
    "\n",
    "If you do not have sufficient memory or get an error message launching the server, please use the CLTL server. Details how to connect to the CLTL server will be provided in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6120a-c1e3-47e9-9220-8bf4ce6044e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
