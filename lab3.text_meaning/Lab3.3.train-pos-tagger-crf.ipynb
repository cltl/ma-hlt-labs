{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ae9d9a-b347-4f82-a1ce-a172e6932a6e",
   "metadata": {},
   "source": [
    "# Lab3.3 Part-of-Speech tagger as Token in Sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46559498-a422-4a49-933d-044deed52cb6",
   "metadata": {},
   "source": [
    "In Lab3.2, you represented a text as a Bag-of-Words or BoW. In a BoW, the order of the words does not matter. This may be fine for text classification tasks such as emotion detection and topic classification in which the text as a whole can be associated with an interpretation. However, for many other NLP tasks words or phrases need to be interpreted as they occur in context. For this we need to represent a text as a **sequence** of words and we need to classify words or phrases as being part of such a sequence.\n",
    "\n",
    "The task of assigning an interpretation to a word or phrase in a sequence is called **Token in Sequence** or **TiS** classification. Typical **TiS** classification tasks are: Part-of-Speech tagging, Named Entity Recognition, Event Recognition and Semantic Role Labelling.\n",
    "\n",
    "Whereas in the case of a Text Classification task, we represent the complete text as a feature vector such as a BoW, in the case of TiS we need to represent each token in a sequence using a feature vector.\n",
    "\n",
    "Assume we use a feature vector with four dimension for the presence of words \"the\", \"chicken\", \"produced\" and \"egg\" in this order. The difference between a very simple BoW and a TiS representation for the following two sentences would be as follows:\n",
    "\n",
    "```\n",
    "the chicken produced the egg\n",
    "    BoW = [1, 1, 1, 1]\n",
    "    TiS = [1, 0, 0, 0][0, 1, 0, 0][0, 0, 1, 0][1, 0, 0, 0][0, 0, 0, 1]\n",
    "the egg produced the chicken\n",
    "    BoW = [1, 1, 1, 1]\n",
    "    TiS = [1, 0, 0, 0][0, 0, 0, 1][0, 0, 1, 0][1, 0, 0, 0][0, 1, 0, 0]\n",
    "```\n",
    "\n",
    "The BoW repersentation of the two sentences would be the same but the TiS representations are different. Here the TiS representation only captures the word itself as a so-called one-hot vector. In reality, we package a lot of features in the representation of a token among which word-shape, suffixes and prefixes, previous and following words.\n",
    "\n",
    "These rich representations are used to predict a label such as Part-of-Speech or Named-Entity Type to each word or token separately.\n",
    "\n",
    "In this notebook, we are going to demonstrate how you can train a so-called **Conditional Random Field** or **CRF** classifier to predict the part-of-speech of words in a sequence. For this we are going to represent the training data as TiS and feed this to the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ca5e9-ed47-4e5f-90f1-4f2c53bc1877",
   "metadata": {},
   "source": [
    "## 1. Conditional Random Fields or CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76410b6c-1c72-4ceb-b4d0-cfea817205c9",
   "metadata": {},
   "source": [
    "**CRF** is a discriminative classifier  that evaluates the probabilities that a set of states are dependent or not between themselves based on a set of observations. In this case, it evaluates the probabilities that a word observed in a context belongs to a specific PoS. In training time, it takes what is the best state given the set of current observations and probabilities.\n",
    "\n",
    "In next image, X1 to X4 repersent a sequence of input tokens or words. The goal is to predict Y1 to Y4 as labels given the sequence of input tokens. For each X there is a probability $\\Phi$ for the token to predict a label Y. We also see that there is another probability $\\Phi$ that a label Y is followed by another Y. In a sequence classification task these conditional probablities are taken into account as well. The algorithm progresses through the sequence to choose the optimal set of corresponding labels.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/CRF.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Further mathematical details can be found in: https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7985f-d192-49c5-b80f-bf702122fba8",
   "metadata": {},
   "source": [
    "## 2. Creating a token in sequence representation of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b2691-3293-42d8-bb66-a86ee800d9c7",
   "metadata": {},
   "source": [
    "Token-in-Sequence or TiS representations are typically create for sentences and not for long documents. The sequential relations between words are stronger in a sentence with some grammatical structure. Long-distance (beyond the sentence boundary) dependencies between words are more difficult to capture.\n",
    "\n",
    "In order to make a prediction for each word in a sequence, you need to know a lot about the word and about the sequence in which it occurs. Furthermore, you want the model to generalise over the word itself. For example, the sequence ```determiner - adjective``` is very likely to be followed by ```noun``` in English regardless of the words. Furthermore to know that something could be an adjective or a noun can also be learned from the beginning (prefix) and ending (suffix) of a word. In the same vein, the use of capitals, digits, or the position of the word in the sentence can play a role.\n",
    "\n",
    "For TiS classification tasks, very rich feature bundles are defined to define each word or token in a sequence. Below is a very simple function that takes a list of tokens as a sentence as input and returns a feature bundle for the n-th word in a sentence. Take a minute to study the next function. Note that the ```re``` module is used for creating regular expressions, which are small grammars that define patterns of characters in strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "03604c70-62cd-4b6b-aa66-f334c2b69fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex module for checking alphanumeric values.\n",
    "import re\n",
    "def extract_features(sentence, index):\n",
    "  return {\n",
    "      'word':sentence[index], ## the word itself is a feature\n",
    "      'is_first':index==0, ## True if it is the first word in a sentence\n",
    "      'is_last':index ==len(sentence)-1, ## True if it is the last word in a sentence\n",
    "      'is_capitalized':sentence[index][0].upper() == sentence[index][0], ## The first character is a capital\n",
    "      'is_all_caps': sentence[index].upper() == sentence[index], ## All characters capitalized\n",
    "      'is_all_lower': sentence[index].lower() == sentence[index], ## All characters are lower case\n",
    "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))), ## Mixture of characters and digits\n",
    "      'prefix-1':sentence[index][0], ## first character\n",
    "      'prefix-2':sentence[index][:2], ## first two chars\n",
    "      'prefix-3':sentence[index][:3], ## first three chars\n",
    "      'suffix-1':sentence[index][-1], ## last character\n",
    "      'suffix-2':sentence[index][-2:], ## last two chars\n",
    "      'suffix-3':sentence[index][-3:], ## last three chars\n",
    "      'prev_word':'' if index == 0 else sentence[index-1], ## previous word if any\n",
    "      'next_word':'' if index < len(sentence) else sentence[index+1], ## next word if any\n",
    "      'has_hyphen': '-' in sentence[index], ## If it has a hyphen True\n",
    "      'is_numeric': sentence[index].isdigit(), ## only digits\n",
    "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:] ## Capitals inside the word\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3928f7a-9172-4af0-9f12-6526d982ce99",
   "metadata": {},
   "source": [
    "By tokenizing a sentence using ```NLTK```, we can turn a sentence in a sequence list and pass the words one by one the function through their position in the list. The function will return the feature bundle for each word, which we append to the feature sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0e17e66a-00a4-41be-9336-8bb1933063f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'Fruit', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'F', 'prefix-2': 'Fr', 'prefix-3': 'Fru', 'suffix-1': 't', 'suffix-2': 'it', 'suffix-3': 'uit', 'prev_word': '', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'flies', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'f', 'prefix-2': 'fl', 'prefix-3': 'fli', 'suffix-1': 's', 'suffix-2': 'es', 'suffix-3': 'ies', 'prev_word': 'Fruit', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'like', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'l', 'prefix-2': 'li', 'prefix-3': 'lik', 'suffix-1': 'e', 'suffix-2': 'ke', 'suffix-3': 'ike', 'prev_word': 'flies', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'a', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'a', 'prefix-2': 'a', 'prefix-3': 'a', 'suffix-1': 'a', 'suffix-2': 'a', 'suffix-3': 'a', 'prev_word': 'like', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'banana', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'b', 'prefix-2': 'ba', 'prefix-3': 'ban', 'suffix-1': 'a', 'suffix-2': 'na', 'suffix-3': 'ana', 'prev_word': 'a', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': '.', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': '.', 'prefix-2': '.', 'prefix-3': '.', 'suffix-1': '.', 'suffix-2': '.', 'suffix-3': '.', 'prev_word': 'banana', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"Fruit flies like a banana.\"\n",
    "token_sequence_sentence = nltk.word_tokenize(sentence)\n",
    "feature_sequence_sentence = []\n",
    "for index in range(len(token_sequence_sentence)):\n",
    "    feature_sequence_sentence.append(extract_features(token_sequence_sentence, index)),\n",
    "\n",
    "for item in feature_sequence_sentence:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858a24e-c170-49e8-b95b-8acbb931ec75",
   "metadata": {},
   "source": [
    "We see that each word is now represented by a dictionary with feature names and values. The classifier will turn these feature representations internally into numeric representations and associate these with sequences of labels. Below, we will associate these with part-of-speech tags from the [Penn Treebank](https://paperswithcode.com/dataset/penn-treebank), which is included in NLTK. The Penn Treebank is a corpus that is split into sentences and annotated with Part-of-Speech tags by human annotators.\n",
    "\n",
    "The next code will iterate over all the sentences (represented by file ids) and get each word and PoS tag in sequence. We create two paired sequences lists of words and their PoS tags for each sentence and add these to ```penn_treebank```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1222d454-7c36-4328-b3ce-d5c4933de341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /Users/piek/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Ensure that the treebank corpus is downloaded\n",
    "nltk.download('treebank')\n",
    "\n",
    "#Load the treebank corpus class\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "#Now we iterate over all samples from the corpus (the fileids are equivalent to sentences)\n",
    "#and retrieve the word and the pre-labeled PoS tag. This will be added as a list of tuples with\n",
    "#a list of words and a list of their respective PoS tags (in the same order).\n",
    "penn_treebank = []\n",
    "for fileid in treebank.fileids():\n",
    "  tokens = []\n",
    "  tags = []\n",
    "  for word, tag in treebank.tagged_words(fileid):\n",
    "    tokens.append(word)\n",
    "    tags.append(tag)\n",
    "  penn_treebank.append((tokens, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2dd5e-8125-4114-b146-46ad33945921",
   "metadata": {},
   "source": [
    "Each sentence is now represented as two list: a sequence of words and a sequences of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "45bf24d2-c87c-4988-b856-cc011fd1f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of annotated sentences 199\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.'] ['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.', 'NNP', 'NNP', 'VBZ', 'NN', 'IN', 'NNP', 'NNP', ',', 'DT', 'NNP', 'VBG', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Nr. of annotated sentences', len(penn_treebank))\n",
    "\n",
    "### Showing the first two\n",
    "for token_tag_sequences in penn_treebank:\n",
    "    print(token_tag_sequences[0], token_tag_sequences[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b75f0-70dc-4234-8c1e-f4a99ab7d0af",
   "metadata": {},
   "source": [
    "We can now use the feature extraction function that we described above to represent all sentences and associate these with the sequences of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "48e44777-9576-4909-929b-33b9158a7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns the data as two lists, one of Dicts of features and the other with the labels.\n",
    "def transform_to_dataset(tagged_sentence_pairs):\n",
    "  feature_sequences, label_sequences = [], []\n",
    "  for sentence, tags in tagged_sentence_pairs:\n",
    "    sent_word_features, sent_tags = [],[]\n",
    "    for index in range(len(sentence)):\n",
    "        sent_word_features.append(extract_features(sentence, index)),\n",
    "        sent_tags.append(tags[index])\n",
    "    feature_sequences.append(sent_word_features)\n",
    "    label_sequences.append(sent_tags)\n",
    "  return feature_sequences, label_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d40ab-ceba-434b-91c2-864c40be9f72",
   "metadata": {},
   "source": [
    "Before we transform the Penn treebank with the above function, we split in 80% train and 20% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "04543f2d-2b2f-4d09-a107-69e9d503aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "penn_train_size = int(0.8*len(penn_treebank))\n",
    "penn_training = penn_treebank[:penn_train_size]\n",
    "penn_testing = penn_treebank[penn_train_size:]\n",
    "# Extract for each the feature sequences and label sequences\n",
    "penn_train_feature_sequences, penn_train_label_sequences = transform_to_dataset(penn_training)\n",
    "penn_test_feature_sequences, penn_test_label_sequences = transform_to_dataset(penn_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b3d39620-de80-4e34-8803-e704c0407b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of annotated sentences 159 159\n",
      "{'word': 'Pierre', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'P', 'prefix-2': 'Pi', 'prefix-3': 'Pie', 'suffix-1': 'e', 'suffix-2': 're', 'suffix-3': 'rre', 'prev_word': '', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'Vinken', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'V', 'prefix-2': 'Vi', 'prefix-3': 'Vin', 'suffix-1': 'n', 'suffix-2': 'en', 'suffix-3': 'ken', 'prev_word': 'Pierre', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': ',', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': ',', 'prefix-2': ',', 'prefix-3': ',', 'suffix-1': ',', 'suffix-2': ',', 'suffix-3': ',', 'prev_word': 'Vinken', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': '61', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': '6', 'prefix-2': '61', 'prefix-3': '61', 'suffix-1': '1', 'suffix-2': '61', 'suffix-3': '61', 'prev_word': ',', 'next_word': '', 'has_hyphen': False, 'is_numeric': True, 'capitals_inside': False}\n",
      "{'word': 'years', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'y', 'prefix-2': 'ye', 'prefix-3': 'yea', 'suffix-1': 's', 'suffix-2': 'rs', 'suffix-3': 'ars', 'prev_word': '61', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'old', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'o', 'prefix-2': 'ol', 'prefix-3': 'old', 'suffix-1': 'd', 'suffix-2': 'ld', 'suffix-3': 'old', 'prev_word': 'years', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': ',', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': ',', 'prefix-2': ',', 'prefix-3': ',', 'suffix-1': ',', 'suffix-2': ',', 'suffix-3': ',', 'prev_word': 'old', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'will', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'w', 'prefix-2': 'wi', 'prefix-3': 'wil', 'suffix-1': 'l', 'suffix-2': 'll', 'suffix-3': 'ill', 'prev_word': ',', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'join', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'j', 'prefix-2': 'jo', 'prefix-3': 'joi', 'suffix-1': 'n', 'suffix-2': 'in', 'suffix-3': 'oin', 'prev_word': 'will', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'the', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 't', 'prefix-2': 'th', 'prefix-3': 'the', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'the', 'prev_word': 'join', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'board', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'b', 'prefix-2': 'bo', 'prefix-3': 'boa', 'suffix-1': 'd', 'suffix-2': 'rd', 'suffix-3': 'ard', 'prev_word': 'the', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'as', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'a', 'prefix-2': 'as', 'prefix-3': 'as', 'suffix-1': 's', 'suffix-2': 'as', 'suffix-3': 'as', 'prev_word': 'board', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'a', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'a', 'prefix-2': 'a', 'prefix-3': 'a', 'suffix-1': 'a', 'suffix-2': 'a', 'suffix-3': 'a', 'prev_word': 'as', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'nonexecutive', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'n', 'prefix-2': 'no', 'prefix-3': 'non', 'suffix-1': 'e', 'suffix-2': 've', 'suffix-3': 'ive', 'prev_word': 'a', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'director', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'd', 'prefix-2': 'di', 'prefix-3': 'dir', 'suffix-1': 'r', 'suffix-2': 'or', 'suffix-3': 'tor', 'prev_word': 'nonexecutive', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'Nov.', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'N', 'prefix-2': 'No', 'prefix-3': 'Nov', 'suffix-1': '.', 'suffix-2': 'v.', 'suffix-3': 'ov.', 'prev_word': 'director', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': '29', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': '2', 'prefix-2': '29', 'prefix-3': '29', 'suffix-1': '9', 'suffix-2': '29', 'suffix-3': '29', 'prev_word': 'Nov.', 'next_word': '', 'has_hyphen': False, 'is_numeric': True, 'capitals_inside': False}\n",
      "{'word': '.', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': '.', 'prefix-2': '.', 'prefix-3': '.', 'suffix-1': '.', 'suffix-2': '.', 'suffix-3': '.', 'prev_word': '29', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'Mr.', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'M', 'prefix-2': 'Mr', 'prefix-3': 'Mr.', 'suffix-1': '.', 'suffix-2': 'r.', 'suffix-3': 'Mr.', 'prev_word': '.', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'Vinken', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'V', 'prefix-2': 'Vi', 'prefix-3': 'Vin', 'suffix-1': 'n', 'suffix-2': 'en', 'suffix-3': 'ken', 'prev_word': 'Mr.', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'is', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'i', 'prefix-2': 'is', 'prefix-3': 'is', 'suffix-1': 's', 'suffix-2': 'is', 'suffix-3': 'is', 'prev_word': 'Vinken', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'chairman', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'c', 'prefix-2': 'ch', 'prefix-3': 'cha', 'suffix-1': 'n', 'suffix-2': 'an', 'suffix-3': 'man', 'prev_word': 'is', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'o', 'prefix-2': 'of', 'prefix-3': 'of', 'suffix-1': 'f', 'suffix-2': 'of', 'suffix-3': 'of', 'prev_word': 'chairman', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'Elsevier', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'E', 'prefix-2': 'El', 'prefix-3': 'Els', 'suffix-1': 'r', 'suffix-2': 'er', 'suffix-3': 'ier', 'prev_word': 'of', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'N.V.', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'N', 'prefix-2': 'N.', 'prefix-3': 'N.V', 'suffix-1': '.', 'suffix-2': 'V.', 'suffix-3': '.V.', 'prev_word': 'Elsevier', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': True}\n",
      "{'word': ',', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': ',', 'prefix-2': ',', 'prefix-3': ',', 'suffix-1': ',', 'suffix-2': ',', 'suffix-3': ',', 'prev_word': 'N.V.', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'the', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 't', 'prefix-2': 'th', 'prefix-3': 'the', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'the', 'prev_word': ',', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'Dutch', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'D', 'prefix-2': 'Du', 'prefix-3': 'Dut', 'suffix-1': 'h', 'suffix-2': 'ch', 'suffix-3': 'tch', 'prev_word': 'the', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'publishing', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'p', 'prefix-2': 'pu', 'prefix-3': 'pub', 'suffix-1': 'g', 'suffix-2': 'ng', 'suffix-3': 'ing', 'prev_word': 'Dutch', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': 'group', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'g', 'prefix-2': 'gr', 'prefix-3': 'gro', 'suffix-1': 'p', 'suffix-2': 'up', 'suffix-3': 'oup', 'prev_word': 'publishing', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "{'word': '.', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': '.', 'prefix-2': '.', 'prefix-3': '.', 'suffix-1': '.', 'suffix-2': '.', 'suffix-3': '.', 'prev_word': 'group', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}\n",
      "['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.', 'NNP', 'NNP', 'VBZ', 'NN', 'IN', 'NNP', 'NNP', ',', 'DT', 'NNP', 'VBG', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Nr. of annotated sentences', len(penn_train_feature_sequences), len(penn_train_label_sequences))\n",
    "\n",
    "### Showing the first feature sequence\n",
    "for features in penn_train_feature_sequences:\n",
    "    for feature in features:\n",
    "        print(feature)\n",
    "    break\n",
    "\n",
    "### Showing the first tag sequence\n",
    "for tags in penn_train_label_sequences:\n",
    "    print(tags)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a1226-bbec-47bc-af71-4d47c6ed3cc2",
   "metadata": {},
   "source": [
    "We can now feed the data to an Sklearn CRF classifier as is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d659097-4d6c-4f91-8b99-22a986ac7a52",
   "metadata": {},
   "source": [
    "## 3 Training  the CRF Pos-tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628dea5-8873-4f16-8f42-6cee1d98ae4c",
   "metadata": {},
   "source": [
    "Now, we use the Conditional Random Fields (CRF) algorithm that is provided in a specific package of Sklearn called ```sklearn_crfsuite``` to train a Token in Sequence or TiS classifier that assigns PoS tags to sequences of words in a sentence. The package should already be installed at the start of the course. Otherwise, run the next cell after removing the comment tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fc3018ac-6954-4cf2-b02c-157fb1132172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn_crfsuite==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd99576-892b-42e8-91e9-b60b67f27c6d",
   "metadata": {},
   "source": [
    "We can create an instance of ```CRF``` with the deault settings as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9c55e726-9050-4d52-92bb-c7405a629e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "penn_crf_pos = CRF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113a831-0553-4be8-bb49-013b47511129",
   "metadata": {},
   "source": [
    "We created the CRF instance without specifying any parameters. THis means we are using the default settings. There are a number of parameters that can be defined. Here are some:\n",
    "\n",
    "* algorithm: methodology used to check if results are improving. Default is lbfgs (gradient descent).\n",
    "* c1 and c2:  coefficients used for regularization.\n",
    "* max_iterations: max number of iterations\n",
    "* all_possible_transitions: crf creates a \"network\" of probability transition states, this option allows it to map \"connections\" not directly present in the data.\n",
    "\n",
    "You could also use a more advance setting to define our instance:\n",
    "```\n",
    "penn_crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.01,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d83ca-a4f7-469d-8187-e982f5c0391e",
   "metadata": {},
   "source": [
    "We call the ```fit``` function to pass the feature representation of the sentences and the corresponding PoS tag sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f715a345-29b7-4ff0-80c4-a34c206e7203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training on Penn Treebank corpus!\n",
      "Finished training on Penn Treebank corpus!\n"
     ]
    }
   ],
   "source": [
    "#The fit method is the default name used by Machine Learning algorithms to start training.\n",
    "print(\"Started training on Penn Treebank corpus!\")\n",
    "penn_crf_pos.fit(penn_train_feature_sequences, penn_train_label_sequences)\n",
    "print(\"Finished training on Penn Treebank corpus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d393c-3a73-4bcf-a55f-3b57faa4c376",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0941ce6-0e55-4fa6-aa88-9a526a1e420a",
   "metadata": {},
   "source": [
    "For eavaluating the Penn Treebank test set that we split off, we can feed the feature sequence representation directly to the classiifer as we did for the \"Fruit flies\" sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c1369333-2223-4f37-b216-773ee835a53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "The first sentence gold labels in sequence\n",
      "['NNP', 'NNP', 'VBD', 'DT', 'NN', 'JJ', 'NN', 'IN', '$', 'CD', 'CD', '-NONE-', ',', 'CC', 'CD', 'NNS', 'DT', 'NN', ',', 'VBN', 'IN', 'JJ', 'NN', 'IN', '$', 'CD', 'CD', '-NONE-', ',', 'CC', 'CD', 'NN', 'DT', 'NN', '.', 'DT', 'NN', 'IN', 'DT', 'NNP', ',', 'JJ', 'NN', 'VBD', '-NONE-', 'NNS', 'VBD', 'DT', 'NN', 'IN', '$', 'CD', 'CD', '-NONE-', 'IN', 'DT', 'NN', ':', 'IN', 'NN', ',', 'DT', 'NN', 'VBD', 'VBN', '-NONE-', 'IN', 'VBG', 'NNS', 'VBG', '$', 'CD', 'CD', '-NONE-', 'CC', '$', 'CD', 'CD', '-NONE-', 'IN', 'NN', 'NNS', 'IN', 'PRP', 'VBD', '-NONE-', 'IN', '``', 'JJ', '.', \"''\", 'DT', 'NNS', 'VBD', 'RB', 'VBN', '-NONE-', 'IN', 'DT', '$', 'CD', 'CD', '-NONE-', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'CD', 'JJ', 'NNS', ',', 'PRP', 'VBD', '-NONE-', '-NONE-', '.', 'NN', 'VBD', 'CD', 'NN', 'TO', '$', 'CD', 'CD', '-NONE-', ',', 'IN', '$', 'CD', 'CD', '-NONE-', 'DT', 'NN', 'JJR', '.', 'NNP', 'VBD', '``', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'IN', 'WDT', 'NNP', 'VBZ', '-NONE-', '.']\n",
      "The first sentence predicted labels in sequence\n",
      "['NNP', 'NNP', 'VBD', 'DT', 'JJ', 'JJ', 'NN', 'IN', '$', 'CD', 'CD', '-NONE-', ',', 'CC', 'CD', 'NNS', 'DT', 'NN', ',', 'VBN', 'IN', 'JJ', 'NN', 'IN', '$', 'CD', 'CD', '-NONE-', ',', 'CC', 'CD', 'NN', 'DT', 'NN', '.', 'DT', 'NN', 'IN', 'DT', 'NNP', ',', 'VBN', 'NN', 'VBD', '-NONE-', 'NNS', 'VBD', 'DT', 'NN', 'IN', '$', 'CD', 'CD', '-NONE-', 'IN', 'DT', 'NN', ':', 'IN', 'NN', ',', 'DT', 'NN', 'VBD', 'VBN', '-NONE-', 'IN', 'JJ', 'NNS', 'VBG', '$', 'CD', 'CD', '-NONE-', 'CC', '$', 'CD', 'CD', '-NONE-', 'IN', 'NN', 'NNS', 'IN', 'PRP', 'VBD', '-NONE-', 'IN', '``', 'JJ', '.', \"''\", 'DT', 'NNS', 'VBD', 'RB', 'VBN', '-NONE-', 'IN', 'DT', '$', 'CD', 'CD', '-NONE-', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'CD', 'JJ', 'NNS', ',', 'PRP', 'VBD', '-NONE-', '-NONE-', '.', 'NNP', 'VBD', 'CD', 'NN', 'TO', '$', 'CD', 'CD', '-NONE-', ',', 'IN', '$', 'CD', 'CD', '-NONE-', 'DT', 'NN', 'JJR', '.', 'NNP', 'VBD', '``', 'DT', 'JJ', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'NNS', 'IN', 'DT', 'NN', 'NNS', 'IN', 'WDT', 'NNP', 'VBZ', '-NONE-', '.']\n"
     ]
    }
   ],
   "source": [
    "penn_predictions=penn_crf_pos.predict(penn_test_feature_sequences)\n",
    "print(len(penn_test_label_sequences))\n",
    "print(len(penn_predictions))\n",
    "print('The first sentence gold labels in sequence')\n",
    "print(penn_test_label_sequences[0])\n",
    "print('The first sentence predicted labels in sequence')\n",
    "print(penn_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6995d-1245-41b5-a0f8-6fec69af3e37",
   "metadata": {},
   "source": [
    "The predictions are made sentence by sentence and the result is a list of 40 sentence predictions. To evaluate the results, we  need to use a specific method ```flat_classification_report``` from ```sklearn_crfsuite.metrics``` to handle lists of lists as predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1426c61a-cb2f-4dbf-ab41-ae7025f1a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Penn Treebank CRF PoS tagger##\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           $      1.000     1.000     1.000       170\n",
      "          ''      1.000     1.000     1.000        52\n",
      "           ,      1.000     1.000     1.000       592\n",
      "       -LRB-      1.000     1.000     1.000        16\n",
      "      -NONE-      0.997     1.000     0.998       871\n",
      "       -RRB-      1.000     1.000     1.000        16\n",
      "           .      1.000     1.000     1.000       509\n",
      "           :      1.000     1.000     1.000        49\n",
      "          CC      1.000     0.997     0.998       287\n",
      "          CD      1.000     0.990     0.995       683\n",
      "          DT      0.990     0.993     0.992      1062\n",
      "          EX      0.750     1.000     0.857         3\n",
      "          IN      0.976     0.981     0.978      1285\n",
      "          JJ      0.856     0.893     0.874       731\n",
      "         JJR      0.830     0.936     0.880        47\n",
      "         JJS      0.821     0.958     0.885        24\n",
      "          MD      0.993     1.000     0.996       135\n",
      "          NN      0.951     0.939     0.945      1899\n",
      "         NNP      0.948     0.962     0.955      1213\n",
      "        NNPS      0.667     0.378     0.483        37\n",
      "         NNS      0.945     0.973     0.959       740\n",
      "         PDT      0.000     0.000     0.000         4\n",
      "         POS      0.992     1.000     0.996       124\n",
      "         PRP      1.000     1.000     1.000       150\n",
      "        PRP$      1.000     1.000     1.000        74\n",
      "          RB      0.919     0.878     0.898       296\n",
      "         RBR      0.800     0.308     0.444        13\n",
      "         RBS      1.000     1.000     1.000         1\n",
      "          RP      0.600     0.720     0.655        25\n",
      "          TO      1.000     1.000     1.000       298\n",
      "          VB      0.964     0.933     0.948       313\n",
      "         VBD      0.959     0.939     0.949       492\n",
      "         VBG      0.927     0.892     0.909       185\n",
      "         VBN      0.897     0.903     0.900       279\n",
      "         VBP      0.878     0.902     0.890       112\n",
      "         VBZ      0.934     0.904     0.919       219\n",
      "         WDT      0.969     1.000     0.984        62\n",
      "          WP      1.000     1.000     1.000        13\n",
      "         WP$      1.000     1.000     1.000         4\n",
      "         WRB      1.000     0.909     0.952        22\n",
      "          ``      1.000     1.000     1.000        55\n",
      "\n",
      "    accuracy                          0.961     13162\n",
      "   macro avg      0.916     0.909     0.908     13162\n",
      "weighted avg      0.961     0.961     0.960     13162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "print(\"## Penn Treebank CRF PoS tagger##\")\n",
    "print(metrics.flat_classification_report(\n",
    "    penn_test_label_sequences, penn_predictions, labels=penn_crf.classes_, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0497d2-2e7d-4d56-bb98-5cf8004e91a5",
   "metadata": {},
   "source": [
    "A weighted average is not state-of-the art but is a reasonable score. Still on average almost one out of fourteen words gets a wrong Part-of-Speech.\n",
    "\n",
    "We can see that Penn Treebank uses a large variety of PoS tags and that the performance varies across these tags, which correlates also with the support. Lower support means less training data and more arbitrary testing. For example ```RBS`` has a support of 1 and a zero score. Due to the variation, there is a large difference between the macro average and the weighted average. Adapting the parameters when creating the CRF instance may improve the macro average results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "be2666bc-2be0-4f69-902c-3e980f023fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 11267, 'IN': 8572, 'NNP': 8197, 'DT': 7103, '-NONE-': 5721, 'NNS': 5307, 'JJ': 5103, ',': 4294, '.': 3365, 'CD': 2863, 'VBD': 2551, 'RB': 2526, 'VB': 2241, 'CC': 1978, 'VBZ': 1906, 'TO': 1881, 'VBN': 1855, 'PRP': 1566, 'VBG': 1275, 'VBP': 1209, 'MD': 792, 'POS': 700, 'PRP$': 692, '``': 657, \"''\": 642, '$': 554, ':': 514, 'WDT': 383, 'JJR': 334, 'WP': 228, 'NNPS': 207, 'RP': 191, 'JJS': 158, 'WRB': 156, 'RBR': 123, '-RRB-': 110, '-LRB-': 104, 'EX': 85, 'RBS': 34, 'PDT': 23, '#': 16, 'LS': 13, 'WP$': 10, 'FW': 4, 'UH': 3, 'SYM': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "label_counts = Counter(metrics.flatten(penn_train_label_sequences))\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43f0fa-7e6f-4a14-8194-5f688f2ca1d2",
   "metadata": {},
   "source": [
    "## 4. Representation after thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00846a12-8764-46d1-9747-3c5c50f50919",
   "metadata": {},
   "source": [
    "The DictVectorizer function of Sklearn will convert the data to numerical one-hot vector representations for each feature. It will extract all possible values for each feature from the data and create a vector that can mark which value is true. If you inspect the above representation, you will see that many features only have two values such as True and False, but others are more open, such as the word itself and the suffixes. The latter require long sparse vectors as one-hot encodings.\n",
    "\n",
    "CRF does not provide functions to extract the feature vectors that it extracts directly. However, we can use the DictVectorizer to create the vectors from our feature representation and print these to inspect them. Lets do that for our \"Fruit flies\" sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f434038b-6bc0-4c49-bd93-c9e13bb4c131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature representation of the \"Fruit flies like a banana.\" sentence.\n",
      "[{'word': 'Fruit', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'is_alphanumeric': 0, 'prefix-1': 'F', 'prefix-2': 'Fr', 'prefix-3': 'Fru', 'suffix-1': 't', 'suffix-2': 'it', 'suffix-3': 'uit', 'prev_word': '', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'flies', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'f', 'prefix-2': 'fl', 'prefix-3': 'fli', 'suffix-1': 's', 'suffix-2': 'es', 'suffix-3': 'ies', 'prev_word': 'Fruit', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'like', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'l', 'prefix-2': 'li', 'prefix-3': 'lik', 'suffix-1': 'e', 'suffix-2': 'ke', 'suffix-3': 'ike', 'prev_word': 'flies', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'a', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'a', 'prefix-2': 'a', 'prefix-3': 'a', 'suffix-1': 'a', 'suffix-2': 'a', 'suffix-3': 'a', 'prev_word': 'like', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'banana', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': 'b', 'prefix-2': 'ba', 'prefix-3': 'ban', 'suffix-1': 'a', 'suffix-2': 'na', 'suffix-3': 'ana', 'prev_word': 'a', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '.', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'is_alphanumeric': 0, 'prefix-1': '.', 'prefix-2': '.', 'prefix-3': '.', 'suffix-1': '.', 'suffix-2': '.', 'suffix-3': '.', 'prev_word': 'banana', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}]\n",
      "Shape (6, 57)\n",
      "[[0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 1. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "['capitals_inside' 'has_hyphen' 'is_all_caps' 'is_all_lower'\n",
      " 'is_alphanumeric' 'is_capitalized' 'is_first' 'is_last' 'is_numeric'\n",
      " 'next_word=' 'prefix-1=.' 'prefix-1=F' 'prefix-1=a' 'prefix-1=b'\n",
      " 'prefix-1=f' 'prefix-1=l' 'prefix-2=.' 'prefix-2=Fr' 'prefix-2=a'\n",
      " 'prefix-2=ba' 'prefix-2=fl' 'prefix-2=li' 'prefix-3=.' 'prefix-3=Fru'\n",
      " 'prefix-3=a' 'prefix-3=ban' 'prefix-3=fli' 'prefix-3=lik' 'prev_word='\n",
      " 'prev_word=Fruit' 'prev_word=a' 'prev_word=banana' 'prev_word=flies'\n",
      " 'prev_word=like' 'suffix-1=.' 'suffix-1=a' 'suffix-1=e' 'suffix-1=s'\n",
      " 'suffix-1=t' 'suffix-2=.' 'suffix-2=a' 'suffix-2=es' 'suffix-2=it'\n",
      " 'suffix-2=ke' 'suffix-2=na' 'suffix-3=.' 'suffix-3=a' 'suffix-3=ana'\n",
      " 'suffix-3=ies' 'suffix-3=ike' 'suffix-3=uit' 'word=.' 'word=Fruit'\n",
      " 'word=a' 'word=banana' 'word=flies' 'word=like']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "print('Feature representation of the \"Fruit flies like a banana.\" sentence.')\n",
    "print(feature_sequence_sentence)\n",
    "\n",
    "vec = DictVectorizer(sparse=True)\n",
    "sentence_vec = vec.fit_transform(feature_sequence_sentence)  # Sparse matrix of shape (n_tokens, n_features)\n",
    "\n",
    "# Dense vector for easier viewing\n",
    "print('Shape', sentence_vec.shape)\n",
    "for token in sentence_vec:\n",
    "    print(token.toarray())\n",
    "\n",
    "# Feature names (column names of the vector)\n",
    "print(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b1096-4105-41ed-90d1-39a4812024af",
   "metadata": {},
   "source": [
    "We can see that our single sentence needs 57 dimensions to represent all the feature-value combinations represented by all the six words in this sentence.\n",
    "\n",
    "What will happen if we need to represent the full Penn Treebank corpus?\n",
    "\n",
    "We will not do this for the full data set but for a small set of sentences. To get the vectors we need to flatten the list of sentene representations to a single list of all the tokens. We also need some other functions to print these as the vectors get very long. We will only print the vector for the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "aedacd7d-e03c-4930-9a7b-767c5b2cbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have a list of tokens for the selection of sentences from the Penn treebank, each represented by a long sparse vector.\n",
      "Shape for the total set of tokens (840, 1794)\n",
      "[[0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "['capitals_inside', 'has_hyphen', 'is_all_caps', 'is_all_lower', 'is_alphanumeric', 'is_capitalized', 'is_first', 'is_last', 'is_numeric', 'next_word=', 'prefix-1=&', \"prefix-1='\", 'prefix-1=*', 'prefix-1=,', 'prefix-1=-', 'prefix-1=.', 'prefix-1=0', 'prefix-1=1', 'prefix-1=2', 'prefix-1=3', 'prefix-1=5', 'prefix-1=6', 'prefix-1=9', 'prefix-1=A', 'prefix-1=B', 'prefix-1=C', 'prefix-1=D', 'prefix-1=E', 'prefix-1=F', 'prefix-1=G', 'prefix-1=H', 'prefix-1=I', 'prefix-1=J', 'prefix-1=K', 'prefix-1=L', 'prefix-1=M', 'prefix-1=N', 'prefix-1=P', 'prefix-1=R', 'prefix-1=T', 'prefix-1=U', 'prefix-1=V', 'prefix-1=W', 'prefix-1=Y', 'prefix-1=`', 'prefix-1=a', 'prefix-1=b', 'prefix-1=c', 'prefix-1=d', 'prefix-1=e', 'prefix-1=f', 'prefix-1=g', 'prefix-1=h', 'prefix-1=i', 'prefix-1=j', 'prefix-1=k', 'prefix-1=l', 'prefix-1=m', 'prefix-1=n', 'prefix-1=o', 'prefix-1=p', 'prefix-1=q', 'prefix-1=r', 'prefix-1=s', 'prefix-1=t', 'prefix-1=u', 'prefix-1=v', 'prefix-1=w', 'prefix-1=y', 'prefix-2=&', \"prefix-2=''\", \"prefix-2='r\", \"prefix-2='s\", 'prefix-2=*', 'prefix-2=*-', 'prefix-2=*?', 'prefix-2=*I', 'prefix-2=*T', 'prefix-2=,', 'prefix-2=--', 'prefix-2=.', 'prefix-2=0', 'prefix-2=16', 'prefix-2=18', 'prefix-2=19', 'prefix-2=28', 'prefix-2=29', 'prefix-2=30', 'prefix-2=33', 'prefix-2=35', 'prefix-2=55', 'prefix-2=61', 'prefix-2=9.', 'prefix-2=A', 'prefix-2=A.', 'prefix-2=Ab', 'prefix-2=Ag', 'prefix-2=Al', 'prefix-2=Am', 'prefix-2=Ar', 'prefix-2=Bo', 'prefix-2=Br', 'prefix-2=Bu', 'prefix-2=By', 'prefix-2=Ca', 'prefix-2=Co', 'prefix-2=Da', 'prefix-2=Dr', 'prefix-2=Du', 'prefix-2=El', 'prefix-2=En', 'prefix-2=Fi', 'prefix-2=Fo', 'prefix-2=Fr', 'prefix-2=Go', 'prefix-2=Gr', 'prefix-2=Ha', 'prefix-2=Ho', 'prefix-2=In', 'prefix-2=It', 'prefix-2=Ja', 'prefix-2=Jo', 'prefix-2=Ju', 'prefix-2=Ke', 'prefix-2=Lo', 'prefix-2=Ma', 'prefix-2=Me', 'prefix-2=Mi', 'prefix-2=Mo', 'prefix-2=Mr', 'prefix-2=N.', 'prefix-2=Na', 'prefix-2=Ne', 'prefix-2=No', 'prefix-2=PL', 'prefix-2=Ph', 'prefix-2=Pi', 'prefix-2=Pr', 'prefix-2=Ru', 'prefix-2=T.', 'prefix-2=Ta', 'prefix-2=Th', 'prefix-2=U.', 'prefix-2=Un', 'prefix-2=Ve', 'prefix-2=Vi', 'prefix-2=Vo', 'prefix-2=We', 'prefix-2=Wo', 'prefix-2=Yo', 'prefix-2=``', 'prefix-2=a', 'prefix-2=ab', 'prefix-2=ac', 'prefix-2=ag', 'prefix-2=al', 'prefix-2=am', 'prefix-2=an', 'prefix-2=ap', 'prefix-2=ar', 'prefix-2=as', 'prefix-2=at', 'prefix-2=aw', 'prefix-2=ba', 'prefix-2=be', 'prefix-2=bi', 'prefix-2=bl', 'prefix-2=bo', 'prefix-2=br', 'prefix-2=bu', 'prefix-2=by', 'prefix-2=ca', 'prefix-2=ch', 'prefix-2=ci', 'prefix-2=cl', 'prefix-2=co', 'prefix-2=cr', 'prefix-2=cu', 'prefix-2=de', 'prefix-2=di', 'prefix-2=do', 'prefix-2=dr', 'prefix-2=du', 'prefix-2=ea', 'prefix-2=en', 'prefix-2=ev', 'prefix-2=ex', 'prefix-2=fa', 'prefix-2=fe', 'prefix-2=fi', 'prefix-2=fo', 'prefix-2=fr', 'prefix-2=gr', 'prefix-2=ha', 'prefix-2=he', 'prefix-2=hi', 'prefix-2=hu', 'prefix-2=im', 'prefix-2=in', 'prefix-2=is', 'prefix-2=it', 'prefix-2=jo', 'prefix-2=ki', 'prefix-2=la', 'prefix-2=le', 'prefix-2=li', 'prefix-2=lu', 'prefix-2=ma', 'prefix-2=me', 'prefix-2=mi', 'prefix-2=mo', \"prefix-2=n'\", 'prefix-2=na', 'prefix-2=ne', 'prefix-2=no', 'prefix-2=nu', 'prefix-2=of', 'prefix-2=ol', 'prefix-2=on', 'prefix-2=ot', 'prefix-2=ou', 'prefix-2=ov', 'prefix-2=ow', 'prefix-2=pa', 'prefix-2=pe', 'prefix-2=pl', 'prefix-2=po', 'prefix-2=pr', 'prefix-2=pu', 'prefix-2=qu', 'prefix-2=ra', 'prefix-2=re', 'prefix-2=ri', 'prefix-2=sa', 'prefix-2=sc', 'prefix-2=sh', 'prefix-2=sm', 'prefix-2=so', 'prefix-2=sp', 'prefix-2=st', 'prefix-2=su', 'prefix-2=sy', 'prefix-2=ta', 'prefix-2=te', 'prefix-2=th', 'prefix-2=ti', 'prefix-2=to', 'prefix-2=ty', 'prefix-2=un', 'prefix-2=up', 'prefix-2=us', 'prefix-2=ve', 'prefix-2=vi', 'prefix-2=wa', 'prefix-2=we', 'prefix-2=wh', 'prefix-2=wi', 'prefix-2=wo', 'prefix-2=ye', 'prefix-2=yo', 'prefix-3=&', \"prefix-3=''\", \"prefix-3='re\", \"prefix-3='s\", 'prefix-3=*', 'prefix-3=*-1', 'prefix-3=*-2', 'prefix-3=*-3', 'prefix-3=*-4', 'prefix-3=*-5', 'prefix-3=*-6', 'prefix-3=*-7', 'prefix-3=*-8', 'prefix-3=*?*', 'prefix-3=*IC', 'prefix-3=*T*', 'prefix-3=,', 'prefix-3=--', 'prefix-3=.', 'prefix-3=0', 'prefix-3=160', 'prefix-3=18', 'prefix-3=195', 'prefix-3=199', 'prefix-3=28', 'prefix-3=29', 'prefix-3=30', 'prefix-3=33', 'prefix-3=35', 'prefix-3=55', 'prefix-3=61', 'prefix-3=9.8', 'prefix-3=A', 'prefix-3=A.', 'prefix-3=Abo', 'prefix-3=Age', 'prefix-3=Agn', 'prefix-3=Alt', 'prefix-3=Amo', 'prefix-3=Are', 'prefix-3=Bos', 'prefix-3=Bri', 'prefix-3=Bro', 'prefix-3=But', 'prefix-3=By', 'prefix-3=Can', 'prefix-3=Co.', 'prefix-3=Col', 'prefix-3=Con', 'prefix-3=Cor', 'prefix-3=Dan', 'prefix-3=Dar', 'prefix-3=Dr.', 'prefix-3=Dut', 'prefix-3=Els', 'prefix-3=Eng', 'prefix-3=Env', 'prefix-3=Fie', 'prefix-3=Fou', 'prefix-3=Fro', 'prefix-3=Gol', 'prefix-3=Gro', 'prefix-3=Har', 'prefix-3=Hol', 'prefix-3=In', 'prefix-3=Inc', 'prefix-3=Ins', 'prefix-3=It', 'prefix-3=Jam', 'prefix-3=Jou', 'prefix-3=Jul', 'prefix-3=Ken', 'prefix-3=Loe', 'prefix-3=Lor', 'prefix-3=Mas', 'prefix-3=Med', 'prefix-3=Mic', 'prefix-3=Mor', 'prefix-3=Mos', 'prefix-3=Mr.', 'prefix-3=N.V', 'prefix-3=Nat', 'prefix-3=Nei', 'prefix-3=New', 'prefix-3=Nov', 'prefix-3=PLC', 'prefix-3=Phi', 'prefix-3=Pie', 'prefix-3=Pro', 'prefix-3=Rud', 'prefix-3=T.', 'prefix-3=Tal', 'prefix-3=The', 'prefix-3=Thi', 'prefix-3=U.S', 'prefix-3=Uni', 'prefix-3=Ver', 'prefix-3=Vin', 'prefix-3=Vos', 'prefix-3=We', 'prefix-3=Wes', 'prefix-3=Wor', 'prefix-3=Yor', 'prefix-3=``', 'prefix-3=a', 'prefix-3=abo', 'prefix-3=acc', 'prefix-3=ace', 'prefix-3=ago', 'prefix-3=all', 'prefix-3=alm', 'prefix-3=amo', 'prefix-3=amp', 'prefix-3=an', 'prefix-3=and', 'prefix-3=any', 'prefix-3=app', 'prefix-3=are', 'prefix-3=arg', 'prefix-3=as', 'prefix-3=asb', 'prefix-3=at', 'prefix-3=att', 'prefix-3=awa', 'prefix-3=ban', 'prefix-3=be', 'prefix-3=bea', 'prefix-3=bef', 'prefix-3=bil', 'prefix-3=bin', 'prefix-3=blu', 'prefix-3=boa', 'prefix-3=bod', 'prefix-3=bri', 'prefix-3=bui', 'prefix-3=bur', 'prefix-3=by', 'prefix-3=can', 'prefix-3=cau', 'prefix-3=cha', 'prefix-3=chr', 'prefix-3=cig', 'prefix-3=cla', 'prefix-3=clo', 'prefix-3=com', 'prefix-3=con', 'prefix-3=cot', 'prefix-3=cou', 'prefix-3=cro', 'prefix-3=cur', 'prefix-3=dea', 'prefix-3=dec', 'prefix-3=des', 'prefix-3=dia', 'prefix-3=die', 'prefix-3=dif', 'prefix-3=dir', 'prefix-3=dis', 'prefix-3=doe', 'prefix-3=dry', 'prefix-3=dum', 'prefix-3=dus', 'prefix-3=ear', 'prefix-3=eas', 'prefix-3=ent', 'prefix-3=eve', 'prefix-3=exh', 'prefix-3=exp', 'prefix-3=fac', 'prefix-3=fan', 'prefix-3=far', 'prefix-3=few', 'prefix-3=fib', 'prefix-3=fil', 'prefix-3=fin', 'prefix-3=fiv', 'prefix-3=for', 'prefix-3=fou', 'prefix-3=fro', 'prefix-3=gra', 'prefix-3=gro', 'prefix-3=has', 'prefix-3=hav', 'prefix-3=he', 'prefix-3=hea', 'prefix-3=hig', 'prefix-3=hug', 'prefix-3=hum', 'prefix-3=hun', 'prefix-3=imp', 'prefix-3=in', 'prefix-3=inc', 'prefix-3=ind', 'prefix-3=inf', 'prefix-3=int', 'prefix-3=is', 'prefix-3=it', 'prefix-3=its', 'prefix-3=joi', 'prefix-3=kin', 'prefix-3=lar', 'prefix-3=lat', 'prefix-3=led', 'prefix-3=lik', 'prefix-3=lun', 'prefix-3=mad', 'prefix-3=mak', 'prefix-3=mal', 'prefix-3=man', 'prefix-3=mat', 'prefix-3=mec', 'prefix-3=med', 'prefix-3=men', 'prefix-3=mes', 'prefix-3=mix', 'prefix-3=mod', 'prefix-3=mor', 'prefix-3=mos', \"prefix-3=n't\", 'prefix-3=nam', 'prefix-3=nat', 'prefix-3=nee', 'prefix-3=new', 'prefix-3=no', 'prefix-3=non', 'prefix-3=nor', 'prefix-3=now', 'prefix-3=num', 'prefix-3=of', 'prefix-3=old', 'prefix-3=on', 'prefix-3=onc', 'prefix-3=one', 'prefix-3=oth', 'prefix-3=our', 'prefix-3=out', 'prefix-3=ove', 'prefix-3=own', 'prefix-3=pap', 'prefix-3=par', 'prefix-3=pat', 'prefix-3=per', 'prefix-3=pla', 'prefix-3=pou', 'prefix-3=pre', 'prefix-3=pro', 'prefix-3=pub', 'prefix-3=que', 'prefix-3=rat', 'prefix-3=rec', 'prefix-3=reg', 'prefix-3=rej', 'prefix-3=rem', 'prefix-3=rep', 'prefix-3=res', 'prefix-3=ris', 'prefix-3=sac', 'prefix-3=sai', 'prefix-3=sch', 'prefix-3=sho', 'prefix-3=smo', 'prefix-3=sol', 'prefix-3=som', 'prefix-3=spo', 'prefix-3=sta', 'prefix-3=sto', 'prefix-3=str', 'prefix-3=stu', 'prefix-3=sub', 'prefix-3=suc', 'prefix-3=sup', 'prefix-3=sur', 'prefix-3=sym', 'prefix-3=tal', 'prefix-3=tea', 'prefix-3=tha', 'prefix-3=the', 'prefix-3=thi', 'prefix-3=tho', 'prefix-3=thr', 'prefix-3=tim', 'prefix-3=to', 'prefix-3=tod', 'prefix-3=too', 'prefix-3=tot', 'prefix-3=typ', 'prefix-3=und', 'prefix-3=uni', 'prefix-3=unu', 'prefix-3=up', 'prefix-3=us', 'prefix-3=use', 'prefix-3=usi', 'prefix-3=ven', 'prefix-3=ver', 'prefix-3=vic', 'prefix-3=vir', 'prefix-3=was', 'prefix-3=wer', 'prefix-3=whe', 'prefix-3=whi', 'prefix-3=who', 'prefix-3=wil', 'prefix-3=wit', 'prefix-3=wor', 'prefix-3=yea', 'prefix-3=you', 'prev_word=', 'prev_word=&', \"prev_word=''\", \"prev_word='re\", \"prev_word='s\", 'prev_word=*', 'prev_word=*-1', 'prev_word=*-2', 'prev_word=*-3', 'prev_word=*-4', 'prev_word=*-5', 'prev_word=*-6', 'prev_word=*-7', 'prev_word=*-8', 'prev_word=*?*', 'prev_word=*ICH*-1', 'prev_word=*ICH*-2', 'prev_word=*T*-1', 'prev_word=*T*-2', 'prev_word=*T*-3', 'prev_word=*T*-4', 'prev_word=*T*-5', 'prev_word=*T*-6', 'prev_word=*T*-7', 'prev_word=*T*-8', 'prev_word=,', 'prev_word=--', 'prev_word=.', 'prev_word=0', 'prev_word=160', 'prev_word=18', 'prev_word=1950s', 'prev_word=1953', 'prev_word=1955', 'prev_word=1956', 'prev_word=1997', 'prev_word=28', 'prev_word=29', 'prev_word=30', 'prev_word=33', 'prev_word=35', 'prev_word=55', 'prev_word=61', 'prev_word=9.8', 'prev_word=A', 'prev_word=A.', 'prev_word=About', 'prev_word=Agency', 'prev_word=Agnew', 'prev_word=Although', 'prev_word=Among', 'prev_word=Areas', 'prev_word=Boston', 'prev_word=British', 'prev_word=Brooke', 'prev_word=But', 'prev_word=By', 'prev_word=Cancer', 'prev_word=Co.', 'prev_word=College', 'prev_word=Consolidated', 'prev_word=Corp.', 'prev_word=Dana-Farber', 'prev_word=Darrell', 'prev_word=Dr.', 'prev_word=Dutch', 'prev_word=Elsevier', 'prev_word=England', 'prev_word=Environmental', 'prev_word=Fields', 'prev_word=Four', 'prev_word=From', 'prev_word=Gold', 'prev_word=Groton', 'prev_word=Harvard', 'prev_word=Hollingsworth', 'prev_word=In', 'prev_word=Inc.', 'prev_word=Institute', 'prev_word=It', 'prev_word=James', 'prev_word=Journal', 'prev_word=July', 'prev_word=Kent', 'prev_word=Loews', 'prev_word=Lorillard', 'prev_word=Mass.', 'prev_word=Medicine', 'prev_word=Micronite', 'prev_word=More', 'prev_word=Mossman', 'prev_word=Mr.', 'prev_word=N.V.', 'prev_word=National', 'prev_word=Neither', 'prev_word=New', 'prev_word=Nov.', 'prev_word=PLC', 'prev_word=Phillips', 'prev_word=Pierre', 'prev_word=Protection', 'prev_word=Rudolph', 'prev_word=T.', 'prev_word=Talcott', 'prev_word=The', 'prev_word=There', 'prev_word=This', 'prev_word=U.S.', 'prev_word=University', 'prev_word=Vermont', 'prev_word=Vinken', 'prev_word=Vose', 'prev_word=We', 'prev_word=West', 'prev_word=Western', 'prev_word=Workers', 'prev_word=York-based', 'prev_word=``', 'prev_word=a', 'prev_word=about', 'prev_word=according', 'prev_word=acetate', 'prev_word=ago', 'prev_word=all', 'prev_word=almost', 'prev_word=among', 'prev_word=amounts', 'prev_word=amphobiles', 'prev_word=an', 'prev_word=and', 'prev_word=any', 'prev_word=anyone', 'prev_word=appear', 'prev_word=appears', 'prev_word=are', 'prev_word=area', 'prev_word=argue', 'prev_word=as', 'prev_word=asbestos', 'prev_word=asbestos-related', 'prev_word=asbestosis', 'prev_word=at', 'prev_word=attention', 'prev_word=aware', 'prev_word=ban', 'prev_word=be', 'prev_word=bearing', 'prev_word=before', 'prev_word=billion', 'prev_word=bin', 'prev_word=blue', 'prev_word=board', 'prev_word=body', 'prev_word=brief', 'prev_word=bring', 'prev_word=buildings', 'prev_word=burlap', 'prev_word=by', 'prev_word=cancer', 'prev_word=cancer-causing', 'prev_word=caused', 'prev_word=causing', 'prev_word=chairman', 'prev_word=chrysotile', 'prev_word=cigarette', 'prev_word=cigarettes', 'prev_word=class', 'prev_word=classified', 'prev_word=closely', 'prev_word=clouds', 'prev_word=common', 'prev_word=company', 'prev_word=conglomerate', 'prev_word=contract', 'prev_word=contracted', 'prev_word=cotton', 'prev_word=countries', 'prev_word=crocidolite', 'prev_word=curly', 'prev_word=deaths', 'prev_word=decades', 'prev_word=described', 'prev_word=diagnosed', 'prev_word=died', 'prev_word=different', 'prev_word=director', 'prev_word=diseases', 'prev_word=does', 'prev_word=dry', 'prev_word=dumped', 'prev_word=dust', 'prev_word=dusty', 'prev_word=early', 'prev_word=easily', 'prev_word=enters', 'prev_word=even', 'prev_word=events', 'prev_word=exhaust', 'prev_word=expected', 'prev_word=explained', 'prev_word=exposed', 'prev_word=exposures', 'prev_word=factory', 'prev_word=fans', 'prev_word=far', 'prev_word=few', 'prev_word=fiber', 'prev_word=fibers', 'prev_word=filter', 'prev_word=filters', 'prev_word=finding', 'prev_word=findings', 'prev_word=five', 'prev_word=for', 'prev_word=force', 'prev_word=form', 'prev_word=former', 'prev_word=forum', 'prev_word=found', 'prev_word=from', 'prev_word=gradual', 'prev_word=group', 'prev_word=has', 'prev_word=have', 'prev_word=having', 'prev_word=he', 'prev_word=heard', 'prev_word=high', 'prev_word=higher', 'prev_word=highest', 'prev_word=huge', 'prev_word=human', 'prev_word=hung', 'prev_word=imported', 'prev_word=imposed', 'prev_word=in', 'prev_word=including', 'prev_word=industrial', 'prev_word=industrialized', 'prev_word=information', 'prev_word=into', 'prev_word=is', 'prev_word=it', 'prev_word=its', 'prev_word=join', 'prev_word=kind', 'prev_word=large', 'prev_word=later', 'prev_word=latest', 'prev_word=led', 'prev_word=likely', 'prev_word=lung', 'prev_word=lungs', 'prev_word=made', 'prev_word=make', 'prev_word=makes', 'prev_word=making', 'prev_word=malignant', 'prev_word=managers', 'prev_word=material', 'prev_word=mechanically', 'prev_word=medical', 'prev_word=men', 'prev_word=mesothelioma', 'prev_word=mixed', 'prev_word=modest', 'prev_word=morbidity', 'prev_word=more', 'prev_word=most', \"prev_word=n't\", 'prev_word=named', 'prev_word=nations', 'prev_word=needle-like', 'prev_word=new', 'prev_word=no', 'prev_word=nonexecutive', 'prev_word=nor', 'prev_word=now', 'prev_word=number', 'prev_word=of', 'prev_word=old', 'prev_word=on', 'prev_word=once', 'prev_word=one', 'prev_word=other', 'prev_word=our', 'prev_word=outlawed', 'prev_word=over', 'prev_word=owned', 'prev_word=paper', 'prev_word=particularly', 'prev_word=parts', 'prev_word=pathlogy', 'prev_word=percentage', 'prev_word=place', 'prev_word=plant', 'prev_word=poured', 'prev_word=preliminary', 'prev_word=president', 'prev_word=probably', 'prev_word=problem', 'prev_word=process', 'prev_word=products', 'prev_word=professor', 'prev_word=properties', 'prev_word=publishing', 'prev_word=question', 'prev_word=questionable', 'prev_word=rate', 'prev_word=recently', 'prev_word=recognize', 'prev_word=regulate', 'prev_word=regulation', 'prev_word=rejected', 'prev_word=remaining', 'prev_word=replaced', 'prev_word=reported', 'prev_word=research', 'prev_word=researchers', 'prev_word=resilient', 'prev_word=resources', 'prev_word=results', 'prev_word=risk', 'prev_word=sacks', 'prev_word=said', 'prev_word=schools', 'prev_word=should', 'prev_word=show', 'prev_word=smokers', 'prev_word=smooth', 'prev_word=sold', 'prev_word=some', 'prev_word=spokeswoman', 'prev_word=spokewoman', 'prev_word=standard', 'prev_word=stopped', 'prev_word=story', 'prev_word=striking', 'prev_word=stringently', 'prev_word=studied', 'prev_word=study', 'prev_word=substance', 'prev_word=such', 'prev_word=support', 'prev_word=surviving', 'prev_word=symptoms', 'prev_word=talking', 'prev_word=team', 'prev_word=than', 'prev_word=that', 'prev_word=the', 'prev_word=these', 'prev_word=this', 'prev_word=those', 'prev_word=though', 'prev_word=three', 'prev_word=times', 'prev_word=to', 'prev_word=today', 'prev_word=took', 'prev_word=total', 'prev_word=type', 'prev_word=under', 'prev_word=unit', 'prev_word=unusually', 'prev_word=up', 'prev_word=us', 'prev_word=used', 'prev_word=useful', 'prev_word=users', 'prev_word=uses', 'prev_word=using', 'prev_word=ventilated', 'prev_word=very', 'prev_word=vice', 'prev_word=virtually', 'prev_word=was', 'prev_word=were', 'prev_word=where', 'prev_word=whether', 'prev_word=which', 'prev_word=who', 'prev_word=will', 'prev_word=with', 'prev_word=work', 'prev_word=worked', 'prev_word=workers', 'prev_word=year', 'prev_word=years', 'prev_word=you', 'suffix-1=&', \"suffix-1='\", 'suffix-1=*', 'suffix-1=,', 'suffix-1=-', 'suffix-1=.', 'suffix-1=0', 'suffix-1=1', 'suffix-1=2', 'suffix-1=3', 'suffix-1=4', 'suffix-1=5', 'suffix-1=6', 'suffix-1=7', 'suffix-1=8', 'suffix-1=9', 'suffix-1=A', 'suffix-1=C', 'suffix-1=`', 'suffix-1=a', 'suffix-1=d', 'suffix-1=e', 'suffix-1=f', 'suffix-1=g', 'suffix-1=h', 'suffix-1=k', 'suffix-1=l', 'suffix-1=m', 'suffix-1=n', 'suffix-1=o', 'suffix-1=p', 'suffix-1=r', 'suffix-1=s', 'suffix-1=t', 'suffix-1=u', 'suffix-1=w', 'suffix-1=y', 'suffix-2=&', \"suffix-2=''\", \"suffix-2='s\", \"suffix-2='t\", 'suffix-2=*', 'suffix-2=,', 'suffix-2=--', 'suffix-2=-1', 'suffix-2=-2', 'suffix-2=-3', 'suffix-2=-4', 'suffix-2=-5', 'suffix-2=-6', 'suffix-2=-7', 'suffix-2=-8', 'suffix-2=.', 'suffix-2=.8', 'suffix-2=0', 'suffix-2=0s', 'suffix-2=18', 'suffix-2=28', 'suffix-2=29', 'suffix-2=30', 'suffix-2=33', 'suffix-2=35', 'suffix-2=53', 'suffix-2=55', 'suffix-2=56', 'suffix-2=60', 'suffix-2=61', 'suffix-2=97', 'suffix-2=?*', 'suffix-2=A', 'suffix-2=A.', 'suffix-2=By', 'suffix-2=In', 'suffix-2=It', 'suffix-2=LC', 'suffix-2=S.', 'suffix-2=T.', 'suffix-2=V.', 'suffix-2=We', 'suffix-2=``', 'suffix-2=a', 'suffix-2=al', 'suffix-2=am', 'suffix-2=an', 'suffix-2=ap', 'suffix-2=ar', 'suffix-2=as', 'suffix-2=at', 'suffix-2=ay', 'suffix-2=be', 'suffix-2=by', 'suffix-2=c.', 'suffix-2=ce', 'suffix-2=ch', 'suffix-2=ct', 'suffix-2=cy', 'suffix-2=de', 'suffix-2=ds', 'suffix-2=dy', 'suffix-2=ea', 'suffix-2=ed', 'suffix-2=ee', 'suffix-2=ef', 'suffix-2=em', 'suffix-2=en', 'suffix-2=er', 'suffix-2=es', 'suffix-2=ew', 'suffix-2=ge', 'suffix-2=gh', 'suffix-2=go', 'suffix-2=gs', 'suffix-2=gy', 'suffix-2=he', 'suffix-2=ho', 'suffix-2=hs', 'suffix-2=id', 'suffix-2=in', 'suffix-2=is', 'suffix-2=it', 'suffix-2=ke', 'suffix-2=ks', 'suffix-2=ld', 'suffix-2=le', 'suffix-2=ll', 'suffix-2=ls', 'suffix-2=ly', 'suffix-2=ma', 'suffix-2=me', 'suffix-2=ms', 'suffix-2=nd', 'suffix-2=ne', 'suffix-2=ng', 'suffix-2=no', 'suffix-2=ns', 'suffix-2=nt', 'suffix-2=ny', 'suffix-2=o.', 'suffix-2=of', 'suffix-2=ok', 'suffix-2=om', 'suffix-2=on', 'suffix-2=or', 'suffix-2=os', 'suffix-2=ou', 'suffix-2=ow', 'suffix-2=p.', 'suffix-2=pe', 'suffix-2=ph', 'suffix-2=ps', 'suffix-2=r.', 'suffix-2=rd', 'suffix-2=re', 'suffix-2=rk', 'suffix-2=rm', 'suffix-2=rn', 'suffix-2=rs', 'suffix-2=rt', 'suffix-2=ry', 'suffix-2=s.', 'suffix-2=se', 'suffix-2=sh', 'suffix-2=sk', 'suffix-2=ss', 'suffix-2=st', 'suffix-2=te', 'suffix-2=th', 'suffix-2=to', 'suffix-2=ts', 'suffix-2=tt', 'suffix-2=ty', 'suffix-2=ue', 'suffix-2=ul', 'suffix-2=um', 'suffix-2=up', 'suffix-2=ur', 'suffix-2=us', 'suffix-2=ut', 'suffix-2=v.', 'suffix-2=ve', 'suffix-2=ws', 'suffix-2=ze', 'suffix-3=&', \"suffix-3=''\", \"suffix-3='re\", \"suffix-3='s\", 'suffix-3=*', 'suffix-3=*-1', 'suffix-3=*-2', 'suffix-3=*-3', 'suffix-3=*-4', 'suffix-3=*-5', 'suffix-3=*-6', 'suffix-3=*-7', 'suffix-3=*-8', 'suffix-3=*?*', 'suffix-3=,', 'suffix-3=--', 'suffix-3=.', 'suffix-3=.S.', 'suffix-3=.V.', 'suffix-3=0', 'suffix-3=160', 'suffix-3=18', 'suffix-3=28', 'suffix-3=29', 'suffix-3=30', 'suffix-3=33', 'suffix-3=35', 'suffix-3=50s', 'suffix-3=55', 'suffix-3=61', 'suffix-3=9.8', 'suffix-3=953', 'suffix-3=955', 'suffix-3=956', 'suffix-3=997', 'suffix-3=A', 'suffix-3=A.', 'suffix-3=But', 'suffix-3=By', 'suffix-3=Co.', 'suffix-3=Dr.', 'suffix-3=In', 'suffix-3=It', 'suffix-3=Mr.', 'suffix-3=New', 'suffix-3=PLC', 'suffix-3=T.', 'suffix-3=The', 'suffix-3=We', 'suffix-3=``', 'suffix-3=a', 'suffix-3=ace', 'suffix-3=act', 'suffix-3=ade', 'suffix-3=age', 'suffix-3=ago', 'suffix-3=aid', 'suffix-3=ake', 'suffix-3=all', 'suffix-3=an', 'suffix-3=and', 'suffix-3=ans', 'suffix-3=ant', 'suffix-3=any', 'suffix-3=ard', 'suffix-3=are', 'suffix-3=ars', 'suffix-3=ary', 'suffix-3=as', 'suffix-3=ass', 'suffix-3=at', 'suffix-3=ate', 'suffix-3=ave', 'suffix-3=ban', 'suffix-3=be', 'suffix-3=bed', 'suffix-3=ber', 'suffix-3=bin', 'suffix-3=ble', 'suffix-3=bly', 'suffix-3=by', 'suffix-3=cal', 'suffix-3=ced', 'suffix-3=cer', 'suffix-3=ces', 'suffix-3=cks', 'suffix-3=cts', 'suffix-3=day', 'suffix-3=der', 'suffix-3=des', 'suffix-3=dry', 'suffix-3=eam', 'suffix-3=ear', 'suffix-3=eas', 'suffix-3=ege', 'suffix-3=ell', 'suffix-3=ely', 'suffix-3=ent', 'suffix-3=ere', 'suffix-3=ern', 'suffix-3=ers', 'suffix-3=ery', 'suffix-3=ese', 'suffix-3=ess', 'suffix-3=est', 'suffix-3=ews', 'suffix-3=far', 'suffix-3=few', 'suffix-3=for', 'suffix-3=ful', 'suffix-3=gue', 'suffix-3=han', 'suffix-3=has', 'suffix-3=hat', 'suffix-3=he', 'suffix-3=her', 'suffix-3=his', 'suffix-3=how', 'suffix-3=ial', 'suffix-3=ice', 'suffix-3=ich', 'suffix-3=ied', 'suffix-3=ief', 'suffix-3=ier', 'suffix-3=ies', 'suffix-3=igh', 'suffix-3=ike', 'suffix-3=ile', 'suffix-3=ill', 'suffix-3=ily', 'suffix-3=in', 'suffix-3=ind', 'suffix-3=ine', 'suffix-3=ing', 'suffix-3=ion', 'suffix-3=ips', 'suffix-3=is', 'suffix-3=ish', 'suffix-3=isk', 'suffix-3=it', 'suffix-3=ite', 'suffix-3=ith', 'suffix-3=its', 'suffix-3=ity', 'suffix-3=ive', 'suffix-3=ize', 'suffix-3=ked', 'suffix-3=ken', 'suffix-3=kes', 'suffix-3=lap', 'suffix-3=lds', 'suffix-3=led', 'suffix-3=lem', 'suffix-3=les', 'suffix-3=lly', 'suffix-3=lph', 'suffix-3=lts', 'suffix-3=lue', 'suffix-3=man', 'suffix-3=med', 'suffix-3=men', 'suffix-3=mer', 'suffix-3=mes', 'suffix-3=mon', \"suffix-3=n't\", 'suffix-3=nal', 'suffix-3=nc.', 'suffix-3=nce', 'suffix-3=ncy', 'suffix-3=ned', 'suffix-3=new', 'suffix-3=ngs', 'suffix-3=nit', 'suffix-3=no', 'suffix-3=nor', 'suffix-3=now', 'suffix-3=nto', 'suffix-3=nts', 'suffix-3=ody', 'suffix-3=oes', 'suffix-3=of', 'suffix-3=ogy', 'suffix-3=oin', 'suffix-3=oke', 'suffix-3=old', 'suffix-3=ols', 'suffix-3=oma', 'suffix-3=ome', 'suffix-3=oms', 'suffix-3=on', 'suffix-3=one', 'suffix-3=ong', 'suffix-3=ons', 'suffix-3=ont', 'suffix-3=ook', 'suffix-3=ore', 'suffix-3=ork', 'suffix-3=orm', 'suffix-3=ort', 'suffix-3=ory', 'suffix-3=ose', 'suffix-3=ost', 'suffix-3=oth', 'suffix-3=ott', 'suffix-3=oup', 'suffix-3=our', 'suffix-3=out', 'suffix-3=ov.', 'suffix-3=ped', 'suffix-3=per', 'suffix-3=rce', 'suffix-3=rch', 'suffix-3=rea', 'suffix-3=red', 'suffix-3=ree', 'suffix-3=res', 'suffix-3=rge', 'suffix-3=rly', 'suffix-3=rom', 'suffix-3=rp.', 'suffix-3=rre', 'suffix-3=rth', 'suffix-3=rts', 'suffix-3=rum', 'suffix-3=sed', 'suffix-3=ses', 'suffix-3=sis', 'suffix-3=sor', 'suffix-3=ss.', 'suffix-3=sty', 'suffix-3=tal', 'suffix-3=tch', 'suffix-3=ted', 'suffix-3=ter', 'suffix-3=tes', 'suffix-3=the', 'suffix-3=ths', 'suffix-3=tly', 'suffix-3=to', 'suffix-3=ton', 'suffix-3=tor', 'suffix-3=tos', 'suffix-3=tte', 'suffix-3=ual', 'suffix-3=uch', 'suffix-3=uds', 'suffix-3=udy', 'suffix-3=uge', 'suffix-3=ugh', 'suffix-3=uld', 'suffix-3=uly', 'suffix-3=und', 'suffix-3=ung', 'suffix-3=up', 'suffix-3=us', 'suffix-3=ust', 'suffix-3=ute', 'suffix-3=ven', 'suffix-3=ver', 'suffix-3=was', 'suffix-3=wed', 'suffix-3=who', 'suffix-3=xed', 'suffix-3=you', 'suffix-3=ype', 'suffix-3=zed', 'word=&', \"word=''\", \"word='re\", \"word='s\", 'word=*', 'word=*-1', 'word=*-2', 'word=*-3', 'word=*-4', 'word=*-5', 'word=*-6', 'word=*-7', 'word=*-8', 'word=*?*', 'word=*ICH*-1', 'word=*ICH*-2', 'word=*T*-1', 'word=*T*-2', 'word=*T*-3', 'word=*T*-4', 'word=*T*-5', 'word=*T*-6', 'word=*T*-7', 'word=*T*-8', 'word=,', 'word=--', 'word=.', 'word=0', 'word=160', 'word=18', 'word=1950s', 'word=1953', 'word=1955', 'word=1956', 'word=1997', 'word=28', 'word=29', 'word=30', 'word=33', 'word=35', 'word=55', 'word=61', 'word=9.8', 'word=A', 'word=A.', 'word=About', 'word=Agency', 'word=Agnew', 'word=Although', 'word=Among', 'word=Areas', 'word=Boston', 'word=British', 'word=Brooke', 'word=But', 'word=By', 'word=Cancer', 'word=Co.', 'word=College', 'word=Consolidated', 'word=Corp.', 'word=Dana-Farber', 'word=Darrell', 'word=Dr.', 'word=Dutch', 'word=Elsevier', 'word=England', 'word=Environmental', 'word=Fields', 'word=Four', 'word=From', 'word=Gold', 'word=Groton', 'word=Harvard', 'word=Hollingsworth', 'word=In', 'word=Inc.', 'word=Institute', 'word=It', 'word=James', 'word=Journal', 'word=July', 'word=Kent', 'word=Loews', 'word=Lorillard', 'word=Mass.', 'word=Medicine', 'word=Micronite', 'word=More', 'word=Mossman', 'word=Mr.', 'word=N.V.', 'word=National', 'word=Neither', 'word=New', 'word=Nov.', 'word=PLC', 'word=Phillips', 'word=Pierre', 'word=Protection', 'word=Rudolph', 'word=T.', 'word=Talcott', 'word=The', 'word=There', 'word=This', 'word=U.S.', 'word=University', 'word=Vermont', 'word=Vinken', 'word=Vose', 'word=We', 'word=West', 'word=Western', 'word=Workers', 'word=York-based', 'word=``', 'word=a', 'word=about', 'word=according', 'word=acetate', 'word=ago', 'word=all', 'word=almost', 'word=among', 'word=amounts', 'word=amphobiles', 'word=an', 'word=and', 'word=any', 'word=anyone', 'word=appear', 'word=appears', 'word=are', 'word=area', 'word=argue', 'word=as', 'word=asbestos', 'word=asbestos-related', 'word=asbestosis', 'word=at', 'word=attention', 'word=aware', 'word=ban', 'word=be', 'word=bearing', 'word=before', 'word=billion', 'word=bin', 'word=blue', 'word=board', 'word=body', 'word=brief', 'word=bring', 'word=buildings', 'word=burlap', 'word=by', 'word=cancer', 'word=cancer-causing', 'word=caused', 'word=causing', 'word=chairman', 'word=chrysotile', 'word=cigarette', 'word=cigarettes', 'word=class', 'word=classified', 'word=closely', 'word=clouds', 'word=common', 'word=company', 'word=conglomerate', 'word=contract', 'word=contracted', 'word=cotton', 'word=countries', 'word=crocidolite', 'word=curly', 'word=deaths', 'word=decades', 'word=described', 'word=diagnosed', 'word=died', 'word=different', 'word=director', 'word=diseases', 'word=does', 'word=dry', 'word=dumped', 'word=dust', 'word=dusty', 'word=early', 'word=easily', 'word=enters', 'word=even', 'word=events', 'word=exhaust', 'word=expected', 'word=explained', 'word=exposed', 'word=exposures', 'word=factory', 'word=fans', 'word=far', 'word=few', 'word=fiber', 'word=fibers', 'word=filter', 'word=filters', 'word=finding', 'word=findings', 'word=five', 'word=for', 'word=force', 'word=form', 'word=former', 'word=forum', 'word=found', 'word=from', 'word=gradual', 'word=group', 'word=has', 'word=have', 'word=having', 'word=he', 'word=heard', 'word=high', 'word=higher', 'word=highest', 'word=huge', 'word=human', 'word=hung', 'word=imported', 'word=imposed', 'word=in', 'word=including', 'word=industrial', 'word=industrialized', 'word=information', 'word=into', 'word=is', 'word=it', 'word=its', 'word=join', 'word=kind', 'word=large', 'word=later', 'word=latest', 'word=led', 'word=likely', 'word=lung', 'word=lungs', 'word=made', 'word=make', 'word=makes', 'word=making', 'word=malignant', 'word=managers', 'word=material', 'word=mechanically', 'word=medical', 'word=men', 'word=mesothelioma', 'word=mixed', 'word=modest', 'word=morbidity', 'word=more', 'word=most', \"word=n't\", 'word=named', 'word=nations', 'word=needle-like', 'word=new', 'word=no', 'word=nonexecutive', 'word=nor', 'word=now', 'word=number', 'word=of', 'word=old', 'word=on', 'word=once', 'word=one', 'word=other', 'word=our', 'word=outlawed', 'word=over', 'word=owned', 'word=paper', 'word=particularly', 'word=parts', 'word=pathlogy', 'word=percentage', 'word=place', 'word=plant', 'word=poured', 'word=preliminary', 'word=president', 'word=probably', 'word=problem', 'word=process', 'word=products', 'word=professor', 'word=properties', 'word=publishing', 'word=question', 'word=questionable', 'word=rate', 'word=recently', 'word=recognize', 'word=regulate', 'word=regulation', 'word=rejected', 'word=remaining', 'word=replaced', 'word=reported', 'word=research', 'word=researchers', 'word=resilient', 'word=resources', 'word=results', 'word=risk', 'word=sacks', 'word=said', 'word=schools', 'word=should', 'word=show', 'word=smokers', 'word=smooth', 'word=sold', 'word=some', 'word=spokeswoman', 'word=spokewoman', 'word=standard', 'word=stopped', 'word=story', 'word=striking', 'word=stringently', 'word=studied', 'word=study', 'word=substance', 'word=such', 'word=support', 'word=surviving', 'word=symptoms', 'word=talking', 'word=team', 'word=than', 'word=that', 'word=the', 'word=these', 'word=this', 'word=those', 'word=though', 'word=three', 'word=times', 'word=to', 'word=today', 'word=took', 'word=total', 'word=type', 'word=under', 'word=unit', 'word=unusually', 'word=up', 'word=us', 'word=used', 'word=useful', 'word=users', 'word=uses', 'word=using', 'word=ventilated', 'word=very', 'word=vice', 'word=virtually', 'word=was', 'word=were', 'word=where', 'word=whether', 'word=which', 'word=who', 'word=will', 'word=with', 'word=work', 'word=worked', 'word=workers', 'word=year', 'word=years', 'word=you']\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "# Flatten your list of sentence features into a single list of token features\n",
    "# This ignores the sentence structure and creates a single list of the tokens of all sentence selected\n",
    "tokens_flat = list(chain.from_iterable(penn_train_feature_sequences[:3])) ## selecting three sentence\n",
    "\n",
    "vec = DictVectorizer(sparse=True)\n",
    "penn_vec = vec.fit_transform(tokens_flat)  # Sparse matrix of shape (n_tokens, n_features)\n",
    "\n",
    "print('We now have a list of tokens for the selection of sentences from the Penn treebank, each represented by a long sparse vector.')\n",
    "print('Shape for the total set of tokens', penn_vec.shape)\n",
    "# Dense vector for easier viewing\n",
    "for sentence_vec in penn_vec:    \n",
    "    # Dense vector for easier viewing\n",
    "    for token in sentence_vec:\n",
    "        with np.printoptions(threshold=sys.maxsize):\n",
    "            print(token.toarray())\n",
    "    break\n",
    "    \n",
    "# Feature names (column names of the vector)\n",
    "print(list(vec.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fc66d-609d-4f06-ac00-84cf58b3a56b",
   "metadata": {},
   "source": [
    "Each token from the first three sentences in the Penn Treebank is represented by the same features but the number of values increased a lot. From the first three sentences it extracted a flat list of 840 tokens, which resulted in a 1794 dimensions, each representing a possible feature-value combination.\n",
    "\n",
    "This is a lot more than the 57 feature-value combinations or dimensions that we got for our \"Fruit flies\" sentence. This is because the open-values grow with each sentence. Try to include more sentences than three from the Penn Treebank and see how the dimensions explode.\n",
    "\n",
    "With 20 sentences, we get a shape of: (5986, 6444), so more than three times the number of dimensions. At some point, the dimensions will slow down growing as the representations preeat more and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdecd23-e441-4f7c-9e85-fcd3ace3b436",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81472b-d486-4def-a681-ceffb9a9a682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
