{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to spaCy\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "Whereas NLTK contains separate modules that need to be called individually, we now look at another toolkit that hides these steps in one processing module. Basically, the toolkit takes the plain text as input, applies a whole series of NLP modules to the text and outputs a complex object with the output of these modules stacked on top of the elements of the text.\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a rather complete NLP pipeline (the output of one module feeds to the input of the next): it takes a raw document and performs tokenization, POS-tagging, stop word recognition, morphological analysis, lemmatization, sentence splitting, dependency parsing and Named Entity Recognition (NER), among others. It also supports similarity prediction, but that is outside of the scope of this notebook. The advantage of spaCy is that it is really fast, and it has a reasonable accuracy. In addition, it currently supports multiple languages: https://spacy.io/models.\n",
    "\n",
    "So whereas in NLTK you built your own pipeline, spaCy is for lazy people and can do it all for you. That is not completely correct as you can build your own pipeline also in spaCy but that is not so trivial and most people are lazy.\n",
    "\n",
    "In this notebook, we will show you the basic usage. If you want to learn more, please visit spaCy's website; it has extensive documentation and provides excellent user guides. \n",
    "\n",
    "**At the end of this notebook, you will be able to extract the output from spaCy for the following NLP tasks**:\n",
    "* **Sentence splitting**: attribute **sents** of a `Doc` (of type *spacy.tokens.doc.Doc*)\n",
    "* **Tokenization**: `Doc` contains a sequence of `Token` objects (of type *spacy.tokens.token.Token*)\n",
    "* **Part-of-speech (POS) tagging**: attributes **pos_** and **tag_** of `Token`\n",
    "* **Stop words recognition** attribute **is_stop** of `Token`\n",
    "* **Stemming and lemmatization**: attribute **lemma_** of `Token`\n",
    "* **Constituency/dependency parsing:** attributes **dep_** and **head**\n",
    "* **Named Entity Recognition (NER):** attribute **ents** (of type *spacy.tokens.span.Span*) of `Doc` (of type *spacy.tokens.doc.Doc*). \n",
    "\n",
    "In addition, you will be able to use spaCy to visualize the output for each NLP task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and loading spaCy\n",
    "\n",
    "To install spaCy, check out the instructions [here](https://spacy.io/usage). It explains exactly how to install spaCy for your operating system, package manager and desired languages. Simply run the suggested commands in your terminal ([Anaconda Prompt](https://docs.anaconda.com/anaconda/user-guide/getting-started/) or cmd). Alternatively, you can probably also just run the following cells in this notebook:\n",
    "\n",
    "**Tip**: comment out the next two commands after using them. You can comment out commands by putting a \"#\" in front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/piek/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - spacy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.10.3               |   py37hf985489_0         3.0 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.0 MB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              pkgs/main::conda-4.10.3-py37hecd8cb5_0 --> conda-forge::conda-4.10.3-py37hf985489_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? \n",
      "\n",
      "Downloading and Extracting Packages\n",
      "conda-4.10.3         | 3.0 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are no errors, you installed the software on your local machine. Congratulations!  But this is not enough. You also need to have the language resources to feed the software: trained models, grammars, lexicons, etc. spaCy comes with language resources for many different languages and this is growing. Maybe one day you may even contribute a module for your own language.\n",
    "\n",
    "In this notebook, we are going to download the English language resources. The standard donwload command from the command line is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/Users/piek/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the messages, you see that there is a download and install operation followed by a linking operation. There may be errors with the linking, which we try to solve below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first load spaCy in the notebook and check if we can load the English language resources. We import the spaCy module and load the English tokenizer, tagger, parser, NER, and word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp= spacy.load('en') # other languages: de, es, pt, fr, it, nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no message, you can assume that everything went well and you can skip the next part on errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to check the models that are installed for spaCy for their compatibility with the spaCy version that you are running.\n",
    "The next cell shows you how to do this on the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Installed models (spaCy v2.1.8) ======================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/spacy\u001b[0m\n",
      "\n",
      "TYPE      NAME              MODEL             VERSION                            \n",
      "package   nl-core-news-sm   nl_core_news_sm   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "package   en-core-web-sm    en_core_web_sm    \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "link      en                en_core_web_sm    \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, there are two models and a link, all compatible with my spaCy. If there is no model listed, something went wrong with downloading. If spaCy lists a compatibility problem, it suggests how to fix it. Below are some more possible fixes. Note that these toolkits are improved rapidly and a new release may change things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible error when downloading language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get a variation of the following error:\n",
    "```\n",
    "OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there is a problem with linking the language model of spaCy. You can try to load it in the following way instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if this works, you might still want to invest some time to make sure that the linking was succesful, i.e., that you can load spaCy with spacy.load('en'). Here is some more information on how to fix this.\n",
    "\n",
    "**Troubleshooting (optional)**\n",
    "\n",
    "*Cause*: Anaconda prompt does not have enough priviliges to execute the linking part of `python -m spacy download en`. The same is true for any other `python -m spacy [...]` command.\n",
    "\n",
    "*Solution:* \n",
    "\n",
    "1. Create the link manually\n",
    "\n",
    "The prompt should display something along the lines of:\n",
    "\n",
    "```\n",
    "<Data Downloaded>\n",
    "You do not have sufficient privilege to perform this operation.\n",
    "\n",
    "    Linking successful\n",
    "    <Anaconda dir>\\lib\\site-packages\\en_core_web_sm -->\n",
    "    <Anaconda dir>\\lib\\site-packages\\spacy\\data\\en\n",
    "\n",
    "    You can now load the model via spacy.load('en')\n",
    "\n",
    "Use the following command:\n",
    "\n",
    "mklink /D <Anaconda>\\lib\\site-packages\\spacy\\data\\en <Anaconda>\\lib\\site-packages\\en_core_web_sm\n",
    "\n",
    " Note that the target is pointing to the link, not the other way around. The syntax of the arguments is 'target' followed by 'link'\n",
    " \n",
    "```\n",
    "\n",
    "2. Give Anaconda Permissions to create link. Using \"runas ... python -m spacy ...\" may not suffice\n",
    "\n",
    "3. More details: https://github.com/explosion/spaCy/issues/1283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If none of this works (can happen on Windows)** You might want to install the model manually from SpaCy's GitHub through pip:\n",
    "\n",
    "```\n",
    "    pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still having problems? Come to the VU feedback sessions or schedule a zoom meeting with the teachers to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spaCy\n",
    "\n",
    "If you succesfully loaded the English model (or another language) with the above command, you created the spaCy object 'nlp'. \n",
    "\n",
    "An 'object'?????\n",
    "\n",
    "So what is an object? You can see it as an instance of a class of things. Whenever you create a variable and assign it a value of some type, this variable is a 'typed' instance. Like a baby that is born is a new instance of the type human and a kitten is a new instance of the type cat. It has all the properties of this type and you can do the things with the variable that were defined for that type.\n",
    "\n",
    "So 'nlp' is an instance of a spaCy object loaded with the models for the English language. This means that 'nlp' has all the functions that the spaCy developers defined.\n",
    "\n",
    "You can use 'nlp' now to process text through a defined pipeline of modules and store the result as a value for another variable for accessing it. The result of processing a text with spaCy is another spaCy object of the type 'Doc'.\n",
    "\n",
    "'Doc' objects are complex and give you access to different analyses that have been applied to the input text. In a Doc object you can access tokens that make up the text, their lemmas, their PoS, the sentences, chunks, named entities, and many more. Below, we will look at all the types of analyses in more detail.\n",
    "\n",
    "To process a text, you simple pass the string to the *nlp* object that we created as input and we assign the result of it to a variable *doc* with a lower case. The variable doc should then be of the tpe Doc. Let's check that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I have an awesome cat. She follows me through the house everywhere I go. Her name is shadow\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it. spaCy has now processed the text in many different ways ang gives you the sentences, tokens, dependency relations, named entities, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So whereas we had to call all these specific functions in the right order in the case of NLTK, i.e. sentence_splitting, tokenization, part-of-speech tagging, parsing and named entity recognition, spaCy did all of that when processing a text. The only thing you need to do is to access the output, which is what we will do next.\n",
    "\n",
    "`doc` is now a Python object of the class `Doc`. It is a container for accessing linguistic annotations and a sequence of `Token` objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc, Token and Span objects\n",
    "\n",
    "At this point, there are three important types of objects to remember:\n",
    "\n",
    "* A `Doc` is a sequence of `Token` objects.\n",
    "* A `Token` object represents an individual token — i.e. a word, punctuation symbol, whitespace, etc. It has attributes representing linguistic annotations. \n",
    "* A `Span` object is a slice from a `Doc` object and consists of a sequence of `Token` objects.\n",
    "\n",
    "Since `Doc` is a sequence of `Token` objects, we can iterate over all of the tokens in the text as shown below, or select a single token from the sequence. We need a 'for-loop' to iterate over the elements in the doc object just as we would do for a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "an\n",
      "awesome\n",
      "cat\n",
      ".\n",
      "She\n",
      "follows\n",
      "me\n",
      "through\n",
      "the\n",
      "house\n",
      "everywhere\n",
      "I\n",
      "go\n",
      ".\n",
      "Her\n",
      "name\n",
      "is\n",
      "shadow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the tokens using a for loop\n",
    "for token in doc:\n",
    "    print(token)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that spaCy does not really create a list but a so-called 'generator'. A generator is a so-called 'lazy iterator' in Python that does not overload memory:\n",
    "\n",
    "https://realpython.com/introduction-to-python-generators/\n",
    "\n",
    "You can turn it into a list however and load it in memory to see the content as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, have, an, awesome, cat, ., She, follows, me, through, the, house, everywhere, I, go, ., Her, name, is, shadow]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with a list, we can also access each token individually using the item index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First token: I\n",
      "Second token: have\n"
     ]
    }
   ],
   "source": [
    "# Select one single token by index\n",
    "first_token = doc[0]\n",
    "print(\"First token:\", first_token)\n",
    "second_token = doc[1]\n",
    "print(\"Second token:\", second_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that even though these tokens look like strings, they are not. Print just gives the print representation of the token. We can also ask for the 'type' in Python and it will tell you what kind of object it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t <class 'spacy.tokens.token.Token'>\n",
      "have \t <class 'spacy.tokens.token.Token'>\n",
      "an \t <class 'spacy.tokens.token.Token'>\n",
      "awesome \t <class 'spacy.tokens.token.Token'>\n",
      "cat \t <class 'spacy.tokens.token.Token'>\n",
      ". \t <class 'spacy.tokens.token.Token'>\n",
      "She \t <class 'spacy.tokens.token.Token'>\n",
      "follows \t <class 'spacy.tokens.token.Token'>\n",
      "me \t <class 'spacy.tokens.token.Token'>\n",
      "through \t <class 'spacy.tokens.token.Token'>\n",
      "the \t <class 'spacy.tokens.token.Token'>\n",
      "house \t <class 'spacy.tokens.token.Token'>\n",
      "everywhere \t <class 'spacy.tokens.token.Token'>\n",
      "I \t <class 'spacy.tokens.token.Token'>\n",
      "go \t <class 'spacy.tokens.token.Token'>\n",
      ". \t <class 'spacy.tokens.token.Token'>\n",
      "Her \t <class 'spacy.tokens.token.Token'>\n",
      "name \t <class 'spacy.tokens.token.Token'>\n",
      "is \t <class 'spacy.tokens.token.Token'>\n",
      "shadow \t <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"\\t\", type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `Token` objects have many useful *methods* and *attributes*, which we can get listed by using the Python function `dir()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't really talked about attributes during this course, but while methods are operations or activities performed by that object, attributes are 'static' features or properties of objects. Methods are called using parantheses (as we have seen with `str.upper()`, for instance), while attributes are indicated without parentheses. We will see some examples below. In the case of spaCy tokens, attributes typically contain *annotations* of the token in the text.\n",
    "\n",
    "You can find more detailed information about the token methods and attributes in the [documentation](https://spacy.io/api/token).\n",
    "\n",
    "Notice that there are many attributes with double listings, one without and once with the suffix `_`. The attributes without `_` actually have numerical values that spaCy uses internally, whereas variants with `_` have the human readable rendering of the value in unicode. The internal numerical repesentations are used to store data more efficiently, whereas the readable values are only generated for rendering output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561228191312463089 -PRON- 95 PRON\n",
      "14692702688101715474 have 100 VERB\n",
      "15099054000809333061 an 90 DET\n",
      "3240785716591152042 awesome 84 ADJ\n",
      "5439657043933447811 cat 92 NOUN\n",
      "12646065887601541794 . 97 PUNCT\n",
      "561228191312463089 -PRON- 95 PRON\n",
      "14462500713227930305 follow 100 VERB\n",
      "561228191312463089 -PRON- 95 PRON\n",
      "18216413589307435838 through 85 ADP\n",
      "7425985699627899538 the 90 DET\n",
      "9471806766518506264 house 92 NOUN\n",
      "10957650314384693728 everywhere 86 ADV\n",
      "561228191312463089 -PRON- 95 PRON\n",
      "8004577259940138793 go 100 VERB\n",
      "12646065887601541794 . 97 PUNCT\n",
      "561228191312463089 -PRON- 90 DET\n",
      "18309932012808971453 name 92 NOUN\n",
      "10382539506755952630 be 100 VERB\n",
      "9756944156339232386 shadow 92 NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma, token.lemma_,token.pos, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use spacy.explain to find out more about certain labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out some more, such as NN, ADP, PRP, VBD, VBP, VBZ, WDT, aux, nsubj, pobj, dobj, npadvmod\n",
    "spacy.explain(\"ADP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting & tokenization\n",
    "spaCy performs sentence splitting for you. The information is stored in the attribute **sents** of `Doc` (of type *spacy.tokens.doc.Doc*).\n",
    "Each `Doc` contains a sequence of `Token` objects, i.e., this is where the output from the tokenizer is found. The token itself can be accessed using the attribute **text**. Each `Doc` instance will also have an index over the tokens to group them into sentences. We can iterate over these sentence indexes and get the tokens from each sentence in sequence. This will access the text sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have an awesome cat. She follows me through the house everywhere I go. Her name is shadow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT SENTENCE\n",
      "I have an awesome cat.\n",
      "I\n",
      "have\n",
      "an\n",
      "awesome\n",
      "cat\n",
      ".\n",
      "NEXT SENTENCE\n",
      "She follows me through the house everywhere I go.\n",
      "She\n",
      "follows\n",
      "me\n",
      "through\n",
      "the\n",
      "house\n",
      "everywhere\n",
      "I\n",
      "go\n",
      ".\n",
      "NEXT SENTENCE\n",
      "Her name is shadow\n",
      "Her\n",
      "name\n",
      "is\n",
      "shadow\n"
     ]
    }
   ],
   "source": [
    "sentences=doc.sents\n",
    "for sentence in sentences:\n",
    "    print('NEXT SENTENCE')\n",
    "    print(sentence)\n",
    "    for token in sentence:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "The output from the lemmatizer is stored in the attribute **lemma_** of each `Token' object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have awesome cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats cat\n"
     ]
    }
   ],
   "source": [
    "cat_token = doc[3]\n",
    "print(cat_token.text, cat_token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the part of speech tagger is stored:\n",
    "* in the attribute **pos_** of each `Token` object: The simple part-of-speech tag\n",
    "* in the attribute **tag_** of each `Token object: The detailed part-of-speech tag ([Penn Treebank POS tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have awesome cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats NOUN NNS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'noun, plural'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_token = doc[3]\n",
    "print(cat_token.text, cat_token.pos_, cat_token.tag_)\n",
    "spacy.explain(cat_token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing\n",
    "The output of the dependency parser can only be accessed by combining the information from multiple attributes. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has a special function *displacy* to display data. We first need to import it and next can apply it to the *doc* object. There are various paramters that can be said. You can read more in the spaCy documentation: https://spacy.io/usage/visualizers. Now, we choose the *style* 'dep' for depenedency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"31e3e05a2d84420b9b84d3c10efe169c-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Autonomous</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">cars</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">shift</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">insurance</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">liability</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">toward</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">manufacturers</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-31e3e05a2d84420b9b84d3c10efe169c-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-31e3e05a2d84420b9b84d3c10efe169c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-31e3e05a2d84420b9b84d3c10efe169c-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-31e3e05a2d84420b9b84d3c10efe169c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-31e3e05a2d84420b9b84d3c10efe169c-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-31e3e05a2d84420b9b84d3c10efe169c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-31e3e05a2d84420b9b84d3c10efe169c-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-31e3e05a2d84420b9b84d3c10efe169c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-31e3e05a2d84420b9b84d3c10efe169c-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-31e3e05a2d84420b9b84d3c10efe169c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-31e3e05a2d84420b9b84d3c10efe169c-0-5\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-31e3e05a2d84420b9b84d3c10efe169c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,179.0 L1103.0,167.0 1087.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that each token has a dependency relation with at least one other token. For example:\n",
    "* **cars** has an **amod** relation with **autonomous**\n",
    "* the main verb **shift** has an **nsubj** relation with **cars**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know what these relations mean, you can use **spacy.explain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjectival modifier'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('amod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy makes use of the terms **child** and **head** in their dependency parsing output.\n",
    "* a relation is always in one direction from a **child** to a **head**, e.g., *autonomous* is the child of *cars*\n",
    "* a head of a phrase can be the child of another token, e.g., *cars* is the child of *shift*\n",
    "* a token without a head is the root of the text or sentence (often the main verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attributes are needed to access this information:\n",
    "* **dep_** provides the syntactic relation, e.g., *nsubj*\n",
    "* **head** provides the **head** of a `Token`, e.g., in the case of *autonomous* the head would be *cars*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous amod cars\n"
     ]
    }
   ],
   "source": [
    "autonomous_token = doc[0]\n",
    "print(autonomous_token, autonomous_token.dep_, autonomous_token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars nsubj shift\n"
     ]
    }
   ],
   "source": [
    "cars_token = doc[1]\n",
    "print(cars_token, cars_token.dep_, cars_token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save tree structure to SVG image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_structure = displacy.render(doc, jupyter=False, style='dep')\n",
    "\n",
    "output_path = 'spacy_tree_structure.svg'\n",
    "with open(output_path, 'w') as outfile:\n",
    "    outfile.write(tree_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "The output from the Named Entity Recognizer is stored in the attribute **ents** of `Doc`.\n",
    "The attribute **label_** and an **ent** (of type *spacy.tokens.span.Span*) contains the named entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"But Google is starting from behind. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption.\"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">But \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is starting from behind. The company made a late push into hardware, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple’s Siri\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", available on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    iPhones\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s Alexa software, which runs on its \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Echo and Dot\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " devices, have clear leads in consumer adoption.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Apple’s Siri ORG\n",
      "iPhones ORG\n",
      "Amazon ORG\n",
      "Echo and Dot ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
