{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB2.2: Using Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce you to word embeddings. Word embeddings are vector representations for words learned by a neural network to predict words that occur in their context. Similar words wil tend to occur in similar context and end up with similar weights. The final weights eventually form the vector that acts as a representation of a word. Usually, vector sizes are limited to 300 upto 500 dimensions (weights) in so-called Word2Vec approaches.\n",
    "\n",
    "For learning these representations a large corpus of text is needed with sufficient occurrences of words in different contexts. We call the word for which we want to learn a representation a *target word* and the words it needs to predict the *context words*. Each observed context word counts as a correct or positive example of a word that needs to be predicted by a target word. As negative examples of words that do not occur in the target's context, random words are chosen.\n",
    "\n",
    "The learning consists of making the weights of the target word more similar to the weights of the correct context words and less similar to the incorrect (random) contexts words.\n",
    "\n",
    "The learning starts with a random initialisation of the weights but gradually the weights get updated after seeing more and more training cases. You could say that updating the weights moves the words away from the negative examples and towards the positive examples in a multidimensional semantic space. We expect that similar and related words end up close to each other in this space.\n",
    "\n",
    "For example given the following texts, the words ```dog```, ```cat```, and ```mouse``` will get rather similar weights because they partially occur in each other contexts and partially share other context words such as ```chase``` and  ```ate```:\n",
    "\n",
    "```The dog chased the cat.\n",
    "The cat chased the mouse.\n",
    "The mouse ate the cheese.\n",
    "The dog ate a bone.\n",
    "The cat ate the fish\n",
    "```\n",
    "\n",
    "They also differ a little bit e.g. because they eat different things. Imagine doing this for hundreds-of-thousands of documents and ten-thousands of words. It will create a large semantic space positioning words that are related in close proximity. In a way such a space is similar to the network structure of WordNet but a word embedding space is derived empirically from texts and not from human judgements.\n",
    "\n",
    "![word-embedding](./images/word-embedding.png)\n",
    "\n",
    "Screen dump taken from: https://projector.tensorflow.org\n",
    "\n",
    "Two major disadvantages of word-embeddings are: 1) different meanings of words get conflated (e.g. star, mouse, organ) and 2) the vectors represent ```relatedness``` in general and not more precise semantic relations as defined in WordNet. The latter means that embeddings do not make a difference between synonyms, antonyms, (co-)hyponyms, inflected forms, derivational forms, etc.\n",
    "\n",
    "One of the technical advantages of representing word meanings as vectors is that comparing vectors of words always gives a result: i.e. the vectors are dense and do not contain zero values. Obviously, words that do not have a representation, such as domain specific terminology that is not included in the embedding vocabulary, cannot be matched to anything.\n",
    "\n",
    "**Reference**: Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems 26 (2013).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to load prebuilt word embeddings and learn how to explore these. \n",
    "\n",
    "There are many packages and data sets with embeddings. We focus on publicly available and trainable embeddings, especially for multiple languages. Concretely, we will use embeddings from:\n",
    "\n",
    "* [Wikipedia2Vec](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/)\n",
    "* [Fasttext](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md)\n",
    "\n",
    "Wikipedia2Vec created word embeddings for several languages built from their Wikipedia pages. Fasttext, created by Facebook, created embeddings for 157 languages from Wikipedia and web data collected by the [Common Crawl project](https://commoncrawl.org).\n",
    "\n",
    "The embeddings are created using different Python packages, but they are also available in a common Word2Vec text format. This makes it possible to load the embeddings regardless of how they have been created. We will use the *GenSim* package to load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Gensim: a package for handling embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is an open source and free python package that is often used for building and using word embeddings:\n",
    "\n",
    "* https://radimrehurek.com/gensim/#\n",
    "* https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here\n",
    "\n",
    "You need to install Gensim version 4 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Gensim is not already installed, install it on your local machine from the command line or the notebook:\n",
    "\n",
    "    * conda install -c conda-forge gensim\n",
    "    \n",
    "    OR\n",
    "    \n",
    "    * pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge gensim\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If gensim is succefully installed you can use existing models models in text (txt) format that are compatable with gensim. To do so, we first import gensim and the package *numpy*, which is often used to represent vectors and arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word embeddings built from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is often used to build and test language models. The reasons are obvious: it is large, freely available and there are Wikipedias in many languages, often also linked to each other and partly covering similar content.\n",
    "\n",
    "You can download pre-trained models in various languages from: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "There are different versions for each language trained for 100, 300, and even 500 dimensions. If your computer has limited capacity, it is better to start with 100 dimensions. For this notebook, we will download ```enwiki_20180420_100d.txt.bz2```, which is a compressed version of the 100-dimensions embedding model built from the English Wikipedia. After download, you need to decompress the \"txt.bz2\" file to a file with the extension \".txt\", using a  file (de)compression utility that can handle \"bz2\" format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to peek into the file that you have downloaded. It is too big to load it in a text editor so we will try to print part of the file to the screen of the terminal.\n",
    "\n",
    "We can use the command line commands **cat** (use **type** on Windows) and **more** to inspect the beginning of the text file to see what it contains. Make sure it is decompressed.\n",
    "\n",
    "Specify the path to your own download after the command and do not forget to add \" | more\" to the end, \n",
    "otherwise it will list a gigabyte large file in your notebook. We only want to inspect the beginning. On a Windows machine, you may need to use *type* instead of *cat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4530030 100\n",
      "the -0.0811 0.5145 0.0368 0.0544 -0.0662 0.2121 0.1636 -0.1011 0.0607 -0.1858 -0.1965 -0.0595 0.0703 -0.1013 0.3130 -0.1717 0.0847 -0.1222 0.1024 0.0753 -0.1384 0.0435 -0.0371 0.1932 -0.1226 -0.2227 -0.1530 -0.2890 0.2371 0.2699 0.2693 -0.1666 0.0240 0.1053 -0.1475 -0.3232 0.0236 -0.2056 0.2847 -0.2817 0.1197 0.0314 -0.1215 0.0782 -0.2850 -0.1316 -0.0844 0.1483 -0.2192 -0.0462 -0.2151 0.0582 0.0372 0.0127 -0.3074 -0.1582 -0.1393 0.0361 -0.2519 -0.0305 -0.1532 -0.0286 -0.0955 0.3037 0.5632 -0.1120 -0.0319 -0.2223 -0.2612 -0.2254 -0.1593 0.1807 0.1205 0.3695 -0.2652 -0.0490 -0.2556 0.0130 -0.0898 0.0322 0.0021 -0.2692 0.3129 0.0179 0.3913 0.5415 -0.0049 0.0884 0.1605 0.0878 0.0004 0.1465 0.1872 0.0521 -0.1492 -0.0882 0.1696 0.1894 -0.0866 0.1184\n",
      "in 0.1245 0.4200 0.2936 0.0924 -0.0669 0.0252 0.1407 -0.0729 0.0680 -0.2951 -0.2720 0.0785 0.0780 0.0248 0.0427 -0.1497 0.1013 -0.0257 0.0364 0.2647 0.0330 0.1047 0.0382 0.0138 -0.0162 -0.0733 0.0960 -0.2090 0.0561 0.1030 0.2898 -0.1914 -0.0927 0.1237 -0.0023 -0.4792 0.0523 -0.0819 0.3551 -0.2274 0.3301 0.0547 -0.1707 0.2304 -0.2599 -0.1389 -0.0106 0.1921 -0.3615 0.0077 -0.2439 0.1056 -0.0010 -0.2522 -0.2321 -0.1604 -0.2652 -0.0134 -0.3000 0.1215 0.0737 0.0215 -0.1647 0.2799 0.5886 -0.0189 -0.1250 -0.2438 -0.1621 -0.3960 -0.1078 0.2162 -0.1173 0.6267 -0.0788 0.0086 -0.1317 0.1440 -0.2035 -0.0742 -0.0536 -0.2773 0.1026 0.0356 0.3384 0.4606 -0.1893 0.0142 0.2269 0.1136 0.0962 0.1648 0.1065 0.0051 -0.1777 -0.0971 0.3434 0.2423 -0.1432 -0.1808\n",
      "of 0.0018 0.3950 0.0662 0.1279 -0.2606 0.3275 -0.0161 -0.1740 0.0530 -0.1168 -0.1781 0.0897 0.0423 -0.1886 0.2358 -0.1118 0.1510 -0.2601 0.0367 -0.0689 -0.1016 :\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat /Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.txt | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *cat* command remains active untill you stop the cell in the notebook (see the ```[*]``` before the cell). Please stop running the cell manually (the square next to the play symbol in the top menu of the notebook) to proceed.\n",
    "\n",
    "If things worked out you see the beginning of the embedding text file. It starts with two digits on the first line.\n",
    "In my case this is \"4530030\" and \"100\". The first is the length of the vocabulary and the second the number of dimensions.\n",
    "\n",
    "The following lines show a word from the learned vocabulary and a list of 100 digits, which are the weights for the dimensions that form the embedding. The first line shows the word \"the\" followed by 100 digits. The rest of the file consists of lines for each word with its embeddings. This is the so-called word2vec format for representing word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the package used to load this file, embeddings may be represented differently in memory but it always boils down to three components:\n",
    "\n",
    "* vocabulary: a list of observed words\n",
    "* matrix of learned weights as word vectors\n",
    "* mapping from the words in the vocabulary to the vectors in the matrix\n",
    "\n",
    "A vector is a list of digits, such that each digit is a value for a dimension in an n-dimensional space. If you have a list of these vectors you have a matrix of n-columns and m-rows. Each row corresponds to the vector of a word in the vocabulary.\n",
    "\n",
    "The matrix of 3 rows and 3 columns\n",
    "```\n",
    "[[.34, .56, ,12],\n",
    " [.12, .39, ,05],\n",
    " [.78, .37, ,01]]\n",
    "```\n",
    "\n",
    "The vocabulary with the word as a key and the matrix (list of lists) index that points to the row with the embedding for the word:\n",
    "\n",
    "```{\"dog\": 0, \"cat\" : 1, \"car\" : 2}```\n",
    "\n",
    "For this data, a simple lookup function for *dog* will give the embedding *[.34, .56, ,12]*, which is the row with the index \"0\" in the matrix.\n",
    "\n",
    "Now let's see how this is implemented in GenSim for the Wikipedia derived word embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading a model in Gensim\n",
    "\n",
    "We will now load the downloaded embedding file using the gensim class \"KeyedVectors\", which has a function \"load_word2vec_format\" that can load such a text file. As a parameter, you give it the path to your local file. As an additional parameter you can set a limit for how many words should be read. If you have limited memory on your computer it is wise to set a limit. Note that less words will be read from the file and included in memory if you set a limit. The order in the file is based on frequency so setting a limit means low frequent words are not loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the local copy of a model built from wikipedia\n",
    "MODEL_FILE='/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/wiki2vec/enwiki_20180420_100d.txt'\n",
    "\n",
    "### If you have a small computer you may want to limit the number of embeddings loaded as shown below:\n",
    "wiki2vec = KeyedVectors.load_word2vec_format(MODEL_FILE, limit=5000) \n",
    "\n",
    "### To load the full model you should drop the limit.\n",
    "# Loading the full model can take a while.\n",
    "##wiki2vec = KeyedVectors.load_word2vec_format(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type of object **gensim** created by loading the data and what the properties and functions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_load_specials', '_log_evaluate_word_analogies', '_save_specials', '_smart_save', '_upconvert_old_d2vkv', '_upconvert_old_vocab', 'add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_mean_vector', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_for_all', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "print(type(wiki2vec))\n",
    "print(dir(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \"wiki2vec\" is an instance of **KeyedVectors**.  KeyedVectors is essentially a mapping from keys to vectors. Each vector is identified by its lookup key, most often a short string token.\n",
    "\n",
    "One of the functions is the \"load_word2vec_format\" function we used to load the data from a file, but we also see interesting functions such as: 'get_vector', 'cosine_similarities', 'distance', 'most_similar', 'vocab'. We will look into a few of them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a dictionary that contains words. Let's check how big the vocabulary is of the model derived from English Wikipedia. Note that you will have a different result if you loaded only part of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 100\n",
      "Vocabulary size = 5000 5000\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', wiki2vec.vector_size)\n",
    "print('Vocabulary size =', len(wiki2vec.key_to_index), len(wiki2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not limit the loading you should see that there are four-and-a-haf millions words in this model. That is a lot more than in the English WordNet. Let's check if the word \"man\" is in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a man?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a man?\")\n",
    "\"man\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlikely that there is a cultural specific Dutch word such as \"tjalk\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a tjalk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a tjalk\")\n",
    "\"tjalk\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is rich, so this one is in there too. Let's go wild:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a tjalk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Is there a tjalk\")\n",
    "\"Is there a tjalk\" in wiki2vec.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, whole sentences are not in there. We can also get the vocabulary as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model vocabulary is represented internally as a...\n",
      "<class 'dict'>\n",
      "['the', 'in', 'of', 'a', 'and', 'is', 'to', 'was', 'by', 'for', 'on', 'as', 'at', 'from', 'with', 'an', 'it', 'that', 'also', 'which', 'first', 'this', 'has', 'he', 'one', 'his', 'are', 'after', 'who', 'were', 'two', 'its', 'new', 'be', 'or', 'but', 'had', 'their', 'been', 'born', 'not', 'other', 'all', 'have', 'during', 'time', 'when', 'may', 'they', 'into']\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "vocabulary = wiki2vec.key_to_index\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(vocabulary))\n",
    "print((list(vocabulary))[:50])\n",
    "print(vocabulary[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here are the first 50 items in the list. As you can see these are very common and frequent English words. We also printed the value for the key \"man\" in the dictionary which give the index position in the list that is used to get its vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  The embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the embedding representation for a specific word that is in the vocabulary using the **get_vector** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information stored in the vocabulary for the word \"man\":\n",
      "<class 'numpy.ndarray'>\n",
      "Distributional meaning of \"planet\" in Wikipedia: [-2.377e-01  2.686e-01 -9.620e-02  2.707e-01 -2.241e-01 -2.489e-01\n",
      "  1.065e-01  4.120e-02 -5.349e-01 -1.445e-01 -8.700e-02 -1.877e-01\n",
      "  1.985e-01 -1.643e-01  1.021e-01 -1.783e-01 -5.520e-02  2.190e-02\n",
      " -2.180e-01  1.569e-01 -2.835e-01 -3.299e-01 -6.780e-02  3.505e-01\n",
      " -3.241e-01 -9.000e-04 -1.234e-01 -3.452e-01 -4.523e-01  7.449e-01\n",
      "  1.470e-01 -1.258e-01 -1.073e-01  4.019e-01  1.120e-01  2.230e-02\n",
      " -3.720e-01  2.026e-01  3.160e-02  2.910e-02 -2.406e-01  1.368e-01\n",
      " -1.750e-02  1.020e-01  8.340e-02  5.012e-01 -3.973e-01  4.010e-02\n",
      " -1.653e-01 -1.892e-01 -1.441e-01  6.290e-02 -5.185e-01 -2.638e-01\n",
      "  3.170e-02 -6.030e-02  1.012e-01 -5.408e-01 -3.528e-01 -1.281e-01\n",
      " -2.617e-01 -2.607e-01 -9.150e-02  3.094e-01  4.468e-01 -2.526e-01\n",
      " -2.842e-01 -9.303e-01 -3.270e-02 -4.669e-01  4.064e-01  2.045e-01\n",
      "  2.223e-01  2.501e-01 -4.577e-01  4.089e-01  1.261e-01 -2.000e-01\n",
      " -8.700e-02 -2.701e-01  3.453e-01  7.210e-02  1.277e-01 -2.570e-02\n",
      "  2.319e-01  3.712e-01  5.400e-02  1.502e-01  2.973e-01  7.630e-02\n",
      " -8.790e-02  2.386e-01  3.562e-01 -6.440e-02  2.086e-01 -3.865e-01\n",
      " -1.940e-02  5.608e-01 -6.439e-01  2.104e-01]\n"
     ]
    }
   ],
   "source": [
    "print('Information stored in the vocabulary for the word \"man\":')\n",
    "man_vector=wiki2vec.get_vector('man')\n",
    "print(type(man_vector))\n",
    "print('Distributional meaning of \"planet\" in Wikipedia:', man_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get exotic numbers for 100 dimensions. What else did you expect? \n",
    "\n",
    "A vector is a sorted bunch of numbers, each representing a dimension. These numbers are the weights learned by the neural network when predicting the context words of 'man'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data object is of the type ```numpy.ndarray```. This is a special array defined in the **numpy** package. Numpy is a package for efficiently processing numerical data and is used a lot in machine learning. For those interested, here is the description:\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many dimensions do we have in these vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy data shape (100,)\n",
      "Number of vector dimensions: 100\n"
     ]
    }
   ],
   "source": [
    "print('Numpy data shape', man_vector.shape)\n",
    "print('Number of vector dimensions:', len(man_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a surprise: we loaded a model with 100 dimensions based on a layer with 100 weights. This is true for all words so also for 'dog'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector4dog = wiki2vec['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector4dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013   0.6476  0.1045 -0.3117  0.1475  0.0809 -0.1523  0.265  -0.6462\n",
      " -0.2058  0.1564 -0.2072  0.4179  0.0386 -0.0194 -0.2241  0.2227 -0.3452\n",
      " -0.426   0.1028 -0.2136 -0.0267  0.1946  0.3652 -0.2265 -0.2736  0.0326\n",
      " -0.0279 -0.2359  0.5077  0.3759 -0.2207 -0.0506  0.7909  0.1344 -0.079\n",
      " -0.4099  0.1559 -0.0066  0.1236 -0.5474 -0.0877 -0.3738 -0.253  -0.4688\n",
      " -0.1184 -0.0501  0.3267 -0.1799 -0.2662  0.0968  0.2891 -0.4816 -0.3374\n",
      "  0.2488  0.1744  0.0889 -0.1873 -0.3312 -0.1903  0.0547 -0.6149 -0.427\n",
      " -0.1079  0.137  -0.1445  0.0521 -0.5711 -0.3859 -0.6626  0.2417 -0.0141\n",
      "  0.3974  0.1331 -0.6726 -0.227   0.1793  0.2454  0.1545 -0.0923 -0.0247\n",
      " -0.4611 -0.1317 -0.2194  0.2143  0.491  -0.2186  0.2463  0.0843  0.1324\n",
      " -0.4565  0.008   0.6242 -0.0217 -0.084  -0.4722  0.1191  0.3299 -0.9191\n",
      "  0.0963]\n"
     ]
    }
   ],
   "source": [
    "print(vector4dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the representations are compatible across the words, we can compare two vector representations through the cosine similarity function:\n",
    "\n",
    "![Cosine similarity](./images/cosine-full.png \"Logo Title Text 1\")\n",
    "\n",
    "So suppose we have two vectors A and B, each with 100 slots, this formula (taken from the Wikipedia page) tells you to sum the results of multiplying each slot across A and B:\n",
    "\n",
    "A[0]\\*B[0]+A[1]\\*B[1]+....A[99]\\*B[99]\n",
    "\n",
    "We divide this sum by the square-root of the total sum of the slots of A, multiplied by the square-root of the total sum of the slots of B. Dividing it that way normalises the value between 0 and 1 and it makes the sum of the products of the numerator relative to the product of the sums of the individual vectors.\n",
    "\n",
    "Embedding software uses such measures to obtain the most similar words. We can now use the *most_similar_by_word* function from Gensim to ask for the words that are most similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 0.8286387324333191), ('horse', 0.7321138978004456), ('wolf', 0.699967622756958), ('bear', 0.6938636302947998), ('animal', 0.6909422874450684), ('horses', 0.6853740811347961), ('boy', 0.6745132803916931), ('wild', 0.642367959022522), ('animals', 0.6414468288421631), ('baby', 0.6405693888664246), ('bird', 0.6398178935050964), ('lion', 0.6347697377204895), ('man', 0.6279305815696716), ('hunting', 0.6278021335601807), ('hunt', 0.6177456378936768), ('girl', 0.6173814535140991), ('kids', 0.6070773601531982), ('cattle', 0.6012865900993347), ('ghost', 0.5963084101676941), ('walking', 0.5899751782417297), ('ride', 0.5694831609725952), ('talking', 0.566556990146637), ('giant', 0.5654652714729309), ('little', 0.5652263164520264), ('snow', 0.5511893033981323), ('eat', 0.5508083701133728), ('blind', 0.5480281710624695), ('eagle', 0.5464779734611511), ('watching', 0.5413812398910522), ('night', 0.5381313562393188), ('fish', 0.5369225144386292), ('hunter', 0.5357449650764465), ('billy', 0.5355514883995056), ('charlie', 0.5347419381141663), ('eye', 0.5325638055801392), ('watch', 0.5323816537857056), ('truck', 0.5320487022399902), ('hair', 0.5319405794143677), ('gang', 0.5318728089332581), ('my', 0.5316421985626221), ('birds', 0.5296006798744202), ('woman', 0.5257622003555298), ('favorite', 0.5238966941833496), ('alive', 0.5232323408126831), ('guy', 0.5230545401573181), ('fox', 0.5222675800323486), ('food', 0.52142733335495), ('riding', 0.5207430720329285), ('kind', 0.520662248134613), ('bad', 0.519469141960144)]\n"
     ]
    }
   ],
   "source": [
    "dog_sim = wiki2vec.similar_by_word(\"dog\", topn=50)\n",
    "print(dog_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at this list and compare it with the *dogs* we got from the WordNet hiearchy. Which is richer, which is more precise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **similarity** function directly gives the cosine similarity score across the vectors of two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8286388\n",
      "0.62793064\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.similarity(\"dog\", \"cat\"))\n",
    "print(wiki2vec.similarity(\"dog\", \"man\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine sets of vectors by providing a list of positive words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 0.7180519104003906), ('girl', 0.7038720846176147), ('boy', 0.6995949149131775), ('baby', 0.6899586915969849), ('songs', 0.6800959706306458), ('me', 0.6741494536399841), ('dream', 0.6591712832450867), ('my', 0.65790855884552), ('album', 0.6526068449020386), ('you', 0.6523215770721436), ('man', 0.6475565433502197), ('love', 0.6428434252738953), ('duo', 0.6397884488105774), ('soul', 0.639680027961731), ('singing', 0.6390323042869568), ('dancing', 0.6380634307861328), ('dreams', 0.633378267288208), ('thing', 0.6268312931060791), ('sing', 0.624162495136261), ('christmas', 0.6210137009620667), ('sweet', 0.6201736927032471), ('favorite', 0.6167346835136414), ('forever', 0.6152535676956177), ('lyrics', 0.6150415539741516), ('dance', 0.6132447123527527), ('johnny', 0.6130370497703552), ('theme', 0.6109194755554199), ('soundtrack', 0.6105639934539795), ('ghost', 0.6094201803207397), ('heaven', 0.6092945337295532), ('billy', 0.6088224053382874), ('ep', 0.6058269143104553), ('kind', 0.6057148575782776), ('sounds', 0.6045936942100525), ('night', 0.6033223271369934), ('horse', 0.6032891869544983), ('fun', 0.6029319167137146), ('compilation', 0.6028576493263245), ('wolf', 0.6011175513267517), ('dj', 0.6000833511352539), ('hop', 0.5994495749473572), ('kids', 0.5967081189155579), ('band', 0.5958008170127869), ('pop', 0.5956150889396667), ('bear', 0.5954610109329224), ('ride', 0.5933997631072998), ('sung', 0.5933097004890442), ('wild', 0.592725932598114), ('show', 0.589067816734314), ('performed', 0.5879281163215637)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=['dog', 'song'], topn=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining the vectors of **dog** and **song**, we created a new derived vector, a cocktail of both. Next, getting the most similar to this cocktail, we get a very different list of rap-related concepts. Try to make your own cocktails, it is fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another operation is to get the word that matches the **least** with others, as in an odd-one-out task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'dog']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add and substract vectors to obtain a target vector in space, assuming that there is parallelism in the semantic space. So we can remove **song** associations from **dog** to get the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('animal', 0.44155311584472656), ('horse', 0.4160136580467224), ('cat', 0.4146558940410614), ('cattle', 0.3957338333129883), ('horses', 0.39379721879959106), ('hunting', 0.3910510540008545), ('animals', 0.3791029751300812), ('wolf', 0.3589412569999695), ('bear', 0.35647258162498474), ('hunt', 0.3361527621746063)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=[\"dog\"], negative=[\"song\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you loaded the full vocabulary, you will see a lot of ENTITY-prefixed results. This is an artifact of the Wikipedia2Vec processing in which they kepts all entity linked words as separate items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most cited example in this respect is not about dogs and cats but about kings and queens (see the original Word2Vec paper Mikolov et al 2013). By adding **woman** to **king** and subtracting **man**, you should end up somewhere in semantic space close to **queen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.830649197101593), ('princess', 0.6878246665000916), ('throne', 0.6823571920394897), ('mother', 0.6350200176239014), ('daughter', 0.6186712384223938), ('ruler', 0.6153962016105652), ('kings', 0.5967732071876526), ('prince', 0.5895270109176636), ('elizabeth', 0.5890194177627563), ('wife', 0.5843674540519714)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec analogy example has been critisized because **king** and **queen** are already very close without doing anything. Nevertheless, adding and substracting did move **queen** closer as it moved from position 12 to 1 and scored 0.83 instead of 0.75 due to our vector manipulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7570435\n",
      "[('kings', 0.7624024152755737), ('queen', 0.7570434808731079), ('prince', 0.730976939201355), ('throne', 0.7229810357093811), ('reign', 0.7097601294517517), ('ruler', 0.695665717124939), ('vi', 0.6845434904098511), ('kingdom', 0.6576052308082581), ('uncle', 0.65497225522995), ('iv', 0.647962212562561), ('nephew', 0.639938235282898), ('son', 0.6387168765068054), ('duke', 0.6294047236442566), ('iii', 0.6281170845031738), ('princess', 0.6230908632278442), ('emperor', 0.6217065453529358), ('grandson', 0.6140714883804321), ('lion', 0.6083786487579346), ('cousin', 0.6015610098838806), ('ii', 0.5979815125465393), ('crown', 0.5971035361289978), ('henry', 0.589224100112915), ('grandfather', 0.5864526033401489), ('father', 0.5855097770690918), ('elder', 0.5843565464019775), ('successor', 0.5833818912506104), ('brother', 0.5782405734062195), ('eldest', 0.5685318112373352), ('philip', 0.5672054290771484), ('legendary', 0.5653011798858643), ('legend', 0.5646098256111145), ('lord', 0.5568930506706238), ('dynasty', 0.551899790763855), ('pope', 0.5507893562316895), ('frederick', 0.5446038842201233), ('edward', 0.5433433651924133), ('tribute', 0.5312865972518921), ('charles', 0.5295990109443665), ('earl', 0.5293132662773132), ('sons', 0.5286513566970825), ('george', 0.5272601842880249), ('daughter', 0.5266226530075073), ('i', 0.5229269862174988), ('him', 0.5172051787376404), ('mother', 0.5127261281013489), ('succeeded', 0.5108527541160583), ('brothers', 0.5097737312316895), ('revenge', 0.5094854831695557), ('denmark', 0.5087653398513794), ('rule', 0.5079477429389954)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.similarity(\"king\", \"queen\"))\n",
    "print(wiki2vec.most_similar(\"king\", topn=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Disambiguating word meanings in word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the positive and negative modifications also to disambiguate the vectors of ambiguous words such as **star**. Let us see what are the most similar words for **planet**, **movie** and **star**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planet related [('earth', 0.7779384851455688), ('moon', 0.709583580493927), ('universe', 0.6131862998008728), ('sky', 0.5947718620300293), ('star', 0.5898671746253967), ('sun', 0.5746722221374512), ('discovery', 0.5633156299591064), ('strange', 0.5613462328910828), ('companion', 0.5607577562332153), ('space', 0.5556343197822571), ('giant', 0.5456600785255432), ('evil', 0.545627236366272), ('ghost', 0.5423661470413208), ('dawn', 0.5373883843421936), ('desert', 0.5262177586555481), ('heroes', 0.5210246443748474), ('wolf', 0.5101659893989563), ('adventures', 0.508303701877594), ('satellite', 0.5034024119377136), ('heaven', 0.5025504231452942)]\n",
      "\n",
      "Movie related [('film', 0.8882695436477661), ('movies', 0.8694241642951965), ('films', 0.8080108761787415), ('sequel', 0.7670617699623108), ('soundtrack', 0.7546120285987854), ('starring', 0.7471210956573486), ('comedy', 0.7360638380050659), ('horror', 0.7279796600341797), ('animated', 0.714120626449585), ('cinema', 0.7102838158607483), ('drama', 0.7020013332366943), ('tv', 0.697636067867279), ('starred', 0.6929982900619507), ('adaptation', 0.6737762689590454), ('television', 0.6724228262901306), ('picture', 0.6589760184288025), ('feature', 0.6533092856407166), ('theatrical', 0.6509866714477539), ('hollywood', 0.6474332213401794), ('actor', 0.6465808749198914)]\n",
      "\n",
      "Star related [('stars', 0.8460984230041504), ('sky', 0.6396337151527405), ('planet', 0.5898672342300415), ('diamond', 0.5856433510780334), ('franchise', 0.5774229764938354), ('heroes', 0.575843095779419), ('wars', 0.5687528252601624), ('bright', 0.5647331476211548), ('sun', 0.5577951669692993), ('starred', 0.5577043890953064), ('phoenix', 0.5560283660888672), ('moon', 0.5451607704162598), ('blue', 0.5360766053199768), ('globe', 0.5351829528808594), ('perfect', 0.5336928367614746), ('series', 0.5331316590309143), ('knight', 0.5329307913780212), ('golden', 0.5314884781837463), ('hero', 0.5295112729072571), ('screen', 0.5270513296127319)]\n"
     ]
    }
   ],
   "source": [
    "planet_sim = wiki2vec.similar_by_word(\"planet\", topn=20)\n",
    "print('Planet related', planet_sim)\n",
    "print()\n",
    "movie_sim = wiki2vec.similar_by_word(\"movie\", topn=20)\n",
    "print('Movie related', movie_sim)\n",
    "print()\n",
    "star_sim = wiki2vec.similar_by_word(\"star\", topn=20)\n",
    "print('Star related', star_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **star** related words are mixed with respect to movies (tsunkatse) and galaxy (v4998 Sagittarii). We can now manipulate the vector with additions and subtractions to \"purify\" the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('earth', 0.5793461203575134), ('moon', 0.5711199641227722), ('sun', 0.559025764465332), ('sky', 0.5364964604377747), ('stars', 0.5075117945671082), ('phoenix', 0.4810594916343689), ('companion', 0.4786052405834198), ('minor', 0.46437937021255493), ('universe', 0.45406731963157654), ('bright', 0.4516937732696533), ('space', 0.44377392530441284), ('discovery', 0.4383010268211365), ('spots', 0.420808345079422), ('giant', 0.4195873737335205), ('dawn', 0.4097822308540344), ('desert', 0.4077986776828766), ('red', 0.40620702505111694), ('wolf', 0.40529564023017883), ('belt', 0.402719646692276), ('knight', 0.39999255537986755)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive = [\"star\", \"planet\"], negative = [\"movie\"], topn=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.714994490146637), ('movies', 0.6806603670120239), ('starring', 0.6611652970314026), ('films', 0.6588518619537354), ('starred', 0.6539907455444336), ('comedy', 0.6453635096549988), ('stars', 0.6431625485420227), ('drama', 0.6371837258338928), ('television', 0.6113601922988892), ('actor', 0.6080341339111328), ('tv', 0.5995522737503052), ('cinema', 0.5963460803031921), ('actors', 0.5732470154762268), ('soundtrack', 0.5640335083007812), ('actress', 0.5583189129829407), ('silent', 0.5474001169204712), ('feature', 0.5368926525115967), ('directed', 0.5320978164672852), ('screen', 0.5302062630653381), ('horror', 0.5292074084281921)]\n"
     ]
    }
   ],
   "source": [
    "print(wiki2vec.most_similar(positive = [\"star\", \"movie\"], negative = [\"planet\"], topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Word Embeddings from Facebook's Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research lab of Facebook, created embeddings in 157 languages from Common Crawl. Common Crawl is a project that scrapes the web for texts in as many languages that are present. Check out the Github of Fasttext and Common Crawl for more details:\n",
    "\n",
    "* https://commoncrawl.org/the-data/get-started/\n",
    "* https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\n",
    "\n",
    "You should download the text version of the embeddings, which are following the same word2vec format as the Wike2Vec models.\n",
    "I downloaded the embeddings for Frysian and Limburgish, which are two local, regional languages. We can inpsect these two text files in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526167 300\n",
      ". 0.0139 -0.0210 -0.0328 -0.0229 0.2410 -0.0260 0.0071 -0.0160 0.0308 0.1494 0.0172 0.0276 -0.0017 0.0205 -0.0072 -0.0089 -0.0154 -0.0288 0.0217 -0.0085 0.0114 0.0119 -0.0284 -0.0170 0.0035 -0.0065 0.0191 -0.0085 0.0190 0.0197 0.0497 -0.0014 0.0060 -0.0069 0.0119 0.0168 0.0049 -0.0049 -0.0081 0.0194 0.0187 -0.0013 -0.0073 0.0139 0.0211 0.0138 -0.0148 0.0042 0.0004 0.0085 -0.0077 -0.0274 -0.0114 -0.0073 -0.0042 0.0038 0.0009 0.1392 0.0047 -0.0014 -0.0010 0.0031 -0.0005 -0.0238 -0.0040 0.0103 0.0276 0.0052 -0.0244 0.0189 0.0208 0.0128 0.0100 -0.0016 0.0019 0.0015 0.0308 -0.0299 -0.0125 0.0094 0.0671 0.0361 -0.0079 -0.0272 0.0130 -0.0070 -0.0004 0.0052 -0.0046 -0.0036 -0.0083 0.0083 0.0192 -0.0097 0.0201 -0.0088 -0.0143 0.0176 -0.0161 -0.0087 0.0108 0.0016 -0.0036 0.0272 0.0440 -0.0102 0.0194 0.0037 0.0198 -0.0054 -0.0010 -0.0030 0.0221 -0.0367 0.0004 0.0175 0.0520 -0.0069 -0.0036 0.0293 -0.0043 -0.0152 -0.0093 0.0255 0.0003 -0.0035 0.0348 0.0176 -0.0236 0.0035 0.0583 -0.0857 -0.0101 0.0213 0.0190 0.0008 -0.0181 0.0081 0.0209 0.0074 -0.0044 -0.0183 -0.0441 -0.0121 -0.0296 -0.0273 -0.0183 -0.0292 -0.0293 0.0112 -0.0128 -0.0079 0.0020 0.0033 -0.0029 0.0103 -0.0195 0.0056 0.0107 -0.0087 -0.0199 0.0067 -0.0024 0.0161 0.0111 -0.0433 -0.0014 -0.0258 -0.0063 -0.0218 -0.1612 -0.0027 0.0227 0.0085 -0.0051 -0.0226 -0.0177 0.0192 0.0071 -0.0080 -0.0211 -0.0169 0.0163 0.1091 0.0094 0.0165 -0.0095 0.0034 -0.0841 0.0296 -0.0232 0.0172 0.0243 -0.1293 -0.0063 -0.0101 -0.0104 -0.0151 0.0037 -0.0299 -0.0078 -0.0068 0.0014 0.2565 -0.0084 -0.0145 0.0246 0.0013 -0.0019 -0.0284 -0.0003 -0.0118 -0.0512 -0.0023 -0.0297 0.0337 0.0002 0.0059 -0.0148 -0.0421 -0.0625 0.0101 -0.0099 0.0114 -0.0115 0.0051 0.0757 -0.0320 0.0172 0.0067 -0.0087 -0.0019 -0.0144 0.020:\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat  \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.fy.300.vec\" | more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306222 300\n",
      "' -0.0150 -0.0400 0.0501 -0.0717 -0.0137 0.0211 0.0416 0.0129 0.0141 0.0216 0.0250 0.0593 -0.0445 0.0193 -0.0003 -0.0169 0.0217 0.0115 -0.0477 -0.0079 -0.0130 0.0283 -0.0274 0.0024 -0.0774 -0.0215 -0.0345 -0.0077 0.0146 -0.0027 -0.0087 0.0011 -0.0105 0.1293 -0.0385 0.0089 -0.0080 -0.0460 0.0075 0.0241 0.0128 0.0006 0.0730 -0.0214 0.1288 0.0327 -0.0051 -0.0403 0.0103 0.0162 -0.0266 -0.0041 0.0075 -0.0028 0.0516 -0.0068 0.0191 -0.0159 0.0393 0.0053 -0.0347 0.0049 0.0063 -0.0582 0.0186 -0.0151 -0.0245 0.0054 0.0352 0.0268 0.0313 0.0166 -0.0037 0.0444 -0.0140 0.0595 0.0168 0.0309 -0.0075 -0.0360 0.0133 -0.0097 0.0309 -0.0339 -0.0262 -0.0637 -0.0074 -0.0029 0.0032 0.0123 -0.0102 -0.0100 -0.0129 -0.0051 -0.0047 -0.0834 -0.0273 -0.0322 -0.0355 -0.0005 -0.0230 0.0571 -0.0147 0.0053 -0.0271 0.0189 -0.0605 0.0250 0.0149 0.0120 -0.0225 -0.0752 -0.0751 0.0189 -0.0105 0.0276 -0.0156 -0.0079 -0.0502 -0.0214 0.0635 0.0127 -0.0148 0.0152 0.0049 -0.0183 0.0351 0.0391 -0.0167 0.0224 0.0248 -0.0057 0.0018 -0.0275 -0.0358 -0.0414 -0.0128 -0.0205 0.0243 0.0138 0.0054 0.0111 -0.0123 -0.0116 -0.0046 0.0290 0.0029 -0.0058 -0.0016 0.0405 0.0277 -0.0029 0.0273 0.0355 -0.0493 -0.0530 0.0177 -0.0318 0.0164 -0.0108 -0.0047 -0.0199 0.0486 -0.0400 0.0238 -0.0121 0.0295 -0.0336 0.0108 -0.0083 -0.0202 -0.0259 -0.0153 -0.0088 -0.0055 0.0055 -0.0043 0.0274 0.0283 -0.0289 -0.0250 -0.0155 0.0300 -0.0481 0.0283 0.0130 0.0679 0.0101 0.0094 -0.0045 0.0328 0.0098 0.0048 -0.0860 0.0593 -0.0411 -0.0361 -0.0155 -0.0471 -0.0191 0.0564 -0.0064 0.0352 -0.0554 0.0229 -0.0149 0.0288 -0.0028 0.0211 0.0071 0.0030 -0.0602 -0.0536 0.0706 -0.0212 0.0224 -0.0222 -0.0491 0.0469 -0.0137 0.0162 0.0087 -0.0149 -0.0094 0.0040 -0.0016 -0.0392 -0.0383 -0.0710 0.0058 0.0040 0.1177 -0.0161 0.0:\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat  \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.li.300.vec\" | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frysian model has 526167 words and the Limburgish model has 306222 words. Let's load the models and inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_my_fasttext_model = \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.fy.300.vec\"\n",
    "fasttext_fy = KeyedVectors.load_word2vec_format(path_to_my_fasttext_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 300\n",
      "Vocabulary size = 526167 526167\n",
      "Word index for \"tjalk\" 141358\n",
      "Vector for \"tjalk\" [-2.80e-02 -6.10e-03 -3.38e-02 -3.78e-02 -1.51e-02 -4.40e-03 -1.70e-03\n",
      " -3.31e-02  5.80e-03 -2.07e-02 -8.60e-03  2.24e-02  2.50e-03  1.31e-02\n",
      " -1.28e-02 -1.37e-02 -2.20e-02 -2.49e-02 -3.70e-03 -3.46e-02 -9.40e-03\n",
      "  3.30e-02  2.13e-02 -9.80e-03  3.20e-03 -1.81e-02  3.38e-02 -3.87e-02\n",
      " -1.57e-02 -1.38e-02  2.29e-02  5.40e-03  1.81e-02  9.30e-03 -2.04e-02\n",
      "  2.55e-02 -1.91e-02  6.40e-03 -8.00e-04  5.83e-02  1.73e-02  1.26e-02\n",
      "  1.97e-02  4.30e-03 -1.90e-03  1.56e-02  1.08e-02 -3.00e-03  8.40e-03\n",
      " -4.40e-03  2.30e-02  8.10e-03 -9.00e-04  8.80e-03  7.50e-03  3.79e-02\n",
      " -2.03e-02 -1.14e-02 -4.22e-02  5.50e-03  2.41e-02 -2.16e-02 -3.00e-03\n",
      "  1.38e-02  2.00e-03 -2.12e-02  2.50e-03 -1.04e-02 -1.48e-02 -3.29e-02\n",
      "  1.56e-02 -1.04e-02 -1.52e-02  7.40e-03  5.05e-02  5.80e-03  1.09e-02\n",
      "  1.77e-02 -2.80e-03 -5.40e-03 -1.15e-02  1.88e-02 -7.40e-03  5.10e-03\n",
      " -7.50e-03 -1.08e-02  1.90e-03 -1.69e-02 -2.59e-02 -2.00e-04  1.33e-02\n",
      " -1.45e-02 -1.20e-02  7.00e-04 -1.25e-02 -1.00e-03 -5.10e-03 -1.00e-04\n",
      "  8.00e-04  1.90e-02 -2.53e-02 -5.60e-03  3.90e-03 -8.60e-03  6.60e-03\n",
      " -7.80e-03  7.60e-03 -2.16e-02  3.26e-02  9.00e-04 -2.86e-02  5.90e-03\n",
      "  2.42e-02  5.70e-03  7.90e-03 -1.90e-03  4.70e-03  2.42e-02  1.34e-02\n",
      " -2.47e-02  4.00e-03  1.70e-03 -8.90e-03 -5.00e-04 -2.20e-03 -1.90e-03\n",
      " -6.70e-03  1.78e-02  1.06e-02 -3.48e-02 -6.63e-02 -2.00e-04  1.92e-02\n",
      " -3.08e-02  7.30e-03 -2.00e-02 -2.32e-02 -1.14e-02  1.00e-04 -7.60e-03\n",
      "  7.30e-03  0.00e+00  5.40e-03  8.80e-03 -2.80e-03  1.89e-02  2.37e-02\n",
      " -2.98e-02  2.50e-03  1.90e-03 -2.70e-02 -2.60e-03  2.39e-02 -1.43e-02\n",
      " -2.87e-02 -1.92e-02 -1.59e-02  1.08e-02 -2.00e-03 -5.70e-03  3.12e-02\n",
      " -1.40e-03  1.94e-02  2.00e-04  2.13e-02 -1.35e-02  1.68e-02 -9.50e-03\n",
      "  3.68e-02 -2.23e-02  8.08e-02  1.15e-02  1.88e-02  1.89e-02  1.32e-02\n",
      "  1.45e-02 -1.45e-02 -2.07e-02  2.77e-02  5.90e-03 -1.35e-02  2.47e-02\n",
      " -2.02e-02 -2.29e-02 -1.39e-02 -3.14e-02  1.82e-02 -1.87e-02  1.69e-02\n",
      "  2.79e-02  9.00e-03  3.60e-02 -3.80e-03 -1.00e-04 -1.92e-02  1.81e-02\n",
      "  1.33e-02 -2.11e-02 -3.16e-02 -2.03e-02 -9.00e-04  1.14e-02  1.04e-02\n",
      " -7.06e-02  1.71e-02 -1.05e-02 -2.01e-02  1.07e-02  6.50e-03  2.60e-03\n",
      " -6.40e-03 -1.00e-04  2.30e-03 -3.40e-03  5.80e-03  5.90e-03  6.50e-03\n",
      "  1.83e-02 -2.73e-02  1.91e-02  4.00e-04  5.40e-03  2.24e-02 -1.40e-03\n",
      "  2.58e-02 -1.77e-02  1.87e-02  2.51e-02  1.51e-02 -6.00e-04 -2.50e-03\n",
      "  1.70e-03 -6.40e-03  1.27e-02 -1.41e-02  2.22e-02 -3.30e-03 -1.91e-02\n",
      " -3.92e-02  8.80e-03 -1.10e-02 -2.52e-02  1.86e-02 -7.50e-03 -3.30e-03\n",
      " -6.80e-03 -1.77e-02 -9.70e-03 -1.11e-02  2.04e-02 -1.41e-02  3.30e-03\n",
      "  4.20e-02  4.02e-02 -2.00e-02 -1.49e-02 -4.10e-03 -1.31e-02 -3.70e-03\n",
      "  3.04e-02 -2.40e-02 -8.40e-03  1.61e-02 -4.59e-02 -1.04e-02  2.55e-02\n",
      "  1.08e-02  1.71e-02  2.70e-02 -7.20e-03 -8.50e-03 -2.80e-02 -8.00e-04\n",
      "  5.00e-04 -1.95e-02  2.48e-02 -9.70e-03 -3.79e-02  2.86e-02 -1.30e-02\n",
      "  3.50e-03  3.08e-02 -2.77e-02 -9.60e-03  2.72e-02 -2.30e-03  2.30e-03\n",
      " -1.00e-04 -1.81e-02  2.80e-03 -4.13e-02  3.52e-02  2.42e-02 -2.50e-03\n",
      " -1.13e-02  5.70e-03  3.10e-03  9.60e-03  3.60e-02 -2.77e-02]\n",
      "Most similar tjalk [('Koftjalk', 0.7346436977386475), ('tsjalk', 0.6244613528251648), ('Dektsjalk', 0.5733082890510559), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.5195964574813843), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "similar_by_word tjalk [('Koftjalk', 0.7346436977386475), ('tsjalk', 0.6244613528251648), ('Dektsjalk', 0.5733082890510559), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.5195964574813843), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "similar_by_vector tjalk [('Koftjalk', 0.7346436977386475), ('tsjalk', 0.6244613528251648), ('Dektsjalk', 0.5733082890510559), ('zeil', 0.5433095097541809), ('Hektsjalk', 0.5390281081199646), ('Koftsjalk', 0.5381300449371338), ('hektsjalk', 0.5345406532287598), ('iseltsjalk', 0.527072012424469), ('seetsjalk', 0.5195964574813843), ('Iseltsjalk', 0.5129781365394592)]\n",
      "\n",
      "doesnt_match ['sneek', 'joure', 'tjalk'] tjalk\n",
      "\n",
      "similarity tjalk sneek 0.35242274\n",
      "\n",
      "n_similarity ['sneek', 'joure', 'tjalk'] ['aap', 'noot', 'mies'] 0.3253808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', fasttext_fy.vector_size)\n",
    "print('Vocabulary size =', len(fasttext_fy.key_to_index), len(fasttext_fy))\n",
    "word_idx = fasttext_fy.key_to_index[\"tjalk\"]\n",
    "print('Word index for \\\"tjalk\\\"', word_idx)\n",
    "vector = fasttext_fy.get_vector(\"tjalk\")\n",
    "print('Vector for \\\"tjalk\\\"', vector)\n",
    "\n",
    "wordA=\"tjalk\"\n",
    "wordB=\"sneek\"\n",
    "wordlistA=[\"sneek\", \"joure\", \"tjalk\"]\n",
    "wordlistB=[\"aap\", \"noot\", \"mies\"]\n",
    "print(\"Most similar\", wordA, fasttext_fy.most_similar(wordA))  # 👍\n",
    "print()\n",
    "print(\"similar_by_word\", wordA, fasttext_fy.similar_by_word(wordA))  # 👍\n",
    "print()\n",
    "print(\"similar_by_vector\", wordA, fasttext_fy.similar_by_vector(wordA))  # 👍\n",
    "print()\n",
    "print(\"doesnt_match\", wordlistA, fasttext_fy.doesnt_match(wordlistA))  # 👍\n",
    "print()\n",
    "print(\"similarity\", wordA, wordB, fasttext_fy.similarity(wordA, wordB))  # 👍\n",
    "print()\n",
    "print(\"n_similarity\", wordlistA, wordlistB, fasttext_fy.n_similarity(wordlistA, wordlistB))  # 👍\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_my_fasttext_model = \"/Users/piek/Desktop/t-ONDERWIJS/data/word-embeddings/fasttext_we_157/cc.li.300.vec\"\n",
    "fasttext_li = KeyedVectors.load_word2vec_format(path_to_my_fasttext_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 300\n",
      "Vocabulary size = 306222 306222\n",
      "Word index for \"stroat\" 240260\n",
      "Vector for \"stroat\" 240260\n",
      "[ 0.0136 -0.0423  0.0041  0.0326 -0.0135 -0.0034 -0.0137 -0.0132  0.0041\n",
      " -0.0108 -0.0007 -0.0127 -0.0194 -0.0115 -0.0003 -0.001  -0.0127 -0.0061\n",
      " -0.0295  0.0258  0.0062 -0.0114  0.0376 -0.0237  0.0093 -0.009   0.0071\n",
      " -0.015   0.0196 -0.0079 -0.0127 -0.0194  0.0105 -0.009   0.0105 -0.035\n",
      "  0.0254  0.0177 -0.0182 -0.0077 -0.0028  0.0133 -0.0063 -0.0225  0.0073\n",
      "  0.0186 -0.0089  0.0334  0.015  -0.0091 -0.0188  0.0133  0.0074  0.0185\n",
      "  0.009   0.0018  0.0021 -0.0166 -0.0023 -0.0148 -0.01   -0.0066  0.0169\n",
      " -0.0052 -0.0095 -0.0342 -0.0152  0.0082  0.0168  0.0042  0.0101  0.0535\n",
      "  0.0059 -0.0028 -0.0021  0.0243  0.0035 -0.0022  0.006  -0.0381 -0.0024\n",
      "  0.0085  0.0036 -0.003  -0.0064 -0.0116  0.0019  0.0009  0.0049  0.0268\n",
      "  0.0118  0.0047 -0.025  -0.0103 -0.0034  0.0168 -0.0158  0.0035 -0.0132\n",
      " -0.0107 -0.0058 -0.0013 -0.0123 -0.0176  0.0092  0.0178  0.0097 -0.0012\n",
      "  0.0371  0.0325 -0.0069  0.019   0.0336  0.004  -0.0067 -0.0036 -0.0057\n",
      " -0.0202  0.0366  0.0066 -0.0147  0.0099 -0.0064 -0.0088 -0.0014 -0.0194\n",
      " -0.0396  0.0277  0.0008  0.0073 -0.0039 -0.0063 -0.0141  0.0046 -0.0206\n",
      "  0.0228  0.043  -0.0133  0.0265 -0.0015  0.0016 -0.0217 -0.0035 -0.007\n",
      "  0.0114  0.0186 -0.0163  0.0091 -0.0236  0.0003 -0.002  -0.0034  0.0067\n",
      "  0.0115  0.0535  0.0137 -0.0014  0.0051  0.0034 -0.0041 -0.003   0.0268\n",
      "  0.0321 -0.0008 -0.0314  0.024   0.0094 -0.0164  0.0154  0.0046  0.0025\n",
      " -0.0333  0.0118  0.0071  0.0067  0.0211  0.0013 -0.0261 -0.0162  0.0512\n",
      "  0.0116 -0.0117  0.0003 -0.0174  0.0026  0.0093 -0.0159 -0.0243  0.0802\n",
      " -0.0066 -0.027   0.0254  0.      0.0226  0.038  -0.0082 -0.0007 -0.0116\n",
      "  0.0101  0.056   0.0059 -0.0154 -0.0055 -0.0058 -0.0024 -0.0037  0.0143\n",
      " -0.0052 -0.0059 -0.0196  0.0032 -0.0031  0.0186 -0.0177  0.0014  0.0035\n",
      "  0.0166  0.0032 -0.0599 -0.0019 -0.0058 -0.0161  0.011   0.0031  0.0094\n",
      " -0.0039 -0.002  -0.0192 -0.0071 -0.0136  0.0038 -0.0387  0.0141 -0.0023\n",
      " -0.0301 -0.013  -0.0037 -0.0134  0.0147 -0.0273 -0.0201  0.0113  0.0155\n",
      " -0.0145 -0.0098  0.0129  0.005  -0.0132 -0.0013 -0.0172  0.0036 -0.0076\n",
      "  0.0558 -0.0032 -0.004  -0.0182 -0.0261  0.0167  0.0192 -0.027   0.0093\n",
      "  0.0244 -0.0043  0.0113 -0.0005  0.0032  0.0212 -0.0039  0.0147 -0.0086\n",
      "  0.0146 -0.0112 -0.0205 -0.0086  0.001  -0.0126  0.0099 -0.0099  0.0458\n",
      " -0.0039 -0.0253 -0.0058  0.0075 -0.004  -0.0117  0.0023 -0.0342  0.0054\n",
      "  0.0656 -0.0128 -0.0183 -0.0117  0.0144  0.0027  0.0097 -0.0146 -0.0233\n",
      "  0.0021 -0.0089 -0.0194]\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', fasttext_li.vector_size)\n",
    "print('Vocabulary size =', len(fasttext_li.key_to_index), len(fasttext_li))\n",
    "word_idx = fasttext_li.key_to_index[\"stroat\"]\n",
    "print('Word index for \\\"stroat\\\"', word_idx)\n",
    "vector = fasttext_li.get_vector(\"stroat\")\n",
    "print('Vector for \\\"stroat\\\"', word_idx)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar stroat [('dörpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460541486740112), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679347038269), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274198889732361), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.611824631690979)]\n",
      "\n",
      "Most similar poet [('fine-boned', 0.5196540355682373), ('laureate', 0.4606165885925293), ('kölsje', 0.4573061466217041), ('face', 0.4414014220237732), ('pipe', 0.3875519335269928), ('Nut.', 0.38502728939056396), ('Jenniches', 0.3829471170902252), ('gepoet', 0.3810572028160095), ('zàèn', 0.37500694394111633), ('walvèsechtege', 0.3712086081504822)]\n",
      "\n",
      "Most similar loemel [('Hoemel', 0.863980233669281), ('foemel', 0.8294392228126526), ('loemelekriemèrsj', 0.8049797415733337), ('Sjtoemel', 0.7726760506629944), ('dremel', 0.7404616475105286), ('Bieemel', 0.7326872944831848), ('loemele', 0.727192759513855), ('kloemele', 0.7036440968513489), ('tasemel', 0.6926780939102173), ('Loemel', 0.683782696723938)]\n",
      "\n",
      "similar_by_word stroat [('dörpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460541486740112), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679347038269), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274198889732361), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.611824631690979)]\n",
      "\n",
      "similar_by_vector stroat [('dörpssjtroat', 0.6635913848876953), ('Heisjtroat', 0.6460541486740112), ('Nuisjtroat', 0.6408706307411194), ('stroate', 0.6325363516807556), ('Pierresjtroat', 0.6303679347038269), ('proat', 0.6282641291618347), ('stroabbwoar', 0.6275120377540588), ('Laurasjtroat', 0.6274198889732361), ('Sjifferheisjtroat', 0.6225829720497131), ('Egsjtroat', 0.611824631690979)]\n",
      "\n",
      "doesnt_match ['stroat', 'poet', 'loemel'] poet\n",
      "\n",
      "similarity stroat poet 0.054689746\n",
      "\n",
      "n_similarity ['stroat', 'poet', 'loemel'] ['aap', 'noot', 'mies'] 0.20698144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordA=\"stroat\"\n",
    "wordB=\"poet\"\n",
    "wordC=\"loemel\"\n",
    "wordlistA=[\"stroat\", \"poet\", \"loemel\"]\n",
    "wordlistB=[\"aap\", \"noot\", \"mies\"]\n",
    "print(\"Most similar\", wordA, fasttext_li.most_similar(wordA))  # 👍\n",
    "print()\n",
    "\n",
    "print(\"Most similar\", wordB, fasttext_li.most_similar(wordB))  # 👍\n",
    "print()\n",
    "\n",
    "print(\"Most similar\", wordC, fasttext_li.most_similar(wordC))  # 👍\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"similar_by_word\", wordA, fasttext_li.similar_by_word(wordA))  # 👍\n",
    "print()\n",
    "print(\"similar_by_vector\", wordA, fasttext_li.similar_by_vector(wordA))  # 👍\n",
    "print()\n",
    "print(\"doesnt_match\", wordlistA, fasttext_li.doesnt_match(wordlistA))  # 👍\n",
    "print()\n",
    "print(\"similarity\", wordA, wordB, fasttext_li.similarity(wordA, wordB))  # 👍\n",
    "print()\n",
    "print(\"n_similarity\", wordlistA, wordlistB, fasttext_li.n_similarity(wordlistA, wordlistB))  # 👍\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Links to existing models available for download\n",
    "\n",
    "Follow the links to browse available models. The sources listed below contain English models trained using different algorithms, data with different degrees of preprocessing and varying hyperparameter settings. Some resources also include models in other languages.\n",
    "\n",
    "### Large and commonly used models (English):\n",
    "\n",
    "* Google word2vec: can be downloaded from here (follow link in instructions): http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "* GloVe (trained on various corpora): https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "* FastText embeddings (Facebook): https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "* Models with different algorithms, hyperparamters, dimensions and degrees of preprocessing (e.g. dependency parsing windows):  https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hltenv",
   "language": "python",
   "name": "hltenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
