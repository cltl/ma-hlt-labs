{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 How to use GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT (Generative Pretrained Transformer) is a model trained to generate text given a preceding input (Brown et al 2020) It can do this repetitively up to a certain length, likewise generating short stories.\n",
    "\n",
    "Another generative model is T5 (Text to Text Transfer Transformer, Raffel et al. 2019). T5 models many tasks as a text generation task, ranging from plain translation, sentiment annotation, question-answering, similarity, to summarisation. Tasks are differentiated through prompt prefixes.\n",
    "\n",
    "<img src=\"T5.gif\">\n",
    "\n",
    "Although T5 often shows better performances than GPT, it is a very large model to work with. Therefore in this notebook, we look into an older model GPT2, which is smaller and publicly available.\n",
    "\n",
    "### References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n",
    "\n",
    "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683.\n",
    "\n",
    "Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\" arXiv preprint arXiv:1910.01108 (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load GPT2 from the Huggingface platform as we did before for BERT and XLM-RoBERTa as part of a pipeline. We now specify the task as **text-generation**. As the model is big, it may take a while to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2pipe = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you succesfully downloaded it, it is saved on disk in cache for futher use. The next time you load the model it will be faster from disk.\n",
    "\n",
    "You can now pass in any text as a prompt to this pipeline instance and it will complete the text according to the model. We create a list of prompts that are very similar except for the populair entity as the subject. In this way, we can test of the model also generates different texts relevant for the different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['Boris Johnson is called to justice for',\n",
    "           'Donald Trump is called to justice for', \n",
    "           'Angela Merkel is called to justice for']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Boris Johnson is called to justice for his involvement in the death of French banker Robert Frawley.\\n\\nThe BBC's political editor Mark Toner says the British leader has been questioned by the BBC about the affair.\\n\\nHe says:\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Donald Trump is called to justice for the crimes committed by foreign leaders. He continues to criticize people who didn't commit the crimes -- even after the release of all of those released from Guantanamo Bay and after the U.S. had begun detaining him\"}]\n",
      "[{'generated_text': \"Angela Merkel is called to justice for the crimes of a few weeks ago\\n\\nAs Merkel's spokeswoman reiterated this week, she's not worried about the future of Berlin and Brussels; she's about getting back to her roots as someone who stood up\"}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(gpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the stories are different for each entity but also show specific details that seem relevant for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a powerful computer GPT2 may be to big to use or too slow. Don't worry! Researchers found a way to compress large models to smaller more efficient models with alomost equal performance. Knowledge distillation is a compression technique in which a compact model is trained to reproduce the behaviour of a larger model or an ensemble of models. The distilled model is trained with a distillation loss over the soft target probabilities of the original model (Sanh et al., 2019).\n",
    "\n",
    "There is also a distilled version of GTP2 called *distilgpt2*, which is smaller (40% of the numerb of parameters) and faster and has almost equal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distilgpt2pipe = pipeline(\"text-generation\", model=\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Boris Johnson is called to justice for the women who died on Friday after being stabbed.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]\n",
      "[{'generated_text': 'Donald Trump is called to justice for his crimes'}]\n",
      "[{'generated_text': 'Angela Merkel is called to justice for migrants Read more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(distilgpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting enough it gives very different output for our prompts. DistilGPT2 has substantial less parameters (40%) and apparently represents less knowldge for the targets entities. The stories are shorter and contains less entity specific details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 for other languages than English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a GPT model from scratch is costly. You not only need a lot of data but also computer power to create such a model. An interesting alternative is to only train the vocabulary part of a model for a language and to keep the hidden layers of the English model for the contextual attention relations and capability to predict the next token embeddings.\n",
    "\n",
    "This is what was done by *de Vries and Nissime (2021)* from Groningen University for Dutch and Italian. You can read the paper for more details.\n",
    "\n",
    "References:\n",
    "\n",
    "de Vries, Wietse, and Malvina Nissim. \"As good as new. How to successfully recycle English GPT-2 to make models for other languages.\" arXiv preprint arXiv:2012.05628 (2020). https://aclanthology.org/2021.findings-acl.74.pdf\n",
    "\n",
    "See also: https://github.com/wietsedv/gpt2-recycle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the models from Huggingface as we did for the English GPT2 and generate a Dutch and Italian short story from a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutchGpt2pipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_prompts = ['Mark Rutte is ter verantwoording geroepen voor',\n",
    "           'Thierry Baudet is ter verantwoording geroepen voor', \n",
    "           'Thierry Rutte is ter verantwoording geroepen voor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Mark Rutte is ter verantwoording geroepen voor het falen van de overheid. Hij zei vorige week:\\n\"Het kabinet heeft gefaald en moet nu naar oplossingen zoeken om te voorkomen dat deze fouten kunnen worden hersteld\", aldus Rutte. \"En als die niet lukt, kan ik u verzekeren dat mijn collega\\'s nog wel iets anders moeten aanpakken.\"Premier Mark Rutte noemde in zijn toespraak een \\'onvoorstelbaar gebrek aan verantwoordelijkheid\\' onder minister Ard van der Steur (Veiligheid en Justitie). De Kamer zal volgende week beslissen of'}]\n",
      "[{'generated_text': 'Thierry Baudet is ter verantwoording geroepen voor de aanslag op een concert in Amsterdam.\\nDe PVV-leider gaf gisteren aan dat hij zich niet schuldig voelt tegenover het optreden van zijn vrouw en kinderen, maar heeft \\'de verantwoordelijkheid\\' om daar te komen wonen. Hij voegde eraan toe: \"Ik doe mijn plicht bij mensen met wie ik zelf geen contact kan hebben.\"Het incident leidde tot ophef toen ook premier Mark Rutte - die eerder dit jaar meerdere keren werd aangevallen door extreem-rechtse demonstranten - sprak over'}]\n",
      "[{'generated_text': 'Thierry Rutte is ter verantwoording geroepen voor de aanslag op een moskee in het centrum van Istanbul, waarbij twee mensen zijn gewond geraakt.\\nDe Turkse minister van Binnenlandse Zaken Ahmet Davutoglu heeft zondag aangekondigd dat er maatregelen kunnen worden genomen om verspreiding van extremistische ideeën en uitingen binnen Turkije te weren. \"Wij hebben tot dusver niet ingegrepen\", zei Erdogan tegen journalisten die vragen stellen of ze zich bedreigd voelen door aanhangers van extreem-rechtse Grijze Democracy (IGE). De IG'}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in dutch_prompts:\n",
    "    print(dutchGpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': \"Een klein kind van zeven en een half jaar, die aan de muur stond te kijken.\\n'Wat is er met jullie gebeurd? Waarom heb jij me hier niet gevraagd om naar je eigen kamer te gaan zitten?' vroeg hij op zijn horloge. 'Ik had geen idee dat ik zo snel mogelijk terug zou kunnen komen.'\\nHaar hart sloeg in haar borstkas toen ze dit zei. Ze wilde hem alleen maar vertellen hoe het ook was geweest -- of liever nog... Maar nu wist ze ineens toch niets meer\"}\n"
     ]
    }
   ],
   "source": [
    " print(dutchGpt2pipe('Een klein kind')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'Uno bambino picologizzato è stato trovato morto da un incidente stradale. La polizia municipale ha arrestato due giovani marocchini per il reato di guida e lesioni a carico del conducente della motoveicolo che, ubriachi o senza patente, si sono scontrati con i carabinieri nel corso dei primi dieci minuti dell\\'incidente. Secondo la denuncia presentata dall\\'associazione nazionale motociclisti (NSI) \"una giovane tunisino non puٍ essere coinvolto in incidenti stradali\". Le indagini hanno anche portato all\\''}\n"
     ]
    }
   ],
   "source": [
    "italianGpt2pipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-italian\")\n",
    "\n",
    "print(italianGpt2pipe('Uno bambino picolo')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
