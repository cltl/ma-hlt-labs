{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 Use generative Language Models as text generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of you have used ChatGPT from OpenAI and noticed that it can respond to your input in a very natural way. ChatGPT is build on top of GPT (Generative Pretrained Transformer) which is a model trained to generate text given a preceding input, so-called prompt (Brown et al 2020). It can do this repetitively up to a certain length, likewise generating short stories.\n",
    "\n",
    "Models such as ChatGPT, although having good performance, are by far too large model to work with locally. Therefore in this notebook, we will use smaller open source models such as Llama and Qwen which are publicly available on huggingface.co.\n",
    "\n",
    "### References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n",
    "\n",
    "OpenAI, 2023. GPT-4 Technical Report. arXiv:2303.08774\n",
    "\n",
    "Llama: Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière et al. \"Llama: Open and efficient foundation language models.\" arXiv preprint arXiv:2302.13971 (2023). [The Llama 3 Herd of Models](https://scontent-ams2-1.xx.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=t6egZJ8QdI4Q7kNvgEka7Z4&_nc_ht=scontent-ams2-1.xx&oh=00_AYBPnjp-CQn7YnUQU_P-yJATmlaN6oRuEHZ1VrXshBoBwQ&oe=66A6EB8D)\n",
    "\n",
    "Qwen: Bai, Jinze, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan et al. \"Qwen technical report.\" arXiv preprint arXiv:2309.16609 (2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative LLMs are trained to respond to prompts. The prompt is given as a list of **dict** strings defining the role of the **system** (the model) and the input given by the **user**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Bach sat down at his organ and played\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```transformers``` module can also be used to create a pipeline to generate a text as a response to prompt:\n",
    "\n",
    "```# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-1.7B-Base\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "response = pipe(messages)\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/piek/.pyenv/versions/3.10.11/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-1.7B-Base\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "response = pipe(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the pipeline modules, we will use another package ```ChatOllama``` from **langchain** that was also used for your chat before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model=\"qwen3:1.7b\"\n",
    "\n",
    "chat = ChatOllama(\n",
    "            model=model,\n",
    "            temperature= 0.1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking aloud <think>\n",
      "Okay, the user wants me to generate 10 alternative sentences by completing the sentence \"Bach sat down at his organ and played\". Let me start by understanding the original sentence. It's about Johann Sebastian Bach playing his organ. The key elements are Bach, sitting down, the organ, and playing.\n",
      "\n",
      "I need to create different sentences that maintain the structure but vary the details. Maybe change the verb tense, the object, or the context. Let me think of different ways to describe what he played. Instead of \"organ,\" maybe \"harpsichord\" or \"piano.\" Also, could vary the action, like \"composed\" instead of \"played,\" or \"sung\" instead of \"played.\" \n",
      "\n",
      "Wait, the original sentence uses \"played\" as a verb. So alternatives could include different verbs. For example, \"composed,\" \"sung,\" \"wrote,\" \"performed,\" \"played a piece,\" \"played a solo,\" etc. Also, maybe different prepositions or adjectives. \n",
      "\n",
      "Let me brainstorm some possibilities:\n",
      "\n",
      "1. Bach sat down at his organ and composed a symphony.\n",
      "2. Bach sat down at his organ and played a solo.\n",
      "3. Bach sat down at his organ and performed a piece.\n",
      "4. Bach sat down at his organ and played a fugue.\n",
      "5. Bach sat down at his organ and played a cantata.\n",
      "6. Bach sat down at his organ and played a sonata.\n",
      "7. Bach sat down at his organ and played a concerto.\n",
      "8. Bach sat down at his organ and played a fugue.\n",
      "9. Bach sat down at his organ and played a cantata.\n",
      "10. Bach sat down at his organ and played a piece.\n",
      "\n",
      "Wait, some of these are duplicates. I need to make sure they're unique. Maybe vary the types of pieces: symphony, solo, piece, fugue, cantata, sonata, concerto. Also, maybe different verbs like \"sung,\" \"wrote,\" \"composed,\" \"performed.\" \n",
      "\n",
      "Another angle: the time of day or the context. For example, \"Bach sat down at his organ and played during the evening.\" But the original doesn't mention time, so maybe keep it general. \n",
      "\n",
      "Alternatively, use different verbs. \"Bach sat down at his organ and played a piece.\" \"Bach sat down at his organ and played a fugue.\" \"Bach sat down at his organ and played a cantata.\" \n",
      "\n",
      "I think the first 10 I listed are okay, but need to check for repetition. Maybe replace \"played\" with \"sung\" or \"composed.\" For example:\n",
      "\n",
      "1. Bach sat down at his organ and composed a symphony.\n",
      "2. Bach sat down at his organ and played a solo.\n",
      "3. Bach sat down at his organ and performed a piece.\n",
      "4. Bach sat down at his organ and played a fugue.\n",
      "5. Bach sat down at his organ and played a cantata.\n",
      "6. Bach sat down at his organ and played a sonata.\n",
      "7. Bach sat down at his organ and played a concerto.\n",
      "8. Bach sat down at his organ and played a piece.\n",
      "9. Bach sat down at his organ and played a composition.\n",
      "10. Bach sat down at his organ and played a musical piece.\n",
      "\n",
      "Hmm, some of these are similar. Maybe \"played a musical piece\" is redundant. Alternatively, \"played a composition\" is better. \n",
      "\n",
      "I think that's a good list. Now, check for any duplicates and ensure each sentence is unique. Also, make sure the verbs are varied. The user might want different verbs, so including \"composed,\" \"performed,\" \"sung,\" etc. \n",
      "\n",
      "Yes, that should work. Now, present them as the 10 alternative sentences.\n",
      "</think>\n",
      "\n",
      "The final answer 1. Bach sat down at his organ and composed a symphony.  2. Bach sat down at his organ and played a solo.  3. Bach sat down at his organ and performed a piece.  4. Bach sat down at his organ and played a fugue.  5. Bach sat down at his organ and played a cantata.  6. Bach sat down at his organ and played a sonata.  7. Bach sat down at his organ and played a concerto.  8. Bach sat down at his organ and played a musical piece.  9. Bach sat down at his organ and played a composition.  10. Bach sat down at his organ and played a piece of music.\n"
     ]
    }
   ],
   "source": [
    "print(\"Thinking aloud\", think)\n",
    "print()\n",
    "print(\"The final answer\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define the client, we can not only adjust the prompt but also the temperature. Temperatures closer to 1.0 make the model become more creative, closer to 1.0 it will follow the instructions more in a strict way. So let's try again with a high temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOllama(\n",
    "            model=model,\n",
    "            temperature= 0.9,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Bach sat down at his organ and played with a fervent dedication.  2. Bach sat down at his organ and played in a graceful, flowing manner.  3. Bach sat down at his organ and played with a deep sense of solemnity.  4. Bach sat down at his organ and played for a long, mesmerizing duration.  5. Bach sat down at his organ and played during a particularly challenging performance.  6. Bach sat down at his organ and played in the presence of his devoted students.  7. Bach sat down at his organ and played as he always had, with unwavering focus.  8. Bach sat down at his organ and played with a unique, personal touch.  9. Bach sat down at his organ and played in the style of his most iconic compositions.  10. Bach sat down at his organ and played with a heartwarming, joyful expression.\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see that it got more expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set the temperature back to 0.1 and try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Beethoven sat at his organ and played.  2. Beethoven seated at his organ and played.  3. Beethoven set down at his organ and played.  4. Beethoven started playing the organ.  5. Beethoven commenced playing the organ.  6. Beethoven played the organ as he sat down.  7. Beethoven played the organ while sitting down.  8. Beethoven played the organ and sat down.  9. Beethoven played the organ, then sat down.  10. Beethoven played the organ, after sitting down.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOllama(\n",
    "            model=model,\n",
    "            temperature= 0.1,\n",
    "        )\n",
    "\n",
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven sat down at his organ and played\"}]\n",
    "\n",
    "\n",
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us switch the language of the input, as Qwen is a multilingual model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Beethoven ging achter zijn orgel zitten en speelde het werk.  2. Beethoven ging achter zijn orgel zitten en speelde het op.  3. Beethoven ging achter zijn orgel zitten en speelde het kantoor.  4. Beethoven ging achter zijn orgel zitten en speelde het licht.  5. Beethoven ging achter zijn orgel zitten en speelde het zachte.  6. Beethoven ging achter zijn orgel zitten en speelde het zacht.  7. Beethoven ging achter zijn orgel zitten en speelde het werk.  8. Beethoven ging achter zijn orgel zitten en speelde het op.  9. Beethoven ging achter zijn orgel zitten en speelde het werk.  10. Beethoven ging achter zijn orgel zitten en speelde het op.\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Generate 10 alternative sentences by completing the next input text in Dutch.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven ging achter zijn orgel zitten en speelde\"}]\n",
    "\n",
    "\n",
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions in English, completions as Dutch. You can also give the instructions in Dutch if you want or try other languages. Which languages are covered by Llama and how well? Interesing result is that Llama varied the sitting and in a few cases the organ (orgel) but never specified what Beethoven is playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Beethoven played his organ with a mix of emotion and technical skill.  2. Beethoven played the organ in a grand, dramatic style.  3. Beethoven played the organ to honor the memory of his father.  4. Beethoven played the organ while composing his symphonies.  5. Beethoven played the organ in a quiet, reflective mood.  6. Beethoven played the organ with great passion, captivating the audience.  7. Beethoven's first performance on the organ was a success.  8. Beethoven played the organ as a way to express his inner emotions.  9. Beethoven played the organ with a blend of technical mastery and artistic expression.  10. Beethoven played the organ in front of the audience, marking a historic moment in music.\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"system\", \"content\": \"Genereer 10 alternatieve zinnen door de volgende text af te maken.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Beethoven ging achter zijn orgel zitten en speelde\"}]\n",
    "\n",
    "response = chat.invoke(prompt)\n",
    "\n",
    "### We need to remove the <think></think> part from the output\n",
    "end_of_think = response.content.find(\"</think>\")\n",
    "answer = response\n",
    "if end_of_think>0:\n",
    "    think = response.content[:end_of_think+8]\n",
    "    answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Dutch instructions do make a big difference as the quality went down drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
