{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 How to use GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT (Generative Pretrained Transformer) is a model trained to generate text given a preceding input, so-called prompt (Brown et al 2020). It can do this repetitively up to a certain length, likewise generating short stories.\n",
    "\n",
    "Another generative model is T5 (Text to Text Transfer Transformer, Raffel et al. 2019). T5 models many tasks as a text generation task, ranging from plain translation, sentiment annotation, question-answering, similarity, to summarisation. Tasks are differentiated through prompt prefixes.\n",
    "\n",
    "<img src=\"T5.gif\">\n",
    "\n",
    "Models such as GPT3,4 and T5, although having good performance, are by far too large model to work with locally. Therefore in this notebook, we look into an older model GPT2, which is smaller and publicly available. It is nevertheless a generative model designed just like the others.\n",
    "\n",
    "### References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n",
    "\n",
    "OpenAI, 2023. GPT-4 Technical Report. arXiv:2303.08774\n",
    "\n",
    "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683.\n",
    "\n",
    "Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\" arXiv preprint arXiv:1910.01108 (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load GPT2 from the Huggingface platform as we did before for BERT and XLM-RoBERTa as part of a pipeline. We now specify the task as **text-generation**. As the model is big, it may take a while to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2pipe = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you succesfully downloaded it, it is saved on disk in cache for futher use. The next time you load the model it will be faster from disk.\n",
    "\n",
    "You can now pass in any text as a prompt to this pipeline instance and it will complete the text according to the model. We create a list of prompts that are very similar except for the entity as the subject. In this way, we can test if the model also generates different texts relevant for the different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = ['Boris Johnson is called to justice for',\n",
    "           'Donald Trump is called to justice for', \n",
    "           'Angela Merkel is called to justice for']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Boris Johnson is called to justice for the man who killed three pedestrians on London Bridge.\\n\\nThe Independent has launched its #FinalSay campaign to demand that voters are given a voice on the final Brexit deal.\\n\\n\\nSign our petition here'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Donald Trump is called to justice for tweeting this video, but the New York Times reports that it never was.\\n\\n\\nHere's some information about this:\\n\\nOn March 8 at 10:25 A.M., three New Yorkers were fatally shot\"}]\n",
      "[{'generated_text': 'Angela Merkel is called to justice for what she did. It\\'s hard to believe that the chancellor has done it herself and does it because it fits in with political correctness.\"\\n\\n\\nPolls suggest a second round of EU divorce talks are drawing to'}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(gpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the stories are different for each entity but also show specific details that seem relevant for each. Whether these stories are correct and factual is a different thing. Generative models do not index facts but make up facts based on word probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a powerful computer, GPT2 may be to big to use or too slow. Researchers found a way to compress large models to smaller more efficient models with almost equal performance. Knowledge distillation is a compression technique in which a smaller model is trained to reproduce the behaviour of a larger model or an ensemble of models. The distilled model is trained with a distillation loss over the soft target probabilities of the original model (Sanh et al., 2019).\n",
    "\n",
    "There is also a distilled version of GTP2 called *distilgpt2*, which is smaller (only 40% of the original parameters) and faster while it is claimed to have almost equal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distilgpt2pipe = pipeline(\"text-generation\", model=\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Boris Johnson is called to justice for the public's safety, but Mr Johnson insists that Mr Johnson should remain the only candidate to have been selected by the British Labour Party.\\n\\n\\n\\n\\n\\nHe says Boris Johnson should remain with the Conservatives\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Donald Trump is called to justice for his campaign's treatment of African Americans, according to a survey by the Democracy & Media Institute, an academic group for the conservative-leaning left-leaning nonprofit Public Policy Polling that tracks electoral outcomes across three presidential campaign\"}]\n",
      "[{'generated_text': 'Angela Merkel is called to justice for her role in the crisis.'}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(distilgpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives very output for our prompts. DistilGPT2 has substantial less parameters (40%) and apparently represents less knowledge for the targets entities. The stories are shorter and contains less entity specific details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 for other languages than English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a GPT model from scratch is costly. You not only need a lot of data but also computer power to create such a model. An interesting alternative is to only train the vocabulary part of a model for a language and to keep the hidden layers of the English model for the contextual attention relations and capability to predict the next token embeddings. You can imagine that once the words in a sentence from a language get reasonable embedding representations, similar relations will hold through the attention mechanism across these embedding representations learned from the English data.\n",
    "\n",
    "Such an apporach was followed by *de Vries and Nissim (2021)* from Groningen University for Dutch and Italian. You can read the paper for more details. In a nutshell, the hidden layers of the English model are frozen and only the lexical embeddings are trained using Dutch and Italian data using the same autoregressive self-attention approach as for English. The lexical embeddings are used to initialise the model with the input prompt and to predict the next word to be generated.\n",
    "\n",
    "References:\n",
    "\n",
    "de Vries, Wietse, and Malvina Nissim. \"As good as new. How to successfully recycle English GPT-2 to make models for other languages.\" arXiv preprint arXiv:2012.05628 (2020). https://aclanthology.org/2021.findings-acl.74.pdf\n",
    "\n",
    "See also: https://github.com/wietsedv/gpt2-recycle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the resulting GPT2 models for Dutch and Italian from Huggingface and generate a Dutch and Italian short story from a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dutchGpt2pipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dutch_prompts = ['Mark Rutte is ter verantwoording geroepen voor',\n",
    "           'Thierry Baudet is ter verantwoording geroepen voor', \n",
    "           'Thierry Rutte is ter verantwoording geroepen voor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Mark Rutte is ter verantwoording geroepen voor de aanslagen van 11 september 2001. De VVD-leider zei in een vraaggesprek met The Washington Post, \\'Ik denk dat er nu wel iets anders moet zijn.\\'\\nPremier Mark Rutte (rechts) noemt het \"een grote eer om te zien hoe we onze rechtsstaat kunnen hervormen.\" Op Twitter heeft hij nog niet gereageerd op deze kritiek. Hij twitterde vrijdag:,,We moeten terug naar die tijd toen wij al zo\\'n twintig keer zoveel mensen hadden gedood.\\'\\''}]\n",
      "[{'generated_text': \"Thierry Baudet is ter verantwoording geroepen voor het falen van de Nederlandse politiek. Hij zegt dat hij zich niet goed voelt om in een politieke partij te zijn, maar wel eens als 'de leider'.\\n'Je kunt je afvragen waarom er zo veel mensen op straat zitten', zei Baudet aan RTL 5-presentatrice Marlon Brando over haar interview met burgemeester Eberhard van der Laan: 'Ik kan me niets voorstellen wat ik die dag moet meemaken.'\\nBaudet was vorige maand na afloop van\"}]\n",
      "[{'generated_text': 'Thierry Rutte is ter verantwoording geroepen voor het gedrag van zijn medewerkers. Dat heeft hij al gezegd in interviews met NRC en The Wall Street Journal. \"Ik ben boos op mezelf, want ik heb geen idee of dat er ooit iets fout zou kunnen worden gemaakt door mensen die me niet konden vertrouwen.\"\\nRutte maakte vorige week een opmerking over \\'het gebrek aan leiderschap\\': \"We hebben de afgelopen twee jaar nog veel meer werk gedaan dan wat we hadden moeten doen om tot stand te komen bij onze'}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in dutch_prompts:\n",
    "    print(dutchGpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'Een klein kind is een van de beste. Je weet niet eens wat dat betekent, hè?\"\\nHij knikte en liet zijn hand onder haar arm glijden. \\'Ik heb het nooit gezegd.\\'\\n\"Waarom zou ik dan zo\\'n domme vrouw willen trouwen? Ik ben er toch geen maagd meer in geslaagd om voor je te leven?\\'\\nHaar hart bonsde toen ze hem aankeek. Haar lippen krulden omhoog naar voren zodat hij zich nauwelijks kon verroeren. Zijn handen gleden over haar rug heen en weer'}\n"
     ]
    }
   ],
   "source": [
    "print(dutchGpt2pipe('Een klein kind')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': \"Uno bambino picologizzato ha il diritto di scegliere tra due modi: la scelta dell'ambiente e l'acquisto del cibo. Gli Stati Uniti hanno fatto un grande passo in avanti nell'individuazione delle condizioni migliori per vivere con i bambini, grazie ad accordi bilaterali che consentono ai paesi poveri di ottenere una maggiore protezione contro gli abusi commessi dalle loro famiglie o dall'UNICEF (Organizzazione Mondiale della Salute). Le Nazioni Unite stanno facendo progressi nel riconoscimento dei diritti degli individui umani a livello nazionale ed internazionale\"}\n"
     ]
    }
   ],
   "source": [
    "italianGpt2pipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-italian\")\n",
    "\n",
    "print(italianGpt2pipe('Uno bambino picolo')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger generative models such as ChatGPT, BARD, LLAMA have seen data in many languages (although still dominantly English, 93% of the data in case of GPT3). These models can directly represent prompts as input and generate text in these languages without further training. However, research into the multilinguality of these models is ongoing and the dominance of English appears to have an impact on the language generated for non-English languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
