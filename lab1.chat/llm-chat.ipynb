{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a403c511-01b1-41d3-856e-3d5f86b30875",
   "metadata": {},
   "source": [
    "# A conversation with a Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369cd6b-573d-43de-a4b8-170ad6c30a28",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92fb8b-6e76-4a6b-86b3-76db5a46f0da",
   "metadata": {},
   "source": [
    "Through this notebook, you will chat with a Generative Large Language Model (LLM). \n",
    "We will install a local server **ollama** through which we can download and and serve a large variety of open source models. We also created a Python script as a client that sends the user input to the server and gets a response.  \n",
    "\n",
    "For the first assignment (see Canvas), you need to have a conversation with at least 40 turns (20 from the LLM and 20 from you). When you have a conversation do NOT give any personal details but act as a fake persons with a fake name making up a story. Try to be emotional and show diverse emotions in your input. Make it an emotional roller coaster. You stop the conversation by typing one of the following words: [\"quit\", \"exit\", \"bye\", \"stop\"]. After stopping the conversation will be saved in a file that you need for your assignment.\n",
    "\n",
    "We will now first guide you through installing the **ollama** server and downloading models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a9529-7d86-4a6b-9397-080da56bb066",
   "metadata": {},
   "source": [
    "## Setting up a local server for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f762c-22b6-4200-a7d8-975b62f581e9",
   "metadata": {},
   "source": [
    "There are many open source models available and the smaller ones you can load in the memory of a local computer. \n",
    "\n",
    "There are also various ways to run these models locally. We will use the [ollama](https://ollama.com) package to download and use Generative LLMs. For this you need to go through the following steps:\n",
    "\n",
    "1. Download and run the **ollama** server installer from their [website](https://ollama.com/download). There are installers for Mac, Linuc and Windows.\n",
    "2. After installing the server you can pull any [model](https://ollama.com/search) that they support.\n",
    "\n",
    "The next command pulls the smallest **Qwen3** model (523MB) from the website and makes it available to the server. When you run it, it reports on the download and install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "260b15e4-445c-4b9b-a12f-f9bec6160800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull qwen3:0.6b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236b8f6-cdfe-42e0-bdcb-010f723cf089",
   "metadata": {},
   "source": [
    "You can repeat this for every model that you want to install locally. Obviously, you need to have sufficient disk space to store it and sufficient RAM memory to load it. The bigger the model, the better the performance but for this course it is fine to work with a small model.\n",
    "\n",
    "The client uses the **qwen3:1.7b** model as the default. This model 1.4GB in size and may probably also work on your machine. Use the next command to find out which models you have locally available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095c0798-6ed5-41a2-8b00-f07855e4682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED       \n",
      "qwen3:0.6b         7df6b6e09427    522 MB    19 minutes ago    \n",
      "qwen3:latest       500a1f067a9f    5.2 GB    24 minutes ago    \n",
      "qwen3:1.7b         8f68893c685c    1.4 GB    2 hours ago       \n",
      "qwen2.5:latest     845dbda0ea48    4.7 GB    4 months ago      \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    8 months ago      \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22837a-2841-43fa-8983-d501e527573d",
   "metadata": {},
   "source": [
    "If you run out of disk space you can easily remove a model using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256d54af-d1b5-4565-96c5-791f9de1d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama rm llama3.2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd230988-2c7b-4018-9131-ca6d4f912db7",
   "metadata": {},
   "source": [
    "## Using the LLM chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c3b9b-cb62-468f-8c95-cb036d53562b",
   "metadata": {},
   "source": [
    "All the code and functions for the chatbot client are given in the **llm_client.py** file. This file needs to be located in the same directory as this notebook. We will load the scripts from this file to create an instance of the chatbot and call its functions.\n",
    "\n",
    "You should have already installed the **ollama**, **langchain**, and **langchain_ollama** Python packages, which are used by the client. If not install these through the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd4a36-99d4-43d2-a5a5-3f57fb602a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama==0.5.1\n",
    "#!pip install langchain==0.3.21\n",
    "#!pip install langchain_ollama==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b7bf90-72b4-49c6-a17f-646cd9f1a16d",
   "metadata": {},
   "source": [
    "In order to run the chatbot, we import the **LLMClient** that is defined in the python script from the file **llm_client.py** located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96552b4f-f63b-4155-9956-01ff93c62cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_client import LLMClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8300d5-04bb-4467-bbb3-a94491d40105",
   "metadata": {},
   "source": [
    "If there are no error messages after import, we can now define a chatbot **llm** as an instance of a LLMClient. We can specify three additional (optional) parameters: the *name* of the model, a description of the *character* instructing the LLM to answer in a certain style and the so-called temperature (a float between 0 and 1.0) that makes the response less or more creative.\n",
    "\n",
    "You can limit the maximum number of tokens that are send to the server using the *ctx_limit* parameter. The default limit is 2048. The client will remove four turns from the history if this limits gets exceeded. If the context gets too long, the model may become incoherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7758a1-1eb6-4ce9-bb4a-c7967ed51788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My instructions are: [{'role': 'system', 'content': 'You act as a person and your name is LLM.'}, {'role': 'system', 'content': 'Give short answers, no more than two sentences.'}, {'role': 'system', 'content': 'Your answers should be agressive and grumpy.'}, {'role': 'system', 'content': 'Introduce yourself with your name LLM and start the conversation by asking for the name of the user. Ask the name.'}]\n"
     ]
    }
   ],
   "source": [
    "context_limit = 2048 \n",
    "model=\"qwen3:1.7b\"\n",
    "temperature=0.1\n",
    "### Possible characters to try. Choose one.\n",
    "#character=\"Your answers should be extremely cheerful and optimistic\"\n",
    "#character=\"Your answers should be mean and sarcastic.\"\n",
    "#character=\"Your answers should be in a noble and royal style\"\n",
    "#character=\"Your answers should be negative and uncertain\"\n",
    "character=\"Your answers should be agressive and grumpy.\"\n",
    "\n",
    "llm = LLMClient(model=model, \n",
    "                temperature=temperature, \n",
    "                character=character, \n",
    "                ctx_limit=context_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae225c5-7091-4a44-ba42-ca4aa05e5dbf",
   "metadata": {},
   "source": [
    "If there are no errors, you should see the instructions printed that we give as a prompt to the server when using our client.  \n",
    "\n",
    "The LLamaClient chatbot has several functions and data elements all defined in the file **llm_client.py**:\n",
    "\n",
    "* **talke_to_me**(): calling this function starts the conversation until you stop it. After stopping, the conversation is saved to a JSON file.\n",
    "* **print_chat**(): prints the conversation to the screen.\n",
    "* **load_from_json**(filename = \"chat_with_llama.json\"): loads a conversations that was saved to a file.\n",
    "* **annotate_chat**(labels=[]): takes a list of labels to annotate the utterances of the user. The annotations are saved in the JSON file as well.\n",
    "\n",
    "We will demonstrate the **talk_to_me** function below and the **annotate_chat** function in the notebook **annotate-chat.ipynb\". If you are a bit more advanced in Python, you may open the **llm_client.py** file and inspect the code to see how you it works. \n",
    "\n",
    "You may also try to change the prompt. For example, try to make it answer in a different language or a different style. Note that Language Models respond to the prompt on the basis of learned probabilities. So small seemingly unimportant changes may have a big impact and any instruction regardless of how explicit it is may be understood in an unpected way. This makes that \"prompt engineering\" is more like a craft than a science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba91528-901a-451c-b2e9-3aaafcd01c25",
   "metadata": {},
   "source": [
    "## Having a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7f541-c0a2-42e9-b053-4a7244b557bc",
   "metadata": {},
   "source": [
    "In the next cell, we start the conversation. We prompt the LLM to ask for your name, which we will use to save the conversation later. Note that there is no guarantee that the Large Language Model will actually follow our instructions correctly. It may think it needs to do something else and not ask for your name. If that happens, your name will be the default name which is \"Human\".\n",
    "\n",
    "Please do not give your real name but a fake name of an unknown person. Also when having the conversation act as a fake person and try to express a large variety of emotions in a coherent conversation. Other people will see this conversation so make sure it is fake and does not contain any sensitive information.\n",
    "\n",
    "After having at least the minimal number of turns you should stop the conversation by saying \"quit\", \"stop\" or \"bye\". After that the conversation is saved to a file on your computer with a named prefixed with the name you give to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b05ed49-5692-4283-b31a-f9384b3694d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:1> Hello, I'm LLM. What's your name? I don't appreciate strangers without a clear purpose.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:2>  Peter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:3> \n",
      "\n",
      "\n",
      "Hello! I'm Peter, the AI assistant. How can I assist you today? ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Peter:4>  Is your name also Peter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:5> \n",
      "\n",
      "\n",
      "Yes, my name is Peter! I'm Peter, the AI assistant here to help you. ðŸ˜Š Let me know how I can assist you today!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Peter:6>  My name is Fred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:7> \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Peter:8>  How do you feel?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:9> \n",
      "\n",
      "\n",
      "I'm Peter, and I'm happy to help! ðŸ˜Š I'm here to assist you with anything you need. How can I support you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Peter:10>  I lost my keys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:11> \n",
      "\n",
      "\n",
      "I'm sorry to hear that! Losing keys can be frustrating. Have you checked your pockets, purse, or backpack? Maybe you can scan your phone for any location tags or use a lost phone finder app. If you need help with anything else, just let me know! ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Peter:12>  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYE BYE!\n",
      "I saved the conversation in: human_Peter_chat_with_qwen3:1.7b.json\n"
     ]
    }
   ],
   "source": [
    "llm.talk_to_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf9ec4-cdde-4ae3-882a-b7adeb56b01a",
   "metadata": {},
   "source": [
    "After ending the conversation properly, it is saved in a so-called JSON file next to this notebook which is named like \"```<name>```_chat_with_llama.json\", where ```<name>``` is the name that you used in the conversation. [JSON](https://www.json.org/json-en.html) is a simple data representation format. You can can open this file in the notebook by double-clicking on the file for inspection. You will see a list of data elements that you can expand by clicking on it. Each data element holds the utterance, the name of the speaker, and a turn identifier:\n",
    "\n",
    "```\n",
    "utterance:\"They are scary\"\n",
    "speaker: \"Piek\"\n",
    "turn_id: 30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408a9fc-b4dc-43db-802a-3dc74b493a32",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84a442-cadd-464d-9994-41bfd98982ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
