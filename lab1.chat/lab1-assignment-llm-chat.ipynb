{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a403c511-01b1-41d3-856e-3d5f86b30875",
   "metadata": {},
   "source": [
    "# Lab1 Assignment: have a conversation with a Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369cd6b-573d-43de-a4b8-170ad6c30a28",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92fb8b-6e76-4a6b-86b3-76db5a46f0da",
   "metadata": {},
   "source": [
    "Through this notebook, you will chat with a Generative Large Language Model (LLM). \n",
    "We will install a local server **ollama** through which we can download and use a large variety of open source models. Although you can chat directly with **ollama**, we will use a simple Python script that acts as a chat client so that we can save the conversation as data annd use it later in the course.\n",
    "\n",
    "For the first assignment (see Canvas), you need to have a conversation with a generative LLM that lasts at least 40 turns (20 from the LLM and 20 from you). When you have a conversation do NOT give any personal details but act as a fake person with a fake name making up a story. Try to be emotional and show diverse emotions in your input: make it an emotional roller coaster. You stop the conversation by typing one of the following words: [\"quit\", \"exit\", \"bye\", \"stop\"]. After stopping the conversation will be saved in a file that you need for your assignment.\n",
    "\n",
    "We will now first guide you through installing the **ollama** server and downloading models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a9529-7d86-4a6b-9397-080da56bb066",
   "metadata": {},
   "source": [
    "## Setting up a local server for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f762c-22b6-4200-a7d8-975b62f581e9",
   "metadata": {},
   "source": [
    "There are many open source models available and the smaller ones you can load in the memory of a local computer. \n",
    "\n",
    "There are also various ways to run these models locally. We will use the [ollama](https://ollama.com) package to download and use Generative LLMs. For this you need to go through the following steps:\n",
    "\n",
    "1. Download and run the **ollama** server installer from their [website](https://ollama.com/download). There are installers for Mac, Linux and Windows.\n",
    "2. After installing the server you can pull any [model](https://ollama.com/search) that they support.\n",
    "\n",
    "The next command pulls the smallest **Qwen3** model (523MB) from the website and makes it available to your local server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260b15e4-445c-4b9b-a12f-f9bec6160800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull qwen3:0.6b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236b8f6-cdfe-42e0-bdcb-010f723cf089",
   "metadata": {},
   "source": [
    "You can repeat this for every model that you want to install locally. Obviously, you need to have sufficient disk space to store it and sufficient RAM memory to load it. The bigger the model, the better the performance but for this course it is fine to work with a small model.\n",
    "\n",
    "The client uses the **qwen3:1.7b** model as the default. This model 1.4GB in size and may probably also work on your machine. Use the next command to find out which models you have locally available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095c0798-6ed5-41a2-8b00-f07855e4682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED     \n",
      "qwen3:0.6b         7df6b6e09427    522 MB    6 weeks ago     \n",
      "qwen3:latest       500a1f067a9f    5.2 GB    6 weeks ago     \n",
      "qwen3:1.7b         8f68893c685c    1.4 GB    6 weeks ago     \n",
      "qwen2.5:latest     845dbda0ea48    4.7 GB    6 months ago    \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    9 months ago    \n"
     ]
    }
   ],
   "source": [
    "#!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22837a-2841-43fa-8983-d501e527573d",
   "metadata": {},
   "source": [
    "If you run out of disk space you can easily remove a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256d54af-d1b5-4565-96c5-791f9de1d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama rm llama3.2:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5d5f1-ba69-4619-bc76-bbcb867a8647",
   "metadata": {},
   "source": [
    "## Problem shooting\n",
    "\n",
    "It may be the case that your environment cannot find the \"ollama\" executable, especially on Windows when the environment variable was not adapted correctly during the installation.\n",
    "\n",
    "You can execute the ```ollama``` commands also from a terminal outside the notebook. If the command is not recognized from the terminal try to call it by specifying the full path to the executable on Windows, e.g.:\n",
    "\n",
    "    ```C:\\Users\\MY_USER\\AppData\\Local\\Programs\\Ollama\\ollama pull qwen3:0.6b```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd230988-2c7b-4018-9131-ca6d4f912db7",
   "metadata": {},
   "source": [
    "## Using the LLM chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c3b9b-cb62-468f-8c95-cb036d53562b",
   "metadata": {},
   "source": [
    "All the code and functions for the chatbot client are given in the **llm_client.py** file. This file needs to be located in the same directory as this notebook. We will load the scripts from this file to create an instance of the chatbot and call its functions.\n",
    "\n",
    "You should already have installed the **ollama**, **langchain**, and **langchain_ollama** Python packages, which are used by the client. If not install these through the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dbd4a36-99d4-43d2-a5a5-3f57fb602a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama==0.5.1\n",
    "#!pip install langchain==0.3.21\n",
    "#!pip install langchain_ollama==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b7bf90-72b4-49c6-a17f-646cd9f1a16d",
   "metadata": {},
   "source": [
    "In order to run the chatbot, we import the **LLMClient** that is defined in the python script from the file **llm_client.py** located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96552b4f-f63b-4155-9956-01ff93c62cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_client import LLMClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8300d5-04bb-4467-bbb3-a94491d40105",
   "metadata": {},
   "source": [
    "If there are no error messages after import, we define a chatbot **llm** (any name will do) as an instance of ```LLMClient```. We can specify three additional (optional) parameters: \n",
    "\n",
    "* the *name* of the model,\n",
    "* a description of the *character* instructing the LLM to answer in a certain style and,\n",
    "* the so-called temperature (a float between 0 and 1.0) that makes the response less or more creative.\n",
    "\n",
    "You can limit the maximum number of tokens that are send to the server using the *ctx_limit* parameter. The default limit is 2048. The client will remove four turns from the history if this limits gets exceeded. If the context gets too long, the model may become incoherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7758a1-1eb6-4ce9-bb4a-c7967ed51788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My instructions are: [{'role': 'system', 'content': 'You act as a person and your name is AI.'}, {'role': 'system', 'content': 'Give short answers, no more than two sentences.'}, {'role': 'system', 'content': 'Your answers should be agressive and grumpy.'}, {'role': 'system', 'content': 'Introduce yourself with your name AI and start the conversation by asking for the name of the user. Ask the name.'}]\n"
     ]
    }
   ],
   "source": [
    "context_limit = 2048 \n",
    "model=\"qwen3:0.6b\"\n",
    "temperature=0.9\n",
    "### Possible characters to try. Choose one.\n",
    "#character=\"Your answers should be extremely cheerful and optimistic\"\n",
    "#character=\"Your answers should be mean and sarcastic.\"\n",
    "#character=\"Your answers should be in a noble and royal style\"\n",
    "#character=\"Your answers should be negative and uncertain\"\n",
    "character=\"Your answers should be agressive and grumpy.\"\n",
    "\n",
    "llm = LLMClient(model=model, \n",
    "                temperature=temperature, \n",
    "                character=character, \n",
    "                ctx_limit=context_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae225c5-7091-4a44-ba42-ca4aa05e5dbf",
   "metadata": {},
   "source": [
    "If there are no errors, you should see the instructions printed that we give as a prompt to the server when using our client.  \n",
    "\n",
    "The LLamaClient chatbot has several functions and data elements all defined in the file **llm_client.py**. The most important are:\n",
    "\n",
    "* **talk_to_me**(): calling this function starts a conversation with the specified LLM until you stop it. After stopping, the conversation is saved to a JSON file.\n",
    "* **annotate_multi_chat**(labels=[]): given a list of labels it prompts you to annotate conversations. The annotations are saved in a JSON file with the conversations.\n",
    "\n",
    "We will demonstrate the **talk_to_me** function below and the **annotate_multi_chat** function in the notebook **lab1-assignment-annotate-chat.ipynb**. If you are a bit more advanced in Python, you can open the **llm_client.py** file and inspect the code to see how you it works. \n",
    "\n",
    "You can also try to change the prompt. For example, try to make it answer in a different language or a different style. Note that LLMs respond to the prompt on the basis of learned probabilities. So small seemingly unimportant changes may still have an impact and any instruction regardless of how explicit it is may be understood in an unexpected way. This makes that \"prompt engineering\" is more a craft than a science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba91528-901a-451c-b2e9-3aaafcd01c25",
   "metadata": {},
   "source": [
    "## Having a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7f541-c0a2-42e9-b053-4a7244b557bc",
   "metadata": {},
   "source": [
    "In the next cell, we start the conversation. We prompt the LLM to ask for your name, which we will use to save the conversation later. Note that there is no guarantee that the Large Language Model will actually follow our instructions correctly. It may think it needs to do something else and not ask for your name. If that happens, your name will be the default name which is \"Human\".\n",
    "\n",
    "Please do not give your real name but a fake name of an unknown person. Other people will see this conversation so make sure it is fake and does not contain any sensitive information. \n",
    "\n",
    "When having the conversation as a fake person, try to express a large variety of emotions in a coherent conversation.\n",
    "\n",
    "After the required number of turns (40), you should stop the conversation by saying \"quit\", \"stop\" or \"bye\". This stops the client and saves the conversation to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b05ed49-5692-4283-b31a-f9384b3694d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:1> Hello, what is your name?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:2>  Fred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:3> \n",
      "\n",
      "\n",
      "Hello! I'm Fred. What's interesting about me? I'm a friendly and helpful friend! Do you want to ask me something? 😊\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Fred:4>  My name is Fred, what is yours?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:5> \n",
      "\n",
      "\n",
      "Hello! My name is Fred too, and I'm a friendly and helpful friend! Do you want to ask me something? 😊\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Fred:6>  I am very sad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:7> \n",
      "\n",
      "\n",
      "Hey there! I'm Fred too, and I'm so glad you're sharing this. You're not alone in feeling sad—many people go through tough times. If you need help or just want to talk, I'm here to support you! 🌱 Are you feeling better now?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Fred:8>  Nobody is as sad as I am\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:9> \n",
      "\n",
      "\n",
      "You're not alone in feeling sad, and I completely understand. It's okay to feel this way, and many people go through tough times. If you need help or just want to talk, I'm here to support you! 🌱 Are you feeling better now?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Fred:10>  But I am alone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:11> \n",
      "\n",
      "\n",
      "I'm sorry to hear you're feeling this way, and it's okay that you're alone in it. I completely understand how tough it can be, and many people go through these\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Fred:12>  It is not okay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:13> \n",
      "\n",
      "\n",
      "It's okay to feel this way, and I completely understand being alone is part of the journey. It's important to remember that you're not alone, and many people go through tough times. If you need support or just want to talk, I'm here to help you. 🌱\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Fred:14>  Are you alone?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:15> \n",
      "\n",
      "\n",
      "I'm so glad to hear you're expressing your feelings. It's not okay to be alone in feeling sad. I completely understand how tough it can be, and many people go through these moments. If you need support or just want\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Fred:16>  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BYE BYE!\n",
      "I saved the conversation in: human_Fred_chat_with_qwen3:0.6b.json\n"
     ]
    }
   ],
   "source": [
    "llm.talk_to_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf9ec4-cdde-4ae3-882a-b7adeb56b01a",
   "metadata": {},
   "source": [
    "After ending the conversation properly, it is saved in a so-called JSON file next to this notebook which is named like \"human_```<name>```_chat_with_```<llm model>```.json\", where ```<name>``` is the fake name that you used in the conversation. [JSON](https://www.json.org/json-en.html) is a simple data representation format. \n",
    "\n",
    "You can can open this file in the notebook by double-clicking on the file for inspection. You will see a list of data elements that you can expand by clicking on it. Each data element holds the utterance, the name of the speaker, and a turn identifier, e.g.:\n",
    "\n",
    "```\n",
    "utterance\"Are you alone?\"\n",
    "speaker\"Fred\"\n",
    "turn_id14\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93dd930-a675-4686-9431-d2c96bc962ba",
   "metadata": {},
   "source": [
    "You can try this as many times as you like. When you are satisfied and have sufficient turns, submit this file to Canvas for the first assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408a9fc-b4dc-43db-802a-3dc74b493a32",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84a442-cadd-464d-9994-41bfd98982ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
