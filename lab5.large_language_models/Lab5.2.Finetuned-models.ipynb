{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3 Use fine-tuned crosslingual transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer models such as BERT and RoBERTa can easily be fine-tuned for downstream tasks. The Huggingface hub lists many of these models already trained for specific tasks. New fine-tuned transformer models are published regularly on the huggingface platform: https://huggingface.co/models\n",
    "\n",
    "In this notebook, we show two examples of fine-tuned models for xlm-roberta. Because the language model is cross-lingual, also the fine-tuned model works for all the 100 languages that xlm-roberta supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pipelines for transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface transfomers provides an option to create an **pipeline** to perform a specific NLP task with a pretrained model:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "The pipelines are abstractions from specific tasks such as sentiment-analysis and entity recognition. In the case of sentiment-analysis, the complete sentence representation of the model is taken as the input and classified with the defined labels. The sentence information is packaged in the special ```[CLS]``` token that was trained in BERT for next sentence prediction. Alternatively, token embeddings of sentences can be summed and averaged,\n",
    "\n",
    "In the case of entity recognition, each token in a sentence is classified separately in a sequence, i.e. a sequence labelling or token classification task. Whether a finetuned model can be used for a specific task depends on the way it was fine-tuned with labeled data, i.e. what type of classification head as added to the model. \n",
    "\n",
    "In this notebook, we will demonstrate two differently fine-tuned models. We will use the ```sentiment-analysis``` pipeline to demonstrate text classification and the ```ner``` pipeline to demonstrate token classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Very Positive', 'score': 0.5586308240890503}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the classification pipeline with the specified model\n",
    "sentiment_task = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")\n",
    "\n",
    "# Classify a new sentence\n",
    "sentence = \"I love this product! It's amazing and works perfectly.\"\n",
    "result = pipe(sentence)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search on the Model Hub of Huggingface for a fine-tuned model for sentiment classification. This is one of the many we find that works for different languges:\n",
    "\n",
    "https://huggingface.co/tabularisai/multilingual-sentiment-analysis\n",
    "\n",
    "The model is exclusively trained on synthetic multilingual data generated by advanced LLMs.\n",
    "\n",
    "Languages covered are: English plus Chinese (中文), Spanish (Español), Hindi (हिन्दी), Arabic (العربية), Bengali (বাংলা), Portuguese (Português), Russian (Русский), Japanese (日本語), German (Deutsch), Malay (Bahasa Melayu), Telugu (తెలుగు), Vietnamese (Tiếng Việt), Korean (한국어), French (Français), Turkish (Türkçe), Italian (Italiano), Polish (Polski), Ukrainian (Українська), Tagalog, Dutch (Nederlands), Swiss German (Schweizerdeutsch), and Swahili.\n",
    "\n",
    "Sentiment analysis is modeled as text classification. This means that the label is associated with a text as a whole and not to individual tokens (as is done for sequence classification). When using the model, we need to choose the same classification type as was set for training. Since we will use the **pipeline** API to the transformer models, we need to select a pipeline name that matches the training settings. You can check the Huggingface model card and training configuration of the model to check the details.\n",
    "\n",
    "In this case, this is easy as the pipeline name is also called \"sentiment-analysis\". We thus initialise a sentiment_task module by the **pipeline** constructor by giving it the task name \"sentiment-analysis\" and the name of the model on the Huggingface site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is loaded, you can pass in any text into the **sentiment_task** instance of the model to get the prediction. You can try out any of the listed languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Very Negative', 'score': 0.5752831697463989}]\n",
      "[{'label': 'Very Positive', 'score': 0.48439693450927734}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_task(\"What an awful movie!\"))\n",
    "print(sentiment_task(\"Wat een waardeloze film!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the documentation of the model, you see that you can also use the model without a pipeline. You need to load the model and the tokenizer separately. Next when you give it a text, you first need to tokenise the text and feed the tokens to the model. The probabilities from the classification head are mapped to a sentiment_map dictionary to get a readible output.\n",
    "\n",
    "The next code from the Huggingface site demonstrates this. We will not go further into the details of the parameters and the other functions you can call. We leave this to the machine learning course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: What an awful movie!\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: Wat een waardeloze film!\n",
      "Sentiment: Very Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]\n",
    "\n",
    "texts = [\"What an awful movie!\", \"Wat een waardeloze film!\"]\n",
    "for text, sentiment in zip(texts, predict_sentiment(texts)):\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity-recognition and classification (NERC) is typically conceived as a sequence labelling or token classification task. This means that every word (token) in the input text will receive a label as it occurs in a sequence. In the case of NERC, the labels mark each token in a text separately as either the beginning of a named-entity expression, being inside such an expression or being outside such an expression. This type of annotation is called **BIO** or **IOB** annotation, where B=beginning, I=inside and O=outside. In addition to the B and I tag, the type of entity is added as a suffix, e.g. B-PER=beginning of an expression that names a person, whereas B-LOC=beginning of an expression that names a location.\n",
    "\n",
    "Transformer models that are fine-tuned for NERC typically are set to do sequence labelling. To use such a model, we can create a pipeline for the task \"ner\". Always check the model card on Huggingface which pipeline task it was designed for:\n",
    "\n",
    "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4479f34ffd846c7b4d6aa3fa77aada9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dd7e0ebc7f4f778b57ee600c37c672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb77a64684446f59a93add33628412b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85753e2056d9423d8ea2b1453165ee55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180d1abadf794c5899926c7203f459a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nerc_task = pipeline(\"ner\", model=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'LABEL_1', 'score': 0.5092719, 'index': 1, 'word': '▁Na', 'start': 0, 'end': 2}\n",
      "{'entity': 'LABEL_1', 'score': 0.512619, 'index': 2, 'word': 'der', 'start': 2, 'end': 5}\n",
      "{'entity': 'LABEL_0', 'score': 0.5670998, 'index': 3, 'word': '▁Jo', 'start': 6, 'end': 8}\n",
      "{'entity': 'LABEL_1', 'score': 0.5187163, 'index': 4, 'word': 'kha', 'start': 8, 'end': 11}\n",
      "{'entity': 'LABEL_1', 'score': 0.50178516, 'index': 5, 'word': 'dar', 'start': 11, 'end': 14}\n",
      "{'entity': 'LABEL_1', 'score': 0.5449812, 'index': 6, 'word': '▁had', 'start': 15, 'end': 18}\n",
      "{'entity': 'LABEL_1', 'score': 0.58548856, 'index': 7, 'word': '▁given', 'start': 19, 'end': 24}\n",
      "{'entity': 'LABEL_1', 'score': 0.51082754, 'index': 8, 'word': '▁Syria', 'start': 25, 'end': 30}\n",
      "{'entity': 'LABEL_1', 'score': 0.6047771, 'index': 9, 'word': '▁the', 'start': 31, 'end': 34}\n",
      "{'entity': 'LABEL_1', 'score': 0.5604353, 'index': 10, 'word': '▁lead', 'start': 35, 'end': 39}\n",
      "{'entity': 'LABEL_1', 'score': 0.59024274, 'index': 11, 'word': '▁with', 'start': 40, 'end': 44}\n",
      "{'entity': 'LABEL_1', 'score': 0.5736892, 'index': 12, 'word': '▁a', 'start': 45, 'end': 46}\n",
      "{'entity': 'LABEL_0', 'score': 0.56547016, 'index': 13, 'word': '▁well', 'start': 47, 'end': 51}\n",
      "{'entity': 'LABEL_1', 'score': 0.5595753, 'index': 14, 'word': '-', 'start': 51, 'end': 52}\n",
      "{'entity': 'LABEL_1', 'score': 0.507107, 'index': 15, 'word': 's', 'start': 52, 'end': 53}\n",
      "{'entity': 'LABEL_1', 'score': 0.5011566, 'index': 16, 'word': 'truck', 'start': 53, 'end': 58}\n",
      "{'entity': 'LABEL_0', 'score': 0.50272596, 'index': 17, 'word': '▁head', 'start': 59, 'end': 63}\n",
      "{'entity': 'LABEL_1', 'score': 0.50413144, 'index': 18, 'word': 'er', 'start': 63, 'end': 65}\n",
      "{'entity': 'LABEL_1', 'score': 0.5664301, 'index': 19, 'word': '▁in', 'start': 66, 'end': 68}\n",
      "{'entity': 'LABEL_1', 'score': 0.5753393, 'index': 20, 'word': '▁the', 'start': 69, 'end': 72}\n",
      "{'entity': 'LABEL_1', 'score': 0.5328295, 'index': 21, 'word': '▁seven', 'start': 73, 'end': 78}\n",
      "{'entity': 'LABEL_1', 'score': 0.51526785, 'index': 22, 'word': 'th', 'start': 78, 'end': 80}\n",
      "{'entity': 'LABEL_0', 'score': 0.52570707, 'index': 23, 'word': '▁minute', 'start': 81, 'end': 87}\n",
      "{'entity': 'LABEL_1', 'score': 0.6018553, 'index': 24, 'word': '.', 'start': 87, 'end': 88}\n"
     ]
    }
   ],
   "source": [
    "example = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\n",
    "nerc_results = nerc_task(example)\n",
    "for result in nerc_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tokens in the output do not correspond to the words from the input. Remember that XLM-RoBERTa captures 100 languages and although the vocabulary is more than 250K items, this is by far not enough to represent all words of these languages. Therefore, the tokenizer of the model breaks down these words to smaller pieces in order to represent the complete sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'LABEL_0', 'score': 0.5145547, 'index': 1, 'word': '▁Mark', 'start': 0, 'end': 4}\n",
      "{'entity': 'LABEL_0', 'score': 0.5238967, 'index': 2, 'word': '▁Rut', 'start': 5, 'end': 8}\n",
      "{'entity': 'LABEL_0', 'score': 0.50275517, 'index': 3, 'word': 'te', 'start': 8, 'end': 10}\n",
      "{'entity': 'LABEL_0', 'score': 0.54380727, 'index': 4, 'word': '▁kon', 'start': 11, 'end': 14}\n",
      "{'entity': 'LABEL_0', 'score': 0.54584754, 'index': 5, 'word': 'digt', 'start': 14, 'end': 18}\n",
      "{'entity': 'LABEL_1', 'score': 0.51212454, 'index': 6, 'word': '▁aan', 'start': 19, 'end': 22}\n",
      "{'entity': 'LABEL_1', 'score': 0.565536, 'index': 7, 'word': '▁dat', 'start': 23, 'end': 26}\n",
      "{'entity': 'LABEL_1', 'score': 0.52824455, 'index': 8, 'word': '▁de', 'start': 27, 'end': 29}\n",
      "{'entity': 'LABEL_0', 'score': 0.5462488, 'index': 9, 'word': '▁VVD', 'start': 30, 'end': 33}\n",
      "{'entity': 'LABEL_1', 'score': 0.5039486, 'index': 10, 'word': '▁tech', 'start': 34, 'end': 38}\n",
      "{'entity': 'LABEL_1', 'score': 0.5374721, 'index': 11, 'word': '▁bedrijven', 'start': 39, 'end': 48}\n",
      "{'entity': 'LABEL_1', 'score': 0.52953345, 'index': 12, 'word': '▁zoals', 'start': 49, 'end': 54}\n",
      "{'entity': 'LABEL_1', 'score': 0.50240165, 'index': 13, 'word': '▁Google', 'start': 55, 'end': 61}\n",
      "{'entity': 'LABEL_1', 'score': 0.58295745, 'index': 14, 'word': ',', 'start': 61, 'end': 62}\n",
      "{'entity': 'LABEL_0', 'score': 0.5396266, 'index': 15, 'word': '▁Facebook', 'start': 63, 'end': 71}\n",
      "{'entity': 'LABEL_1', 'score': 0.55421525, 'index': 16, 'word': '▁en', 'start': 72, 'end': 74}\n",
      "{'entity': 'LABEL_0', 'score': 0.54305285, 'index': 17, 'word': '▁Apple', 'start': 75, 'end': 80}\n",
      "{'entity': 'LABEL_0', 'score': 0.53402203, 'index': 18, 'word': '▁zwaar', 'start': 81, 'end': 86}\n",
      "{'entity': 'LABEL_1', 'score': 0.50247043, 'index': 19, 'word': 'der', 'start': 86, 'end': 89}\n",
      "{'entity': 'LABEL_1', 'score': 0.572611, 'index': 20, 'word': '▁gaat', 'start': 90, 'end': 94}\n",
      "{'entity': 'LABEL_0', 'score': 0.53937477, 'index': 21, 'word': '▁belast', 'start': 95, 'end': 101}\n",
      "{'entity': 'LABEL_0', 'score': 0.53973985, 'index': 22, 'word': 'en', 'start': 101, 'end': 103}\n",
      "{'entity': 'LABEL_1', 'score': 0.52539897, 'index': 23, 'word': '.', 'start': 103, 'end': 104}\n"
     ]
    }
   ],
   "source": [
    "example = \"Mark Rutte kondigt aan dat de VVD tech bedrijven zoals Google, Facebook en Apple zwaarder gaat belasten.\"\n",
    "nerc_results = nerc_task(example)\n",
    "for result in nerc_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hltenv",
   "language": "python",
   "name": "hltenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
