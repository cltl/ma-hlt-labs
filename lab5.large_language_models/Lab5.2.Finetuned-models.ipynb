{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3 Use fine-tuned crosslingual transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer models such as BERT and RoBERTa can easily be fine-tuned for downstream tasks. The Huggingface hub lists many of these models already trained for specific tasks. New fine-tuned transformer models are published regularly on the huggingface platform: https://huggingface.co/models\n",
    "\n",
    "In this notebook, we show two examples of fine-tuned models for xlm-roberta. Because the language model is cross-lingual, also the fine-tuned model works for all the 100 languages that xlm-roberta supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pipelines for transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface transfomers provides an option to create an **pipeline** to perform a specific NLP task with a pretrained model:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "The pipelines are abstractions from specific tasks such as sentiment-analysis and entity recognition. In the case of sentiment-analysis, the complete sentence representation of the model is taken as the input and classified with the defined labels. The sentence information is packaged in the special ```[CLS]``` token that was trained in BERT for next sentence prediction. Alternatively, token embeddings of sentences can be summed and averaged,\n",
    "\n",
    "In the case of entity recognition, each token in a sentence is classified separately in a sequence, i.e. a sequence labelling or token classification task. Whether a finetuned model can be used for a specific task depends on the way it was fine-tuned with labeled data, i.e. what type of classification head as added to the model. \n",
    "\n",
    "In this notebook, we will demonstrate two differently fine-tuned models. We will use the ```sentiment-analysis``` pipeline to demonstrate text classification and the ```ner``` pipeline to demonstrate token classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search on the Model Hub of Huggingface for a fine-tuned xlm-roberta model for sentiment classification, e.g.:\n",
    "\n",
    "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment\n",
    "\n",
    "This model was trained on ~198M tweets and finetuned for sentiment analysis. According to the Huggingface model card text:\n",
    "\n",
    "\"The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\"\n",
    "\n",
    "    Barbieri, Francesco, Luis Espinosa Anke, and Jose Camacho-Collados. \"Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond.\" arXiv preprint arXiv:2104.12250 (2021).\n",
    "\n",
    "The cross-lingual capabilities come from XLM-roberta that has a vocabulary for 100 languages and represents texts in any of these in a language-agnostic way. Fine-tuning it with example in the 8 languages transfers to all 100 languages.\n",
    "\n",
    "Sentiment analysis is modeled as text classification. This means that the label is associated with a text as a whole and not to individual tokens (as is done for sequence classification). When using the model, we need to choose the same classification type as was set for training. Since we will use the **pipeline** API to the transformer models, we need to select a pipeline name that matches the training settings. You can check the Huggingface model card and training configuration of the model to check the details.\n",
    "\n",
    "In this case, this is easy as the pipeline name is also called \"sentiment-analysis\". We thus initialise a sentiment_task module by the **pipeline** constructor by giving it the task name \"sentiment-analysis\" and the name of the model on the Huggingface site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp311-cp311-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1587\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1586\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1730\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1726\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1727\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1728\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1729\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     ).converted()\n\u001b[32m   1731\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     tokenizer = \u001b[38;5;28mself\u001b[39m.tokenizer()\n\u001b[32m   1625\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1626\u001b[39m         [\n\u001b[32m   1627\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1628\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1629\u001b[39m         ]\n\u001b[32m   1630\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1617\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1616\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1617\u001b[39m     vocab_scores, merges = \u001b[38;5;28mself\u001b[39m.extract_vocab_merges_from_model(\u001b[38;5;28mself\u001b[39m.vocab_file)\n\u001b[32m   1618\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1589\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1590\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1591\u001b[39m     )\n\u001b[32m   1593\u001b[39m bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[31mValueError\u001b[39m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2302\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2301\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m   2303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001b[39m, in \u001b[36mXLMRobertaTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m mask_token = AddedToken(mask_token, lstrip=\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    109\u001b[39m     vocab_file,\n\u001b[32m    110\u001b[39m     tokenizer_file=tokenizer_file,\n\u001b[32m    111\u001b[39m     bos_token=bos_token,\n\u001b[32m    112\u001b[39m     eos_token=eos_token,\n\u001b[32m    113\u001b[39m     sep_token=sep_token,\n\u001b[32m    114\u001b[39m     cls_token=cls_token,\n\u001b[32m    115\u001b[39m     unk_token=unk_token,\n\u001b[32m    116\u001b[39m     pad_token=pad_token,\n\u001b[32m    117\u001b[39m     mask_token=mask_token,\n\u001b[32m    118\u001b[39m     **kwargs,\n\u001b[32m    119\u001b[39m )\n\u001b[32m    121\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m fast_tokenizer = convert_slow_tokenizer(\u001b[38;5;28mself\u001b[39m, from_tiktoken=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    140\u001b[39m slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1732\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1731\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1732\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1733\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1734\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1735\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1736\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      2\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sentiment_task = pipeline(\u001b[33m\"\u001b[39m\u001b[33msentiment-analysis\u001b[39m\u001b[33m\"\u001b[39m, model=model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/pipelines/__init__.py:1049\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1046\u001b[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001b[32m   1047\u001b[39m             tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m         tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m   1050\u001b[39m             tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs\n\u001b[32m   1051\u001b[39m         )\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[32m   1054\u001b[39m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1028\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2062\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2059\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2060\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_pretrained(\n\u001b[32m   2063\u001b[39m     resolved_vocab_files,\n\u001b[32m   2064\u001b[39m     pretrained_model_name_or_path,\n\u001b[32m   2065\u001b[39m     init_configuration,\n\u001b[32m   2066\u001b[39m     *init_inputs,\n\u001b[32m   2067\u001b[39m     token=token,\n\u001b[32m   2068\u001b[39m     cache_dir=cache_dir,\n\u001b[32m   2069\u001b[39m     local_files_only=local_files_only,\n\u001b[32m   2070\u001b[39m     _commit_hash=commit_hash,\n\u001b[32m   2071\u001b[39m     _is_local=is_local,\n\u001b[32m   2072\u001b[39m     trust_remote_code=trust_remote_code,\n\u001b[32m   2073\u001b[39m     **kwargs,\n\u001b[32m   2074\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2303\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2301\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2302\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2304\u001b[39m     logger.info(\n\u001b[32m   2305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2307\u001b[39m     )\n\u001b[32m   2308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/hltenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:87\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is loaded, you can pass in any text into the **sentiment_task** instance of the model to get the prediction. You can try out any of the 100 languages of XLM-RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(sentiment_task(\u001b[33m\"\u001b[39m\u001b[33mWhat an awful movie!\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(sentiment_task(\u001b[33m\"\u001b[39m\u001b[33mWat een waardeloze film!\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'sentiment_task' is not defined"
     ]
    }
   ],
   "source": [
    "print(sentiment_task(\"What an awful movie!\"))\n",
    "print(sentiment_task(\"Wat een waardeloze film!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity-recognition and classification (NERC) is typically conceived as a sequence labelling or token classification task. This means that every word (token) in the input text will receive a label as it occurs in a sequence. In the case of NERC, the labels mark each token in a text separately as either the beginning of a named-entity expression, being inside such an expression or being outside such an expression. This type of annotation is called **BIO** or **IOB** annotation, where B=beginning, I=inside and O=outside. In addition to the B and I tag, the type of entity is added as a suffix, e.g. B-PER=beginning of an expression that names a person, whereas B-LOC=beginning of an expression that names a location.\n",
    "\n",
    "Transformer models that are fine-tuned for NERC typically are set to do sequence labelling. To use such a model, we can create a pipeline for the task \"ner\". Always check the model card on Huggingface which pipeline task it was designed for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nerc_task = pipeline(\"ner\", model=\"Davlan/xlm-roberta-base-ner-hrl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.9998417, 'index': 1, 'word': '▁Na', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-PER', 'score': 0.8805617, 'index': 2, 'word': 'der', 'start': 2, 'end': 5}\n",
      "{'entity': 'I-PER', 'score': 0.999816, 'index': 3, 'word': '▁Jo', 'start': 5, 'end': 8}\n",
      "{'entity': 'I-PER', 'score': 0.9998022, 'index': 4, 'word': 'kha', 'start': 8, 'end': 11}\n",
      "{'entity': 'I-PER', 'score': 0.99975294, 'index': 5, 'word': 'dar', 'start': 11, 'end': 14}\n",
      "{'entity': 'B-LOC', 'score': 0.99962485, 'index': 8, 'word': '▁Syria', 'start': 24, 'end': 30}\n"
     ]
    }
   ],
   "source": [
    "example = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\n",
    "nerc_results = nerc_task(example)\n",
    "for result in nerc_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tokens in the output do not correspond to the words from the input. Remember that XLM-RoBERTa captures 100 languages and although the vocabulary is more than 250K items, this is by far not enough to represent all words of these languages. Therefore, the tokenizer of the model breaks down these words to smaller pieces in order to represent the complete sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.9998752, 'index': 1, 'word': '▁Mark', 'start': 0, 'end': 4}\n",
      "{'entity': 'I-PER', 'score': 0.99985504, 'index': 2, 'word': '▁Rut', 'start': 4, 'end': 8}\n",
      "{'entity': 'I-PER', 'score': 0.99987614, 'index': 3, 'word': 'te', 'start': 8, 'end': 10}\n",
      "{'entity': 'B-ORG', 'score': 0.99918514, 'index': 9, 'word': '▁VVD', 'start': 29, 'end': 33}\n",
      "{'entity': 'B-ORG', 'score': 0.99986625, 'index': 13, 'word': '▁Google', 'start': 54, 'end': 61}\n",
      "{'entity': 'B-ORG', 'score': 0.999859, 'index': 15, 'word': '▁Facebook', 'start': 62, 'end': 71}\n",
      "{'entity': 'B-ORG', 'score': 0.9998468, 'index': 17, 'word': '▁Apple', 'start': 74, 'end': 80}\n"
     ]
    }
   ],
   "source": [
    "example = \"Mark Rutte kondigt aan dat de VVD tech bedrijven zoals Google, Facebook en Apple zwaarder gaat belasten.\"\n",
    "nerc_results = nerc_task(example)\n",
    "for result in nerc_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hltenv",
   "language": "python",
   "name": "hltenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
