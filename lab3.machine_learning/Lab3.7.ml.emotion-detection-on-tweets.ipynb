{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.7 Testing classifiers on a different data set with tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set that we are going to use is from an NLP task on emotion detection that was organised in the *Wassa* workshop in 2017. The texts are tweets and therefore a different genre than the spoken utterances from the conversations in the MELD data set:\n",
    "\n",
    "http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html\n",
    "\n",
    "We included the data set in the distribution of this lab and aggregated all the training data in a single file that we now can load using *Pandas*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the tweet data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              Tweet  Label  Score\n",
       "0  10000  How the fu*k! Who the heck! moved my fridge!.....  anger  0.938\n",
       "1  10001  So my Indian Uber driver just called someone t...  anger  0.896\n",
       "2  10002  @DPD_UK I asked for my parcel to be delivered ...  anger  0.896\n",
       "3  10003  so ef whichever butt wipe pulled the fire alar...  anger  0.896\n",
       "4  10004  Don't join @BTCare they put the phone down on ...  anger  0.896"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filepath = './data/wassa/training/all.train.tsv'\n",
    "dftweets_train = pd.read_csv(filepath, sep='\\t')\n",
    "dftweets_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3613 entries, 0 to 3612\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      3613 non-null   int64  \n",
      " 1   Tweet   3613 non-null   object \n",
      " 2   Label   3613 non-null   object \n",
      " 3   Score   3613 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 113.0+ KB\n"
     ]
    }
   ],
   "source": [
    "dftweets_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this data set has 3613 tweets, labels and a score. Lets check the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dftweets_train.Label.value_counts().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Test data\n",
    "filepath = './data/wassa/testing/all.test.tsv'\n",
    "dftweets_test = pd.read_csv(filepath, sep='\\t')\n",
    "dftweets_test.Label.value_counts().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this data set is more evenly distributed and there are no neutral tweets. Also lacking are *disgust* and *surprise*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train classifiers as we did before and see what happens. We first extract the tweets and labels using a similar loop as in the previous notebook Lab3.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances 3613\n",
      "Number of training labels 3613\n"
     ]
    }
   ],
   "source": [
    "tweet_train_instances=[]\n",
    "tweet_train_labels = []\n",
    "for tweet in dftweets_train['Tweet']:\n",
    "    ### We break the loop after 2000 instances \n",
    "    #if index==2000:\n",
    "    #    break\n",
    "    tweet_train_instances.append(tweet)\n",
    "\n",
    "\n",
    "for label in dftweets_train['Label']:\n",
    "    ### We break the loop after 2000 instances \n",
    "    #if index==2000:\n",
    "    #    break    ### we need to surround the next statements with 'try' and 'except' to catch cases \n",
    "    tweet_train_labels.append(label)\n",
    "\n",
    "print('Number of training instances', len(tweet_train_instances))\n",
    "print('Number of training labels', len(tweet_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test instances 3142\n",
      "Test labels 3142\n"
     ]
    }
   ],
   "source": [
    "tweet_test_instances=[]\n",
    "tweet_test_labels = []\n",
    "for tweet in dftweets_test['Tweet']:\n",
    "    ### We break the loop after 2000 instances \n",
    "    #if index==2000:\n",
    "    #    break\n",
    "    tweet_test_instances.append(tweet)\n",
    "\n",
    "\n",
    "for label in dftweets_test['Label']:\n",
    "    ### We break the loop after 2000 instances \n",
    "    #if index==2000:\n",
    "    #    break    ### we need to surround the next statements with 'try' and 'except' to catch cases \n",
    "    tweet_test_labels.append(label)\n",
    "\n",
    "print('Test instances', len(tweet_test_instances))\n",
    "print('Test labels', len(tweet_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Representing and classifying the tweets as averaged word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Representing the tweet training data using the same word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same functions that we used in the notebook *Lab3.6.ml.emotion-detection.embeddings.ipynb* to get averaged embeddings for the tweets. \n",
    "In order to use exactly the same function and to be able to re-use them again, we copied the function to a separate python file \"lab3_util.py\". You can open the file in Jupyter to inspect its content.\n",
    "\n",
    "We can now import this file as we do with other packages and apply it in this notebook but also in other code. This keeps our notebook readable and compact and makes sure we always use the same functions and do not accidently change them across notebooks.\n",
    "\n",
    "For future coding, it is wise to also apply this to your own code. Put reusable code as functions in a separate Python file with an illegible name and import this in different notebooks or other Python files. In this way, you develop your own tools over time and reuse them when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the Python file *lab3_util.py* in this notebook so that the functions are loaded in working memory. The file should be located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab3_util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also import the other 3rd party packages as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load the same word embedding model as we used before to get compatible word embeddings for this data. Choose whatever you used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "# download the model and return as object ready for use\n",
    "wordembeddings=\"glove-twitter-25\"\n",
    "word_embedding_model = api.load(wordembeddings)\n",
    "num_features = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now directly call the functions defined in *lab3_utility.py* through the instantiation *util*, e.g. \"util.getMostFrequentWords\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of words above the frequency threshold 1447\n",
      "Frequency threshold 4\n",
      "Shape of our matrix is: (3613, 25)\n",
      "Review 0 of 3613\n",
      "Review 1000 of 3613\n",
      "Review 2000 of 3613\n",
      "Review 3000 of 3613\n"
     ]
    }
   ],
   "source": [
    "#### We turn the word2vec wordindex into a set for efficiency\n",
    "index2word_set = set(word_embedding_model.wv.index2word)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    " #### We use two word list to keep track of the words that are also in the embedding model and words that are not\n",
    "\n",
    "known_words = []  # utterance words that are also in the embedding model\n",
    "unknown_words =[] # utterance words that are not in the mebedding model\n",
    "\n",
    "frequency_threshold=4\n",
    "\n",
    "#Here we call a function defined in lab3_util.py!!\n",
    "keywords = util.getMostFrequentWords(frequency_threshold, tweet_train_instances)\n",
    "\n",
    "tweet_tokens = []\n",
    "for tweet in tweet_train_instances:\n",
    "    tweet_tokens.append(nltk.tokenize.word_tokenize(tweet))\n",
    "    \n",
    "#Again a function from lab3_util.py\n",
    "trainDataVecs, known_words, unknown_words = util.getAvgFeatureVecs(tweet_tokens, \n",
    "                                                                             keywords, \n",
    "                                                                             stop_words, \n",
    "                                                                             word_embedding_model, \n",
    "                                                                             index2word_set, \n",
    "                                                                             num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we check which words are not in the embedding model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown word types 47\n",
      "Number of unknown word tokens: 870\n",
      "Unknown words counts\n",
      "Counter({'...': 275, '2': 59, 'live.ly': 54, '3': 38, '\\\\n': 29, 'ðŸ˜‚': 24, '4': 21, 'realdonaldtrump': 21, '..': 21, '\\\\n\\\\n': 18, 'bb18': 16, '1': 16, 'hillaryclinton': 16, '15': 14, '10': 13, 'ðŸ™„': 13, '5': 11, '6': 11, '2016': 10, \"y'all\": 10, 'ðŸ˜­': 10, '20': 10, 'ðŸ˜‚ðŸ˜‚': 9, '7': 9, 'ðŸ˜•': 8, '100': 8, 'â¤ï¸': 8, 'ðŸ˜¡': 7, 'w/': 7, '12': 7, '8': 7, '70': 7, 'charlotteprotest': 6, 'ðŸ˜©': 6, '3rd': 6, 'mhchat': 6, 'ðŸ˜¢': 6, 'ðŸ˜³': 6, 'wch2016': 6, 'thenicebot': 6, 'ðŸ¤—': 5, 'ðŸ™ƒ': 5, '17': 5, 'ðŸ˜‚ðŸ˜‚ðŸ˜‚': 5, '2day': 5, 'ðŸ˜…': 5, '48': 5})\n",
      "Proportion of unknown tokens 0.024041782960731756\n",
      "Number of known word types: 1205\n",
      "Number of known word tokens: 35317\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "if len(unknown_words)>0:\n",
    "    unknown_words_count = Counter(unknown_words)\n",
    "    print('Number of unknown word types',len(unknown_words_count))\n",
    "    print('Number of unknown word tokens:', len(unknown_words))\n",
    "    print('Unknown words counts')\n",
    "    print(unknown_words_count)\n",
    "    print('Proportion of unknown tokens', len(unknown_words)/(len(unknown_words)+len(known_words)))\n",
    "\n",
    "\n",
    "known_words_count = Counter(known_words)\n",
    "print('Number of known word types:',len(known_words_count))\n",
    "print('Number of known word tokens:', len(known_words))\n",
    "#print(known_words_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that emoticons are not in the vocabulary of the embedding model. Most words are however covered. Discuss in your group how you could fix this and report in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Representing the tweet test data through embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our matrix is: (3142, 25)\n",
      "Review 0 of 3142\n",
      "Review 1000 of 3142\n",
      "Review 2000 of 3142\n",
      "Review 3000 of 3142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweet_tokens = []\n",
    "for tweet in tweet_test_instances:\n",
    "    tweet_tokens.append(nltk.tokenize.word_tokenize(tweet))\n",
    "\n",
    "testDataVecs, known_words, unknown_words = util.getAvgFeatureVecs(tweet_tokens, \n",
    "                                                                             keywords, \n",
    "                                                                             stop_words, \n",
    "                                                                             word_embedding_model, \n",
    "                                                                             index2word_set, \n",
    "                                                                             num_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown word types 45\n",
      "Number of unknown word tokens: 748\n",
      "Unknown words counts\n",
      "Counter({'...': 301, '2': 67, 'ðŸ˜‚': 32, '3': 26, '1': 23, '\\\\n': 22, '4': 20, 'bb18': 17, '6': 16, '20': 14, '..': 13, '5': 13, 'realdonaldtrump': 12, 'hillaryclinton': 12, '10': 11, \"y'all\": 10, '15': 10, 'mhchat': 10, '7': 9, 'ðŸ˜‚ðŸ˜‚': 9, 'ðŸ™ƒ': 7, '100': 7, 'ðŸ˜©': 7, 'ðŸ˜­': 7, '\\\\n\\\\n': 7, '12': 7, '2016': 6, 'ðŸ˜…': 5, 'ðŸ™„': 5, 'ðŸ˜³': 5, 'w/': 4, '17': 4, '8': 3, 'ðŸ˜¡': 3, 'ðŸ˜¢': 3, 'â¤ï¸': 3, 'charlotteprotest': 3, 'ðŸ˜•': 3, '3rd': 3, 'ðŸ˜‚ðŸ˜‚ðŸ˜‚': 2, '48': 2, 'thenicebot': 2, 'live.ly': 1, 'wch2016': 1, '70': 1})\n",
      "Proportion of unknown tokens 0.02481340189086084\n",
      "Number of known word types: 1139\n",
      "Number of known word tokens: 29397\n"
     ]
    }
   ],
   "source": [
    "if len(unknown_words)>0:\n",
    "    unknown_words_count = Counter(unknown_words)\n",
    "    print('Number of unknown word types',len(unknown_words_count))\n",
    "    print('Number of unknown word tokens:', len(unknown_words))\n",
    "    print('Unknown words counts')\n",
    "    print(unknown_words_count)\n",
    "    print('Proportion of unknown tokens', len(unknown_words)/(len(unknown_words)+len(known_words)))\n",
    "\n",
    "\n",
    "known_words_count = Counter(known_words)\n",
    "print('Number of known word types:',len(known_words_count))\n",
    "print('Number of known word tokens:', len(known_words))\n",
    "#print(known_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unknown words list is a good sanity check. You need to know what you are dealing with and if there are no unexpected differences between train and test. All seems good in this case. Similar words are listed in the top frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we could to convert the labels to the same numeric valus that we used before with our LabelEncoder. However, the set of labels is not the same and, if we do it, we want to keep the numeric mapping the same for those that map. The next code does this, but you can also skip this and use the string labels while eye balling the results across the notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'fear', 'joy', 'sadness']\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# first we instantiate a label encode\n",
    "le = preprocessing.LabelEncoder()\n",
    "# we feed this encoder with the complete list of labels from our data\n",
    "labels = ['fear', 'anger', 'joy',  'sadness']\n",
    "le.fit(labels)\n",
    "print(list(le.classes_))\n",
    "tweet_train_classes = le.transform(tweet_train_labels)\n",
    "tweet_test_classes = le.transform(tweet_test_labels)\n",
    "\n",
    "print(list(tweet_train_classes[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first set of tweets are all expressing *anger* (idex '0') which makes sense given the way the data were concatenated from the separate files with tweets. Let's check the last 10 tweets, which should score for *sadness*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(list(tweet_train_classes[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training and testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'fear' 'joy' 'sadness']\n",
      "Tweet Embeddings SVM LINEAR ----------------------------------------------------------------\n",
      "Word embedding model used glove-twitter-25\n",
      "Word frequency threshold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.4760077 0.3263158 0.3871975       760\n",
      "           1  0.4489281 0.7155779 0.5517241       995\n",
      "           2  0.5636605 0.5952381 0.5790191       714\n",
      "           3  0.4590747 0.1916790 0.2704403       673\n",
      "\n",
      "    accuracy                      0.4818587      3142\n",
      "   macro avg  0.4869178 0.4572027 0.4470952      3142\n",
      "weighted avg  0.4837238 0.4818587 0.4578808      3142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# We choose a Linear model\n",
    "svm_linear_clf = svm.LinearSVC(max_iter=2000)\n",
    "### we train the classifier through the *fit* function and by passing the training vectors and the training labels as paramters:\n",
    "svm_linear_clf.fit(trainDataVecs, tweet_train_classes)\n",
    "\n",
    "# Predicting the Test set results, find macro recall\n",
    "y_pred_svm_linear = svm_linear_clf.predict(testDataVecs)\n",
    "\n",
    "#### this report gives the results for the LINEAR classifier\n",
    "report = classification_report(tweet_test_classes,y_pred_svm_linear,digits = 7)\n",
    "print(le.classes_)\n",
    "print('Tweet Embeddings SVM LINEAR ----------------------------------------------------------------')\n",
    "print('Word embedding model used', wordembeddings)\n",
    "print('Word frequency threshold', frequency_threshold)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we only have test results for the 4 classes because there are no tweet data for the others. The results are not that bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Testing the MELD classifiers on the tweets test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now trained and tested an emotion classifier on tweet data. What about the MELD classifiers that we built before and saved to disk? Can we load these and test them on the same test tweet data set?\n",
    "\n",
    "With embeddings this is easy because the representations are independent of the words but we need to use the same embedding model for both the training and testing. So if we used the *glove_twitter* embeddings for MELD with 25 dimensions, we have to do the same for the tweet test data.\n",
    "\n",
    "For the BoW classifiers, we need to use CountVectorizer and the *transform_fit* function to model the test data accordingly.\n",
    "\n",
    "If you saved the MELD models using the previous notebook, you can load the models now again. Otherwise, you need to rebuilt them, save them and next load them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Loading the MELD classifier for embedding representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename_classifier = './models/svm_linear_clf_embeddings.sav'\n",
    "\n",
    "# load the classifier and the vectorizer from disk\n",
    "svm_linear_clf = pickle.load(open(filename_classifier, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the vector representations for MELD and the test tweets were created using the same function and through the same embedding model we can now use it directly to classify the tweet test data we just created in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'fear' 'joy' 'sadness']\n",
      "SVM NONLINEAR EMBEDDINGS ----------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.4561404 0.0342105 0.0636475       760\n",
      "           1  0.0000000 0.0000000 0.0000000       995\n",
      "           2  0.0000000 0.0000000 0.0000000       714\n",
      "           3  0.2063492 0.2511144 0.2265416       673\n",
      "           4  0.0000000 0.0000000 0.0000000         0\n",
      "           5  0.0000000 0.0000000 0.0000000         0\n",
      "           6  0.0000000 0.0000000 0.0000000         0\n",
      "\n",
      "    accuracy                      0.0620624      3142\n",
      "   macro avg  0.0946414 0.0407607 0.0414556      3142\n",
      "weighted avg  0.1545320 0.0620624 0.0639193      3142\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# have classifier make a prediction\n",
    "tweet_pred_linear_embeddings = svm_linear_clf.predict(testDataVecs)\n",
    "report = classification_report(tweet_test_classes,tweet_pred_linear_embeddings,digits = 7)\n",
    "print(le.classes_)\n",
    "print('SVM NONLINEAR EMBEDDINGS ----------------------------------------------------------------')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the tweets data is different from MELD, we see that there is no support for *disgust*, *neutral*, and *surprise*. These are not in the data set.\n",
    "There is data for *fear* and *joy* but the results are bad with zero recall and zero precision. This clearly shows you cannot easily deploy a classifier to other types and domains of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Representing and classifying the tweets as a bag-of-words vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Representing the tweets as BoW vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to represent the test tweets according to our BoW vector representation derived from the MELD data, we need to load the *utterance_vec* file that we saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifier and the vectorizer from disk\n",
    "filename_vectorizer = './models/utterance_vec.sav'\n",
    "loaded_vectorizer = pickle.load(open(filename_vectorizer, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the transform function to the tokenized tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3142, 1217)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#utterance_tfidf = tfidf_transformer.fit_transform(utterance_counts)\n",
    "tweet_vectors = loaded_vectorizer.transform(tweet_test_instances)\n",
    "tweet_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the matrix has vectors of the same size as before for the BoW token vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the classifier and make the predictions on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier on disk\n",
    "filename_classifier = './models/svm_linear_clf_bow.sav'\n",
    "\n",
    "loaded_bow_classifier = pickle.load(open(filename_classifier, 'rb'))\n",
    "\n",
    "pred_from_loaded_bow_classifier = loaded_bow_classifier.predict(tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'fear' 'joy' 'sadness']\n",
      "SVM LINEAR BOW ----------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.3082707 0.1618421 0.2122519       760\n",
      "           1  0.3116883 0.0482412 0.0835509       995\n",
      "           2  0.1612903 0.0070028 0.0134228       714\n",
      "           3  0.2046385 0.2228826 0.2133713       673\n",
      "           4  0.0000000 0.0000000 0.0000000         0\n",
      "           5  0.0000000 0.0000000 0.0000000         0\n",
      "           6  0.0000000 0.0000000 0.0000000         0\n",
      "\n",
      "    accuracy                      0.1037556      3142\n",
      "   macro avg  0.1408411 0.0628527 0.0746567      3142\n",
      "weighted avg  0.2537551 0.1037556 0.1265523      3142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(tweet_test_classes,pred_from_loaded_bow_classifier,digits = 7)\n",
    "print(le.classes_)\n",
    "print('SVM LINEAR BOW ----------------------------------------------------------------')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the BoW SVM classifier trained from MELD again performs slightly better than the MELD embeddings SVM classifier. We now do get a score for *fear* and *joy*. Nevertheless, the scores are very low compared to the classifier trained on the tweet training data, which confirms the domain and genre bias of the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
